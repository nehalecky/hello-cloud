---
title: "Generate Synthetic Cloud Data"
subtitle: "Task-oriented guide for creating realistic datasets"
---

# How to Generate Synthetic Cloud Data

Step-by-step instructions for creating production-ready synthetic cloud resource datasets.

## Prerequisites

```bash
# Install with data generation dependencies
uv pip install cloud-resource-simulator
```

---

## Quick Start

### Basic Dataset Generation

```{python}
#| eval: false
from cloud_sim.data_generation import WorkloadPatternGenerator, WorkloadType
from datetime import datetime, timedelta

# Initialize generator
generator = WorkloadPatternGenerator(seed=42)

# Generate 30 days of web application data
df = generator.generate_time_series(
    workload_type=WorkloadType.WEB_APP,
    start_time=datetime.now() - timedelta(days=30),
    end_time=datetime.now(),
    interval_minutes=60
)

print(f"Generated {len(df):,} samples")
```

**Output columns:**
- `timestamp`, `cpu_utilization`, `memory_utilization`
- `network_in_mbps`, `network_out_mbps`, `disk_iops`
- `efficiency_score`, `waste_percentage`
- `is_idle`, `is_overprovisioned`

---

## Common Tasks

### 1. Generate Multiple Workload Types

```{python}
#| eval: false
import polars as pl

workloads = {
    WorkloadType.WEB_APP: "web_app_data.parquet",
    WorkloadType.DATABASE_OLTP: "database_data.parquet",
    WorkloadType.ML_TRAINING: "ml_training_data.parquet"
}

for workload_type, filename in workloads.items():
    df = generator.generate_time_series(
        workload_type=workload_type,
        start_time=datetime.now() - timedelta(days=90),
        end_time=datetime.now(),
        interval_minutes=5  # 5-minute granularity
    )

    df.write_parquet(filename)
    print(f"âœ“ Saved {filename}: {len(df):,} samples")
```

---

### 2. Add Anomalies

```{python}
#| eval: false
# Generate data with 2% anomaly rate
df = generator.generate_time_series(
    workload_type=WorkloadType.WEB_APP,
    start_time=datetime.now() - timedelta(days=30),
    end_time=datetime.now(),
    interval_minutes=60,
    include_anomalies=True,
    anomaly_rate=0.02  # 2% anomalies
)

# Check anomaly distribution
anomaly_count = df.filter(pl.col('is_anomaly')).height
print(f"Anomalies: {anomaly_count} ({anomaly_count/len(df)*100:.2f}%)")
```

**Anomaly types:**
- CPU/memory spikes
- Network saturation
- Disk I/O bottlenecks
- Complete failures (zero utilization)

---

### 3. Custom Time Ranges

```{python}
#| eval: false
# Specific date range
from datetime import datetime

df = generator.generate_time_series(
    workload_type=WorkloadType.DATABASE_OLTP,
    start_time=datetime(2024, 1, 1, 0, 0),
    end_time=datetime(2024, 12, 31, 23, 59),
    interval_minutes=15  # 15-minute intervals
)

# Full year of data
print(f"Dataset spans: {df['timestamp'].min()} to {df['timestamp'].max()}")
```

---

### 4. Export to HuggingFace Datasets

```{python}
#| eval: false
from datasets import Dataset

# Convert Polars to HuggingFace format
hf_dataset = Dataset.from_pandas(df.to_pandas())

# Add metadata
hf_dataset = hf_dataset.with_format("polars")

# Save locally
hf_dataset.save_to_disk("cloud_sim_data")

# Or push to HuggingFace Hub
# hf_dataset.push_to_hub("username/cloud-resource-sim")
```

---

### 5. Combine Multiple Sources

```{python}
#| eval: false
# Generate fleet of resources
resource_data = []

for resource_id in range(100):
    df = generator.generate_time_series(
        workload_type=WorkloadType.MICROSERVICES,
        start_time=datetime.now() - timedelta(days=7),
        end_time=datetime.now(),
        interval_minutes=5
    )

    # Add resource identifier
    df = df.with_columns(pl.lit(f"resource-{resource_id:03d}").alias("resource_id"))
    resource_data.append(df)

# Combine all resources
fleet_df = pl.concat(resource_data)
print(f"Fleet data: {len(fleet_df):,} total samples across 100 resources")
```

---

## Advanced Configuration

### Lower-Level Simulator

For custom scenarios, use `CloudMetricsSimulator`:

```{python}
#| eval: false
from cloud_sim.data_generation import CloudMetricsSimulator

simulator = CloudMetricsSimulator(
    num_resources=50,
    start_date=datetime(2024, 1, 1),
    end_date=datetime(2024, 12, 31),
    sampling_interval_minutes=5,
    cloud_providers=["AWS", "Azure", "GCP"],
    resource_types=["compute", "storage", "network"]
)

df = simulator.generate_dataset(
    include_anomalies=True,
    anomaly_rate=0.03,
    include_unit_economics=True  # Add cost data
)

print(df.head())
```

---

## Output Formats

### Parquet (Recommended)

```{python}
#| eval: false
df.write_parquet("data.parquet", compression="snappy")
```

**Benefits:**
- Columnar format (fast analytics)
- Excellent compression
- Preserves data types

### CSV

```{python}
#| eval: false
df.write_csv("data.csv")
```

**When to use:**
- Need Excel compatibility
- External tool integration
- Human inspection

### Polars DataFrame (In-Memory)

```{python}
#| eval: false
# Keep in memory for immediate analysis
df.select(['timestamp', 'cpu_utilization', 'memory_utilization'])
```

---

## Validation Checklist

Before using generated data:

- [ ] **Temporal range**: Check start/end timestamps
- [ ] **Sample count**: Verify expected number of rows
- [ ] **Anomaly rate**: Confirm anomaly percentage
- [ ] **Statistics**: Compare to research benchmarks (13% CPU, 20% memory)
- [ ] **No nulls**: `df.null_count().sum()` should be 0
- [ ] **Temporal order**: Timestamps should be sorted

```{python}
#| eval: false
# Quick validation
print(f"Date range: {df['timestamp'].min()} to {df['timestamp'].max()}")
print(f"Sample count: {len(df):,}")
print(f"Anomalies: {df['is_anomaly'].sum()} ({df['is_anomaly'].mean()*100:.2f}%)")
print(f"Mean CPU: {df['cpu_utilization'].mean():.1f}%")
print(f"Mean Memory: {df['memory_utilization'].mean():.1f}%")
print(f"Nulls: {df.null_count().sum()}")
```

---

## Troubleshooting

### Issue: Data doesn't match research patterns

**Solution:** Check workload type selection

```{python}
#| eval: false
# Different types have different characteristics
# Web apps: 15-25% CPU
# ML training: 70-90% CPU
# Dev environments: 5-10% CPU
```

### Issue: Too many/few anomalies

**Solution:** Adjust anomaly rate

```{python}
#| eval: false
# Operational systems: 0.5-2%
df = generator.generate_time_series(..., anomaly_rate=0.01)

# Research/testing: 5-10%
df = generator.generate_time_series(..., anomaly_rate=0.08)
```

### Issue: Memory constraints with large datasets

**Solution:** Use streaming generation

```{python}
#| eval: false
# Generate in chunks
for month in range(1, 13):
    start = datetime(2024, month, 1)
    end = datetime(2024, month + 1, 1) if month < 12 else datetime(2025, 1, 1)

    df_chunk = generator.generate_time_series(
        workload_type=WorkloadType.WEB_APP,
        start_time=start,
        end_time=end,
        interval_minutes=5
    )

    df_chunk.write_parquet(f"data_2024_{month:02d}.parquet")
```

---

## See Also

- **[Tutorial: Data Exploration](../tutorials/data-exploration.qmd)** - Learn the patterns
- **[Tutorial: Workload Signatures](../tutorials/workload-signatures.qmd)** - Understand archetypes
- **[API Reference](../reference/index.qmd)** - Complete API docs
