---
title: "Train Gaussian Process Models"
subtitle: "Production-ready GP training recipes"
---

# How to Train Gaussian Process Models

Practical recipes for training production-ready GP models for cloud resource forecasting.

## Prerequisites

```bash
# Install with GPU support (recommended)
uv pip install cloud-resource-simulator[gpu]

# Or CPU-only
uv pip install cloud-resource-simulator
```

---

## Quick Start

### Basic Training

```{python}
#| eval: false
import torch
import polars as pl
from cloud_sim.ml_models.gaussian_process import (
    SparseGPModel,
    train_gp_model,
    compute_metrics
)

# Load your data
df = pl.read_parquet("cloud_data.parquet")

# Prepare tensors
X = torch.arange(len(df), dtype=torch.float32).reshape(-1, 1)
y = torch.tensor(df['cpu_utilization'].to_numpy(), dtype=torch.float32)

# Initialize and train
model = SparseGPModel(num_inducing=100, likelihood_type='student_t')
trained_model, losses = train_gp_model(
    model=model,
    train_x=X,
    train_y=y,
    num_epochs=100,
    learning_rate=0.01
)

print("✓ Training complete")
```

---

## Common Tasks

### 1. Choose Inducing Point Count

**Rule of thumb:** `num_inducing = sqrt(n) / 10`

```{python}
#| eval: false
import numpy as np

n_samples = len(df)
num_inducing = int(np.sqrt(n_samples) / 10)
num_inducing = max(50, min(500, num_inducing))  # Clamp to [50, 500]

print(f"Dataset size: {n_samples:,}")
print(f"Recommended inducing points: {num_inducing}")
```

**Guidelines:**
- **Small datasets** (<10k samples): 50-100 points
- **Medium datasets** (10k-100k): 100-300 points
- **Large datasets** (>100k): 300-500 points

---

### 2. Configure Periodic Kernel

For time series with known periodicity:

```{python}
#| eval: false
from cloud_sim.ml_models.gaussian_process import CompositePeriodicKernel

# Daily + weekly patterns (minute-level data)
kernel = CompositePeriodicKernel(
    periods=[1440.0, 10080.0],  # 1 day, 1 week in minutes
    lengthscales=[360.0, 2520.0],  # Quarter-day, half-week
    add_noise=True
)

model = SparseGPModel(
    num_inducing=200,
    likelihood_type='student_t',
    kernel=kernel
)
```

**Common periods (minute-level data):**
- Hourly: `60`
- Daily: `1440`
- Weekly: `10080`
- Monthly: `43200`

---

### 3. Handle Large Datasets

Use mini-batch training:

```{python}
#| eval: false
# Train on batches for memory efficiency
batch_size = 10000

for epoch in range(100):
    # Shuffle data
    indices = torch.randperm(len(X))

    for i in range(0, len(X), batch_size):
        batch_indices = indices[i:i+batch_size]
        X_batch = X[batch_indices]
        y_batch = y[batch_indices]

        # Training step
        model.train()
        optimizer.zero_grad()
        output = model(X_batch)
        loss = -model.likelihood(output, y_batch).log_prob(y_batch).mean()
        loss.backward()
        optimizer.step()
```

---

### 4. Monitor Training Progress

```{python}
#| eval: false
import altair as alt

# Plot training loss
loss_df = pl.DataFrame({'epoch': range(len(losses)), 'loss': losses})

chart = alt.Chart(loss_df.to_pandas()).mark_line().encode(
    x='epoch:Q',
    y=alt.Y('loss:Q', scale=alt.Scale(type='log'))
).properties(
    width=600,
    height=300,
    title='Training Loss (Log Scale)'
)

chart
```

**Expected behavior:**
- Rapid decrease in first 10-20 epochs
- Convergence around epoch 50-80
- Plateaus or slight oscillation at convergence

**Warning signs:**
- Loss increasing → learning rate too high
- Flat from start → learning rate too low or bad initialization
- Oscillating wildly → unstable training

---

### 5. Hyperparameter Tuning

```{python}
#| eval: false
# Grid search over key hyperparameters
configs = [
    {'num_inducing': 100, 'lr': 0.01, 'likelihood': 'student_t'},
    {'num_inducing': 200, 'lr': 0.01, 'likelihood': 'student_t'},
    {'num_inducing': 100, 'lr': 0.005, 'likelihood': 'gaussian'},
    {'num_inducing': 200, 'lr': 0.005, 'likelihood': 'gaussian'},
]

best_model = None
best_rmse = float('inf')

for config in configs:
    model = SparseGPModel(
        num_inducing=config['num_inducing'],
        likelihood_type=config['likelihood']
    )

    trained, _ = train_gp_model(
        model, X_train, y_train,
        num_epochs=50,
        learning_rate=config['lr']
    )

    # Evaluate on validation set
    with torch.no_grad():
        pred = trained(X_val).mean
        rmse = torch.sqrt(((pred - y_val)**2).mean()).item()

    if rmse < best_rmse:
        best_rmse = rmse
        best_model = trained
        best_config = config

    print(f"Config: {config} → RMSE: {rmse:.3f}")

print(f"\nBest config: {best_config} with RMSE: {best_rmse:.3f}")
```

---

## Production Patterns

### Model Persistence

```{python}
#| eval: false
from cloud_sim.ml_models.gaussian_process import save_model, load_model

# Save trained model
save_model(trained_model, "models/gp_cpu_forecaster_v1.pt")

# Load for inference
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
loaded_model = load_model("models/gp_cpu_forecaster_v1.pt", device=device)
```

### Batch Prediction

```{python}
#| eval: false
import gpytorch

# Efficient batch prediction
batch_size = 5000
predictions = []
uncertainties = []

model.eval()
for i in range(0, len(X_test), batch_size):
    X_batch = X_test[i:i+batch_size]

    with torch.no_grad(), gpytorch.settings.fast_pred_var():
        pred_dist = model(X_batch)
        predictions.append(pred_dist.mean.numpy())
        uncertainties.append(pred_dist.stddev.numpy())

pred_mean = np.concatenate(predictions)
pred_std = np.concatenate(uncertainties)
```

### Online Learning

```{python}
#| eval: false
# Retrain on sliding window
def update_model(model, new_data, window_size=10000):
    """Incremental update with recent data"""
    recent = new_data[-window_size:]

    X = torch.arange(len(recent), dtype=torch.float32).reshape(-1, 1)
    y = torch.tensor(recent['value'].to_numpy(), dtype=torch.float32)

    updated, _ = train_gp_model(
        model, X, y,
        num_epochs=20,  # Fewer epochs for updates
        learning_rate=0.005  # Lower LR for fine-tuning
    )

    return updated

# Update hourly
# model = update_model(model, latest_data)
```

---

## Evaluation Best Practices

### Comprehensive Metrics

```{python}
#| eval: false
# Evaluate accuracy AND calibration
metrics = compute_metrics(
    y_true=y_test.numpy(),
    y_pred=pred_mean,
    y_std=pred_std
)

print("Accuracy:")
print(f"  RMSE: {metrics['rmse']:.3f}")
print(f"  MAE: {metrics['mae']:.3f}")
print(f"  R²: {metrics['r2']:.3f}")

print("\nCalibration:")
print(f"  Coverage@95: {metrics['coverage_95']:.1f}%")  # Should be ~95%
print(f"  Calibration Error: {metrics['calibration_error']:.3f}")  # Should be ~0
```

### Cross-Validation

```{python}
#| eval: false
# Time series cross-validation
from sklearn.model_selection import TimeSeriesSplit

tscv = TimeSeriesSplit(n_splits=5)
scores = []

for train_idx, test_idx in tscv.split(X):
    X_train_cv, X_test_cv = X[train_idx], X[test_idx]
    y_train_cv, y_test_cv = y[train_idx], y[test_idx]

    model_cv = SparseGPModel(num_inducing=100)
    trained_cv, _ = train_gp_model(model_cv, X_train_cv, y_train_cv, num_epochs=50)

    with torch.no_grad():
        pred = trained_cv(X_test_cv).mean
        rmse = torch.sqrt(((pred - y_test_cv)**2).mean()).item()

    scores.append(rmse)

print(f"CV RMSE: {np.mean(scores):.3f} ± {np.std(scores):.3f}")
```

---

## Troubleshooting

### Issue: Training is slow

**Solutions:**
1. Reduce `num_inducing` (100 vs. 500)
2. Use GPU if available
3. Reduce `num_epochs` (50 vs. 100)
4. Use mini-batch training

### Issue: Poor predictions

**Solutions:**
1. Check data quality (nulls, outliers)
2. Try Student-t likelihood (robust to outliers)
3. Increase `num_inducing` points
4. Tune kernel hyperparameters
5. Add more training epochs

### Issue: Overconfident predictions

**Solutions:**
1. Switch from Gaussian to Student-t likelihood
2. Add noise kernel component
3. Check for data leakage (train/test split)

### Issue: Underconfident predictions

**Solutions:**
1. Reduce noise in data
2. Train longer (more epochs)
3. Check kernel configuration

---

## See Also

- **[Tutorial: Gaussian Processes](../tutorials/gaussian-processes.qmd)** - Learn the fundamentals
- **[Concepts: GP Design](../concepts/design/gaussian-process-design.qmd)** - Architecture details
- **[API Reference](../reference/index.qmd)** - Complete API documentation
