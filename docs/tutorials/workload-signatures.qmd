---
title: "Workload Signatures Tutorial"
subtitle: "Understand the distinct resource patterns of different application types"
execute:
  eval: false
  echo: true
---

# Understanding Workload Signatures

Different application types have **distinct resource utilization patterns** - their unique "signatures". In this tutorial, you'll learn why these patterns emerge and how to work with them.

## Learning Objectives

- Understand the forces that create workload signatures
- Explore 12+ workload archetypes
- Analyze resource utilization patterns
- Identify temporal characteristics

::: {.callout-tip}
## Estimated Time
20 minutes
:::

---

## Setup

```{python}
import polars as pl
import numpy as np
import altair as alt
from datetime import datetime, timedelta

from cloud_sim.data_generation import WorkloadPatternGenerator, WorkloadType

# Initialize generator
generator = WorkloadPatternGenerator(seed=42)
```

---

## Part 1: Why Do Signatures Exist?

Workload signatures emerge from fundamental computing constraints:

### The Key Forces

1. **Hardware Constraints**
   - CPU-memory bandwidth limits
   - I/O latency (disk, network)
   - Cache hierarchies

2. **Architecture Patterns**
   - Request model (sync/async, batch/stream)
   - State management (stateful/stateless)
   - Concurrency model

3. **Business Drivers**
   - User behavior patterns
   - Business hours and timezones
   - Seasonal demands

4. **Optimization Techniques**
   - Auto-scaling policies
   - Caching strategies
   - Resource scheduling

::: {.callout-note}
## Example: Web Application Signature

Web apps show **moderate CPU, high network, business-hours pattern** because:

- **Request-driven**: CPU spikes with each request
- **I/O bound**: Waiting for database/API calls
- **Human-driven**: Traffic follows business hours
- **Auto-scaled**: Resources adjust to demand
:::

---

## Part 2: The 12+ Workload Archetypes

Let's explore the primary workload types:

```{python}
# Generate representative examples
workloads = {
    WorkloadType.WEB_APP: "Web Application",
    WorkloadType.BATCH_PROCESSING: "Batch Processing",
    WorkloadType.ML_TRAINING: "ML Training",
    WorkloadType.DATABASE_OLTP: "Database OLTP",
    WorkloadType.DATABASE_OLAP: "Database OLAP",
    WorkloadType.STREAMING: "Stream Processing",
    WorkloadType.DEV_ENVIRONMENT: "Development",
    WorkloadType.MICROSERVICES: "Microservices",
    WorkloadType.SERVERLESS: "Serverless/Functions",
    WorkloadType.CONTAINER: "Container Workload"
}

# Generate 7 days of data
signatures = {}
for workload_type, name in workloads.items():
    df = generator.generate_time_series(
        workload_type=workload_type,
        start_time=datetime.now() - timedelta(days=7),
        end_time=datetime.now(),
        interval_minutes=60
    )
    signatures[name] = df
```

---

## Part 3: Signature Analysis

### CPU Utilization Patterns

```{python}
# Compare average CPU utilization
cpu_comparison = []
for name, df in signatures.items():
    cpu_comparison.append({
        'Workload': name,
        'Mean CPU': df['cpu_utilization'].mean(),
        'P50 CPU': df['cpu_utilization'].median(),
        'P95 CPU': df['cpu_utilization'].quantile(0.95),
        'Std Dev': df['cpu_utilization'].std()
    })

cpu_df = pl.DataFrame(cpu_comparison).sort('Mean CPU', descending=True)
print(cpu_df)
```

**Expected patterns:**

| Workload Type | Mean CPU | Characteristics |
|--------------|----------|-----------------|
| ML Training | 70-90% | High, sustained utilization |
| HPC | 80-95% | Very high, consistent |
| Batch Processing | 50-70% | High during job execution |
| Database OLAP | 40-60% | Moderate, query-driven |
| Database OLTP | 40-60% | Moderate, transaction-driven |
| Web Application | 15-25% | Low-moderate, request-driven |
| Microservices | 20-30% | Moderate, distributed |
| Stream Processing | 30-50% | Steady, data-driven |
| Serverless | 10-30% | Sporadic, event-driven |
| Container | 20-40% | Variable by application |
| Development | 5-10% | Very low, intermittent |

---

### Memory-CPU Correlation

Different workloads show different CPU-memory relationships:

```{python}
# Calculate correlations
correlations = []
for name, df in signatures.items():
    corr = df.select([
        pl.corr('cpu_utilization', 'memory_utilization').alias('cpu_mem_corr')
    ])
    correlations.append({
        'Workload': name,
        'Correlation': corr['cpu_mem_corr'][0]
    })

corr_df = pl.DataFrame(correlations).sort('Correlation', descending=True)

# Visualize
chart = alt.Chart(corr_df.to_pandas()).mark_bar().encode(
    x=alt.X('Correlation:Q', scale=alt.Scale(domain=[-0.5, 1.0])),
    y=alt.Y('Workload:N', sort='-x'),
    color=alt.condition(
        alt.datum.Correlation > 0.7,
        alt.value('steelblue'),
        alt.value('lightcoral')
    ),
    tooltip=['Workload', 'Correlation']
).properties(
    width=600,
    height=400,
    title='CPU-Memory Correlation by Workload Type'
)

chart
```

**Interpretation:**

- **High correlation (>0.8)**: Database OLAP, ML Training
  - Memory usage scales with computation
  - Large working sets in memory

- **Moderate correlation (0.4-0.7)**: Web Apps, Microservices
  - Some coupling, but not strict
  - Caching affects the relationship

- **Low correlation (<0.4)**: Serverless, Batch Processing
  - Memory needs independent of CPU
  - Distinct phases (I/O vs. compute)

---

## Part 4: Temporal Patterns

### Daily Cycles

```{python}
# Analyze hourly patterns for web application
web_app = signatures["Web Application"]

# Group by hour of day
hourly = web_app.with_columns(
    pl.col('timestamp').dt.hour().alias('hour')
).group_by('hour').agg([
    pl.col('cpu_utilization').mean().alias('avg_cpu'),
    pl.col('memory_utilization').mean().alias('avg_memory'),
    pl.col('network_in_mbps').mean().alias('avg_network')
]).sort('hour')

# Visualize
base = alt.Chart(hourly.to_pandas()).encode(
    x=alt.X('hour:O', title='Hour of Day')
)

cpu_line = base.mark_line(color='steelblue').encode(
    y=alt.Y('avg_cpu:Q', title='CPU Utilization (%)')
)

memory_line = base.mark_line(color='coral').encode(
    y=alt.Y('avg_memory:Q', title='Memory Utilization (%)')
)

chart = alt.layer(cpu_line, memory_line).resolve_scale(
    y='independent'
).properties(
    width=700,
    height=300,
    title='Web Application: Daily Resource Pattern'
)

chart
```

::: {.callout-note}
## Business Hours Effect

Notice the clear **business hours pattern** (9 AM - 5 PM):

- CPU rises 40-60% during peak hours
- Memory increases more gradually
- Network traffic follows CPU closely

This reflects **human-driven workloads**.
:::

---

### Weekly Seasonality

```{python}
# Analyze day-of-week patterns
weekly = web_app.with_columns(
    pl.col('timestamp').dt.weekday().alias('day_of_week')
).group_by('day_of_week').agg([
    pl.col('cpu_utilization').mean().alias('avg_cpu'),
    pl.col('waste_percentage').mean().alias('avg_waste')
]).sort('day_of_week')

# Add day names
day_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']
weekly = weekly.with_columns(
    pl.Series('day_name', [day_names[i] for i in weekly['day_of_week']])
)

# Visualize
chart = alt.Chart(weekly.to_pandas()).mark_bar().encode(
    x=alt.X('day_name:N', title='Day of Week', sort=day_names),
    y=alt.Y('avg_cpu:Q', title='Average CPU (%)'),
    color=alt.condition(
        alt.datum.day_name.isin(['Sat', 'Sun']),
        alt.value('lightcoral'),
        alt.value('steelblue')
    ),
    tooltip=['day_name', 'avg_cpu', 'avg_waste']
).properties(
    width=600,
    height=300,
    title='Weekly Pattern: Weekdays vs. Weekends'
)

chart
```

**Research finding**: 60-80% drop on weekends for business workloads ✓

---

## Part 5: Efficiency & Waste Patterns

Different workloads show different waste characteristics:

```{python}
# Analyze waste by workload
waste_analysis = []
for name, df in signatures.items():
    waste_analysis.append({
        'Workload': name,
        'Mean Waste (%)': df['waste_percentage'].mean(),
        'Idle Hours': df.filter(pl.col('is_idle')).height,
        'Overprovisioned Hours': df.filter(pl.col('is_overprovisioned')).height,
        'Total Hours': df.height
    })

waste_df = pl.DataFrame(waste_analysis).sort('Mean Waste (%)', descending=True)
print(waste_df)
```

::: {.callout-warning}
## High-Waste Workloads

**Development Environments**: 70%+ waste
- Often idle during non-work hours
- Overprovisioned for peak needs
- Frequently forgotten after projects

**Batch Processing**: 40-50% waste
- Resources idle between jobs
- Peak provisioning for worst case
- Poor scheduling optimization

**Serverless**: Variable (20-60%)
- Cold start overhead
- Over-allocation for performance
- Granularity mismatches
:::

---

## Part 6: Autocorrelation Analysis

Workloads show different temporal memory:

```{python}
from statsmodels.tsa.stattools import acf

# Compare autocorrelation for different workloads
acf_comparison = {}
workloads_to_compare = ['Web Application', 'ML Training', 'Development']

for name in workloads_to_compare:
    cpu_values = signatures[name]['cpu_utilization'].to_numpy()
    autocorr = acf(cpu_values, nlags=24)  # 24 hours
    acf_comparison[name] = autocorr

# Create dataframe for visualization
acf_data = []
for name, autocorr in acf_comparison.items():
    for lag, value in enumerate(autocorr):
        acf_data.append({'Workload': name, 'Lag (hours)': lag, 'Autocorrelation': value})

acf_df = pl.DataFrame(acf_data)

# Visualize
chart = alt.Chart(acf_df.to_pandas()).mark_line().encode(
    x=alt.X('Lag (hours):Q'),
    y=alt.Y('Autocorrelation:Q'),
    color='Workload:N',
    tooltip=['Workload', 'Lag (hours)', 'Autocorrelation']
).properties(
    width=700,
    height=300,
    title='Temporal Autocorrelation by Workload Type'
)

chart
```

**Observations:**

- **Web Application**: Strong daily cycle (lag 24)
- **ML Training**: Sustained pattern (slow decay)
- **Development**: Weak patterns (irregular usage)

---

## Summary

In this tutorial, you learned:

✅ **Why signatures exist**: Hardware, architecture, business forces
✅ **12+ workload archetypes**: Their distinct patterns
✅ **Resource correlations**: CPU-memory relationships
✅ **Temporal patterns**: Daily, weekly, seasonal
✅ **Efficiency analysis**: Waste and optimization opportunities

## Next Steps

- **[Gaussian Process Tutorial](gaussian-processes.qmd)**: Model these patterns for forecasting
- **[How-To: Generate Synthetic Data](../how-to/generate-synthetic-data.qmd)**: Production examples
- **[Concepts: Research Foundation](../concepts/research/cloud-resource-patterns-research.qmd)**: Deep dive into empirical findings

---

::: {.callout-tip}
## Apply What You've Learned

Try analyzing workloads in your own environment:

1. Identify the archetype (Web App? Database? Batch?)
2. Compare to empirical patterns (CPU, memory, correlation)
3. Analyze temporal cycles (business hours? weekends?)
4. Calculate waste and identify opportunities

**The patterns are universal - they apply to real workloads too!**
:::
