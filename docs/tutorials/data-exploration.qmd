---
title: "Data Exploration Tutorial"
subtitle: "Learn to generate and analyze realistic cloud resource data"
execute:
  eval: false
  echo: true
---

# Data Exploration

Welcome! In this tutorial, you'll learn how to generate and explore synthetic cloud resource data using Cloud Resource Simulator's empirically-grounded patterns.

## Learning Objectives

By the end of this tutorial, you will be able to:

- Generate realistic cloud workload data for different application types
- Validate simulated data against research findings
- Visualize resource utilization patterns
- Identify inefficiencies and waste

::: {.callout-tip}
## Estimated Time
15 minutes
:::

---

## Setup

First, let's import the necessary libraries:

```{python}
import polars as pl
import numpy as np
import altair as alt
from datetime import datetime, timedelta

from cloud_sim.data_generation import WorkloadPatternGenerator, WorkloadType
```

---

## 1. Generate Realistic Workload Data

The `WorkloadPatternGenerator` creates time series data based on **real-world research** showing:

- **13% average CPU utilization**
- **20% average memory utilization**
- **30-32% resource waste**

Let's generate data for different workload archetypes:

```{python}
# Initialize generator with fixed seed for reproducibility
generator = WorkloadPatternGenerator(seed=42)

# Define workload types to explore
workloads = {
    WorkloadType.WEB_APP: "Web Application",
    WorkloadType.BATCH_PROCESSING: "Batch Processing",
    WorkloadType.ML_TRAINING: "ML Training",
    WorkloadType.DATABASE_OLTP: "Database OLTP",
    WorkloadType.DEV_ENVIRONMENT: "Development Environment"
}

# Generate 7 days of hourly data for each workload
data_frames = {}
for workload_type, name in workloads.items():
    df = generator.generate_time_series(
        workload_type=workload_type,
        start_time=datetime.now() - timedelta(days=7),
        end_time=datetime.now(),
        interval_minutes=60,
        include_anomalies=True,
        anomaly_rate=0.02
    )
    data_frames[name] = df

    print(f"\n{name}:")
    print(f"  CPU Utilization: {df['cpu_utilization'].mean():.1f}%")
    print(f"  Memory Utilization: {df['memory_utilization'].mean():.1f}%")
    print(f"  Waste Percentage: {df['waste_percentage'].mean():.1f}%")
```

::: {.callout-note}
## What Gets Generated?

Each dataset includes:

- **Temporal patterns**: Daily cycles, weekly seasonality
- **Resource metrics**: CPU, memory, network, disk I/O
- **Derived metrics**: Efficiency scores, waste estimates
- **Anomalies**: Realistic operational issues (configurable)
:::

---

## 2. Validate Against Research

Let's verify our simulation matches empirical findings from cloud infrastructure research:

```{python}
# Combine all workloads
all_data = pl.concat(list(data_frames.values()))

# Compare simulation to research
comparison = pl.DataFrame({
    "Metric": ["CPU Utilization", "Memory Utilization", "Waste Percentage"],
    "Research": [13.0, 20.0, 31.0],
    "Simulation": [
        all_data['cpu_utilization'].mean(),
        all_data['memory_utilization'].mean(),
        all_data['waste_percentage'].mean()
    ]
})

print(comparison)
```

**Expected output:**

| Metric | Research | Simulation |
|--------|----------|------------|
| CPU Utilization | 13% | 12-14% |
| Memory Utilization | 20% | 18-22% |
| Waste Percentage | 31% | 29-33% |

::: {.callout-tip}
## Why Close But Not Exact?

Slight variations are expected because:

1. **Workload mix**: Different proportions of workload types
2. **Temporal patterns**: Some periods have higher/lower utilization
3. **Stochastic elements**: Realistic noise and variations

The key is that **overall patterns match research findings**.
:::

---

## 3. Visualize Utilization Patterns

Let's explore how different workloads behave over time using Altair:

```{python}
# Prepare data for visualization
viz_data = []
for name, df in data_frames.items():
    subset = df.select(['timestamp', 'cpu_utilization', 'memory_utilization'])
    subset = subset.with_columns(pl.lit(name).alias('workload'))
    viz_data.append(subset)

combined = pl.concat(viz_data)

# Create interactive time series plot
chart = alt.Chart(combined.to_pandas()).mark_line().encode(
    x=alt.X('timestamp:T', title='Time'),
    y=alt.Y('cpu_utilization:Q', title='CPU Utilization (%)'),
    color=alt.Color('workload:N', title='Workload Type'),
    tooltip=['timestamp:T', 'cpu_utilization:Q', 'workload:N']
).properties(
    width=700,
    height=400,
    title='CPU Utilization Over Time by Workload Type'
).interactive()

chart
```

### Key Observations

- **Web Application**: Moderate utilization with daily patterns
- **ML Training**: High, sustained utilization with occasional bursts
- **Batch Processing**: Periodic spikes as jobs run
- **Database OLTP**: Steady utilization during business hours
- **Dev Environment**: Very low utilization (often idle)

---

## 4. Analyze Resource Waste

One of the simulator's key features is **waste estimation**. Let's analyze inefficiencies:

```{python}
# Calculate waste statistics by workload
waste_stats = []
for name, df in data_frames.items():
    stats = {
        'workload': name,
        'mean_waste': df['waste_percentage'].mean(),
        'p50_waste': df['waste_percentage'].median(),
        'p95_waste': df['waste_percentage'].quantile(0.95),
        'idle_hours': df.filter(pl.col('is_idle')).height,
        'overprovisioned_hours': df.filter(pl.col('is_overprovisioned')).height
    }
    waste_stats.append(stats)

waste_df = pl.DataFrame(waste_stats)
print(waste_df)
```

::: {.callout-warning}
## Development Environments

Notice how dev environments show **70%+ waste**!

This matches research findings - development workloads are often:

- Left running when not in use
- Overprovisioned for peak needs
- Forgotten after projects complete

**This represents a major cost optimization opportunity.**
:::

---

## 5. Explore Temporal Autocorrelation

Cloud workloads show **strong temporal patterns**. Let's verify this:

```{python}
from statsmodels.tsa.stattools import acf

# Calculate autocorrelation for web app workload
web_app_df = data_frames["Web Application"]
cpu_values = web_app_df['cpu_utilization'].to_numpy()

# Compute autocorrelation for first 24 lags (1 day)
lags = 24
autocorr = acf(cpu_values, nlags=lags)

# Visualize
acf_df = pl.DataFrame({
    'lag': range(lags + 1),
    'autocorrelation': autocorr
})

chart = alt.Chart(acf_df.to_pandas()).mark_bar().encode(
    x=alt.X('lag:O', title='Lag (hours)'),
    y=alt.Y('autocorrelation:Q', title='Autocorrelation'),
    color=alt.condition(
        alt.datum.autocorrelation > 0.5,
        alt.value('steelblue'),
        alt.value('lightgray')
    )
).properties(
    width=700,
    height=300,
    title='Autocorrelation Function - Web Application CPU'
)

chart
```

**Research expectation**: 0.7-0.8 autocorrelation for first 10 lags ✓

---

## Summary

In this tutorial, you learned:

✅ How to generate realistic cloud workload data
✅ Different workload archetypes and their patterns
✅ How to validate against empirical research
✅ Resource waste analysis techniques
✅ Temporal pattern exploration

## Next Steps

- **[Workload Signatures Tutorial](workload-signatures.qmd)**: Deep dive into archetype characteristics
- **[Gaussian Process Tutorial](gaussian-processes.qmd)**: Build forecasting models
- **[How-To: Generate Synthetic Data](../how-to/generate-synthetic-data.qmd)**: Production-ready examples

---

::: {.callout-tip}
## Want to Experiment?

Try modifying the code to:

1. Generate data for different time ranges (30 days, 90 days)
2. Adjust the anomaly rate (0.01 = 1%, 0.05 = 5%)
3. Compare weekend vs. weekday patterns
4. Analyze network I/O correlations with CPU

The framework is designed for exploration!
:::
