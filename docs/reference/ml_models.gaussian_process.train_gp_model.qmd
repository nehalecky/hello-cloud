# ml_models.gaussian_process.train_gp_model { #cloud_sim.ml_models.gaussian_process.train_gp_model }

```python
ml_models.gaussian_process.train_gp_model(
    model,
    likelihood,
    X_train,
    y_train,
    n_epochs=100,
    batch_size=2048,
    learning_rate=0.01,
    cholesky_jitter=0.001,
    cholesky_max_tries=10,
    verbose=True,
)
```

Train sparse GP model using mini-batch variational inference.

Uses maximum numerical stability settings to prevent Cholesky decomposition failures.

Args:
    model: SparseGPModel instance
    likelihood: GPyTorch likelihood (e.g., GaussianLikelihood, StudentTLikelihood)
    X_train: Training inputs of shape (n, d)
    y_train: Training outputs of shape (n,)
    n_epochs: Number of training epochs (default: 100)
    batch_size: Mini-batch size for training (default: 2048)
    learning_rate: Adam optimizer learning rate (default: 0.01)
    cholesky_jitter: Jitter added to diagonal for numerical stability (default: 1e-3)
    cholesky_max_tries: Maximum Cholesky decomposition attempts (default: 10)
    verbose: Print training progress every 10 epochs (default: True)

Returns:
    List of average losses per epoch

Example:
    ```python
    model = SparseGPModel(inducing_points=inducing_pts)
    likelihood = gpytorch.likelihoods.StudentTLikelihood()

    losses = train_gp_model(
        model=model,
        likelihood=likelihood,
        X_train=X_train_norm,
        y_train=y_train,
        n_epochs=100,
        batch_size=2048
    )
    ```