[
  {
    "objectID": "reference/data_generation.CloudMetricsDatasetBuilder.html",
    "href": "reference/data_generation.CloudMetricsDatasetBuilder.html",
    "title": "data_generation.CloudMetricsDatasetBuilder",
    "section": "",
    "text": "data_generation.CloudMetricsDatasetBuilder(cache_dir=None)\nBuild and manage HuggingFace datasets for cloud metrics\n\n\n\n\n\nName\nDescription\n\n\n\n\ncompute_dataset_statistics\nCompute comprehensive statistics for a dataset.\n\n\ncreate_anomaly_detection_dataset\nCreate dataset specifically for anomaly detection\n\n\ncreate_cost_optimization_dataset\nCreate dataset for cost optimization recommendations\n\n\ncreate_time_series_sequences\nCreate sequences for time series models\n\n\ncreate_time_series_splits\nCreate chronological train/test/validation splits for time series data.\n\n\nload_from_hub\nLoad dataset from HuggingFace Hub\n\n\npolars_to_dataset\nConvert Polars DataFrame to HuggingFace Dataset\n\n\nprepare_for_foundation_models\nPrepare dataset for foundation model training.\n\n\nprepare_sliding_windows\nPrepare sliding window features for time series models.\n\n\nvalidate_dataset\nValidate dataset quality and schema.\n\n\n\n\n\ndata_generation.CloudMetricsDatasetBuilder.compute_dataset_statistics(dataset)\nCompute comprehensive statistics for a dataset.\n\n\n\ndata_generation.CloudMetricsDatasetBuilder.create_anomaly_detection_dataset(\n    df,\n    window_size=24,\n    contamination_rate=0.1,\n)\nCreate dataset specifically for anomaly detection\n\n\n\ndata_generation.CloudMetricsDatasetBuilder.create_cost_optimization_dataset(df)\nCreate dataset for cost optimization recommendations\n\n\n\ndata_generation.CloudMetricsDatasetBuilder.create_time_series_sequences(\n    dataset,\n    sequence_length=168,\n    stride=24,\n    target_column='hourly_cost',\n)\nCreate sequences for time series models\n\n\n\ndata_generation.CloudMetricsDatasetBuilder.create_time_series_splits(\n    df,\n    test_size=0.2,\n    val_size=0.1,\n)\nCreate chronological train/test/validation splits for time series data.\nArgs: df: Polars DataFrame with time series data test_size: Fraction of data for test set val_size: Fraction of data for validation set\nReturns: DatasetDict with train, validation, and test splits\n\n\n\ndata_generation.CloudMetricsDatasetBuilder.load_from_hub(\n    repo_name,\n    split=None,\n    token=None,\n)\nLoad dataset from HuggingFace Hub\n\n\n\ndata_generation.CloudMetricsDatasetBuilder.polars_to_dataset(\n    df,\n    split_name='train',\n)\nConvert Polars DataFrame to HuggingFace Dataset\n\n\n\ndata_generation.CloudMetricsDatasetBuilder.prepare_for_foundation_models(df)\nPrepare dataset for foundation model training.\nArgs: df: Polars DataFrame with cloud metrics\nReturns: HuggingFace Dataset formatted for foundation models\n\n\n\ndata_generation.CloudMetricsDatasetBuilder.prepare_sliding_windows(\n    df,\n    window_size=24,\n    stride=6,\n    target_col='hourly_cost',\n)\nPrepare sliding window features for time series models.\nArgs: df: Polars DataFrame with time series data window_size: Size of the context window stride: Step size between windows target_col: Column to use as target\nReturns: List of window dictionaries with context and target\n\n\n\ndata_generation.CloudMetricsDatasetBuilder.validate_dataset(\n    dataset,\n    check_schema=True,\n)\nValidate dataset quality and schema.\nReturns: Tuple of (is_valid, list_of_errors)"
  },
  {
    "objectID": "reference/data_generation.CloudMetricsDatasetBuilder.html#methods",
    "href": "reference/data_generation.CloudMetricsDatasetBuilder.html#methods",
    "title": "data_generation.CloudMetricsDatasetBuilder",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncompute_dataset_statistics\nCompute comprehensive statistics for a dataset.\n\n\ncreate_anomaly_detection_dataset\nCreate dataset specifically for anomaly detection\n\n\ncreate_cost_optimization_dataset\nCreate dataset for cost optimization recommendations\n\n\ncreate_time_series_sequences\nCreate sequences for time series models\n\n\ncreate_time_series_splits\nCreate chronological train/test/validation splits for time series data.\n\n\nload_from_hub\nLoad dataset from HuggingFace Hub\n\n\npolars_to_dataset\nConvert Polars DataFrame to HuggingFace Dataset\n\n\nprepare_for_foundation_models\nPrepare dataset for foundation model training.\n\n\nprepare_sliding_windows\nPrepare sliding window features for time series models.\n\n\nvalidate_dataset\nValidate dataset quality and schema.\n\n\n\n\n\ndata_generation.CloudMetricsDatasetBuilder.compute_dataset_statistics(dataset)\nCompute comprehensive statistics for a dataset.\n\n\n\ndata_generation.CloudMetricsDatasetBuilder.create_anomaly_detection_dataset(\n    df,\n    window_size=24,\n    contamination_rate=0.1,\n)\nCreate dataset specifically for anomaly detection\n\n\n\ndata_generation.CloudMetricsDatasetBuilder.create_cost_optimization_dataset(df)\nCreate dataset for cost optimization recommendations\n\n\n\ndata_generation.CloudMetricsDatasetBuilder.create_time_series_sequences(\n    dataset,\n    sequence_length=168,\n    stride=24,\n    target_column='hourly_cost',\n)\nCreate sequences for time series models\n\n\n\ndata_generation.CloudMetricsDatasetBuilder.create_time_series_splits(\n    df,\n    test_size=0.2,\n    val_size=0.1,\n)\nCreate chronological train/test/validation splits for time series data.\nArgs: df: Polars DataFrame with time series data test_size: Fraction of data for test set val_size: Fraction of data for validation set\nReturns: DatasetDict with train, validation, and test splits\n\n\n\ndata_generation.CloudMetricsDatasetBuilder.load_from_hub(\n    repo_name,\n    split=None,\n    token=None,\n)\nLoad dataset from HuggingFace Hub\n\n\n\ndata_generation.CloudMetricsDatasetBuilder.polars_to_dataset(\n    df,\n    split_name='train',\n)\nConvert Polars DataFrame to HuggingFace Dataset\n\n\n\ndata_generation.CloudMetricsDatasetBuilder.prepare_for_foundation_models(df)\nPrepare dataset for foundation model training.\nArgs: df: Polars DataFrame with cloud metrics\nReturns: HuggingFace Dataset formatted for foundation models\n\n\n\ndata_generation.CloudMetricsDatasetBuilder.prepare_sliding_windows(\n    df,\n    window_size=24,\n    stride=6,\n    target_col='hourly_cost',\n)\nPrepare sliding window features for time series models.\nArgs: df: Polars DataFrame with time series data window_size: Size of the context window stride: Step size between windows target_col: Column to use as target\nReturns: List of window dictionaries with context and target\n\n\n\ndata_generation.CloudMetricsDatasetBuilder.validate_dataset(\n    dataset,\n    check_schema=True,\n)\nValidate dataset quality and schema.\nReturns: Tuple of (is_valid, list_of_errors)"
  },
  {
    "objectID": "reference/ml_models.gaussian_process.compute_metrics.html",
    "href": "reference/ml_models.gaussian_process.compute_metrics.html",
    "title": "ml_models.gaussian_process.compute_metrics",
    "section": "",
    "text": "ml_models.gaussian_process.compute_metrics\nml_models.gaussian_process.compute_metrics(\n    y_true,\n    y_pred,\n    lower_95,\n    upper_95,\n    lower_99,\n    upper_99,\n    model_name='GP Model',\n)\nCompute comprehensive model evaluation metrics.\nIncludes both point prediction accuracy and uncertainty quantification quality.\nArgs: y_true: True values (n,) y_pred: Predicted mean values (n,) lower_95: Lower bound of 95% prediction interval (n,) upper_95: Upper bound of 95% prediction interval (n,) lower_99: Lower bound of 99% prediction interval (n,) upper_99: Upper bound of 99% prediction interval (n,) model_name: Name for result labeling (default: “GP Model”)\nReturns: Dictionary with metrics: - ‘Model’: Model name - ‘RMSE’: Root mean squared error - ‘MAE’: Mean absolute error - ‘R²’: R-squared score - ‘Coverage 95%’: Fraction of points in 95% interval - ‘Coverage 99%’: Fraction of points in 99% interval - ‘Sharpness 95%’: Average width of 95% interval - ‘Sharpness 99%’: Average width of 99% interval\nExample: python     metrics = compute_metrics(         y_true=y_test,         y_pred=mean_predictions,         lower_95=lower_95_interval,         upper_95=upper_95_interval,         lower_99=lower_99_interval,         upper_99=upper_99_interval,         model_name=\"Robust GP\"     )\nNotes: - Calibration: Coverage should match nominal level (95%, 99%) - Sharpness: Narrower intervals are better IF well-calibrated - Point accuracy: RMSE/MAE measure prediction error, R² measures variance explained"
  },
  {
    "objectID": "reference/ml_models.gaussian_process.train_gp_model.html",
    "href": "reference/ml_models.gaussian_process.train_gp_model.html",
    "title": "ml_models.gaussian_process.train_gp_model",
    "section": "",
    "text": "ml_models.gaussian_process.train_gp_model\nml_models.gaussian_process.train_gp_model(\n    model,\n    likelihood,\n    X_train,\n    y_train,\n    n_epochs=100,\n    batch_size=2048,\n    learning_rate=0.01,\n    cholesky_jitter=0.001,\n    cholesky_max_tries=10,\n    verbose=True,\n)\nTrain sparse GP model using mini-batch variational inference.\nUses maximum numerical stability settings to prevent Cholesky decomposition failures.\nArgs: model: SparseGPModel instance likelihood: GPyTorch likelihood (e.g., GaussianLikelihood, StudentTLikelihood) X_train: Training inputs of shape (n, d) y_train: Training outputs of shape (n,) n_epochs: Number of training epochs (default: 100) batch_size: Mini-batch size for training (default: 2048) learning_rate: Adam optimizer learning rate (default: 0.01) cholesky_jitter: Jitter added to diagonal for numerical stability (default: 1e-3) cholesky_max_tries: Maximum Cholesky decomposition attempts (default: 10) verbose: Print training progress every 10 epochs (default: True)\nReturns: List of average losses per epoch\nExample: ```python model = SparseGPModel(inducing_points=inducing_pts) likelihood = gpytorch.likelihoods.StudentTLikelihood()\nlosses = train_gp_model(\n    model=model,\n    likelihood=likelihood,\n    X_train=X_train_norm,\n    y_train=y_train,\n    n_epochs=100,\n    batch_size=2048\n)\n```"
  },
  {
    "objectID": "reference/data_generation.CloudMetricsSimulator.html",
    "href": "reference/data_generation.CloudMetricsSimulator.html",
    "title": "data_generation.CloudMetricsSimulator",
    "section": "",
    "text": "data_generation.CloudMetricsSimulator(\n    start_date=None,\n    end_date=None,\n    num_resources=100,\n    sampling_interval_minutes=60,\n)\nSimulates cloud infrastructure metrics for multiple providers\n\n\n\n\n\nName\nDescription\n\n\n\n\ncalculate_unit_economics\nCalculate unit economics metrics (cost per customer, feature, etc.)\n\n\ngenerate_dataset\nGenerate complete dataset with all resources\n\n\ngenerate_usage_patterns\nGenerate realistic usage patterns for a resource\n\n\ninject_anomalies\nInject realistic anomalies into the data\n\n\n\n\n\ndata_generation.CloudMetricsSimulator.calculate_unit_economics(df)\nCalculate unit economics metrics (cost per customer, feature, etc.)\n\n\n\ndata_generation.CloudMetricsSimulator.generate_dataset(include_anomalies=True)\nGenerate complete dataset with all resources\n\n\n\ndata_generation.CloudMetricsSimulator.generate_usage_patterns(\n    resource,\n    timestamps,\n)\nGenerate realistic usage patterns for a resource\n\n\n\ndata_generation.CloudMetricsSimulator.inject_anomalies(df, anomaly_rate=0.02)\nInject realistic anomalies into the data"
  },
  {
    "objectID": "reference/data_generation.CloudMetricsSimulator.html#methods",
    "href": "reference/data_generation.CloudMetricsSimulator.html#methods",
    "title": "data_generation.CloudMetricsSimulator",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncalculate_unit_economics\nCalculate unit economics metrics (cost per customer, feature, etc.)\n\n\ngenerate_dataset\nGenerate complete dataset with all resources\n\n\ngenerate_usage_patterns\nGenerate realistic usage patterns for a resource\n\n\ninject_anomalies\nInject realistic anomalies into the data\n\n\n\n\n\ndata_generation.CloudMetricsSimulator.calculate_unit_economics(df)\nCalculate unit economics metrics (cost per customer, feature, etc.)\n\n\n\ndata_generation.CloudMetricsSimulator.generate_dataset(include_anomalies=True)\nGenerate complete dataset with all resources\n\n\n\ndata_generation.CloudMetricsSimulator.generate_usage_patterns(\n    resource,\n    timestamps,\n)\nGenerate realistic usage patterns for a resource\n\n\n\ndata_generation.CloudMetricsSimulator.inject_anomalies(df, anomaly_rate=0.02)\nInject realistic anomalies into the data"
  },
  {
    "objectID": "reference/ml_models.gaussian_process.initialize_inducing_points.html",
    "href": "reference/ml_models.gaussian_process.initialize_inducing_points.html",
    "title": "ml_models.gaussian_process.initialize_inducing_points",
    "section": "",
    "text": "ml_models.gaussian_process.initialize_inducing_points\nml_models.gaussian_process.initialize_inducing_points(\n    X_train,\n    num_inducing,\n    method='evenly_spaced',\n)\nInitialize inducing point locations for sparse GP.\nArgs: X_train: Training inputs of shape (n, d) num_inducing: Number of inducing points (M) method: Initialization method: - “evenly_spaced”: Evenly spaced across training range (default) - “random”: Random subset of training points - “kmeans”: K-means clustering (not yet implemented)\nReturns: Inducing points tensor of shape (M, d)\nExample: python     inducing_points = initialize_inducing_points(         X_train=X_train_norm,         num_inducing=200,         method=\"evenly_spaced\"     )"
  },
  {
    "objectID": "tutorials/workload-signatures.html",
    "href": "tutorials/workload-signatures.html",
    "title": "Workload Signatures Tutorial",
    "section": "",
    "text": "Different application types have distinct resource utilization patterns - their unique “signatures”. In this tutorial, you’ll learn why these patterns emerge and how to work with them.\n\n\n\nUnderstand the forces that create workload signatures\nExplore 12+ workload archetypes\nAnalyze resource utilization patterns\nIdentify temporal characteristics\n\n\n\n\n\n\n\nTipEstimated Time\n\n\n\n20 minutes\n\n\n\n\n\n\n\nimport polars as pl\nimport numpy as np\nimport altair as alt\nfrom datetime import datetime, timedelta\n\nfrom cloud_sim.data_generation import WorkloadPatternGenerator, WorkloadType\n\n# Initialize generator\ngenerator = WorkloadPatternGenerator(seed=42)\n\n\n\n\n\nWorkload signatures emerge from fundamental computing constraints:\n\n\n\nHardware Constraints\n\nCPU-memory bandwidth limits\nI/O latency (disk, network)\nCache hierarchies\n\nArchitecture Patterns\n\nRequest model (sync/async, batch/stream)\nState management (stateful/stateless)\nConcurrency model\n\nBusiness Drivers\n\nUser behavior patterns\nBusiness hours and timezones\nSeasonal demands\n\nOptimization Techniques\n\nAuto-scaling policies\nCaching strategies\nResource scheduling\n\n\n\n\n\n\n\n\nNoteExample: Web Application Signature\n\n\n\nWeb apps show moderate CPU, high network, business-hours pattern because:\n\nRequest-driven: CPU spikes with each request\nI/O bound: Waiting for database/API calls\nHuman-driven: Traffic follows business hours\nAuto-scaled: Resources adjust to demand\n\n\n\n\n\n\n\n\nLet’s explore the primary workload types:\n\n# Generate representative examples\nworkloads = {\n    WorkloadType.WEB_APP: \"Web Application\",\n    WorkloadType.BATCH_PROCESSING: \"Batch Processing\",\n    WorkloadType.ML_TRAINING: \"ML Training\",\n    WorkloadType.DATABASE_OLTP: \"Database OLTP\",\n    WorkloadType.DATABASE_OLAP: \"Database OLAP\",\n    WorkloadType.STREAMING: \"Stream Processing\",\n    WorkloadType.DEV_ENVIRONMENT: \"Development\",\n    WorkloadType.MICROSERVICES: \"Microservices\",\n    WorkloadType.SERVERLESS: \"Serverless/Functions\",\n    WorkloadType.CONTAINER: \"Container Workload\"\n}\n\n# Generate 7 days of data\nsignatures = {}\nfor workload_type, name in workloads.items():\n    df = generator.generate_time_series(\n        workload_type=workload_type,\n        start_time=datetime.now() - timedelta(days=7),\n        end_time=datetime.now(),\n        interval_minutes=60\n    )\n    signatures[name] = df\n\n\n\n\n\n\n\n\n# Compare average CPU utilization\ncpu_comparison = []\nfor name, df in signatures.items():\n    cpu_comparison.append({\n        'Workload': name,\n        'Mean CPU': df['cpu_utilization'].mean(),\n        'P50 CPU': df['cpu_utilization'].median(),\n        'P95 CPU': df['cpu_utilization'].quantile(0.95),\n        'Std Dev': df['cpu_utilization'].std()\n    })\n\ncpu_df = pl.DataFrame(cpu_comparison).sort('Mean CPU', descending=True)\nprint(cpu_df)\n\nExpected patterns:\n\n\n\nWorkload Type\nMean CPU\nCharacteristics\n\n\n\n\nML Training\n70-90%\nHigh, sustained utilization\n\n\nHPC\n80-95%\nVery high, consistent\n\n\nBatch Processing\n50-70%\nHigh during job execution\n\n\nDatabase OLAP\n40-60%\nModerate, query-driven\n\n\nDatabase OLTP\n40-60%\nModerate, transaction-driven\n\n\nWeb Application\n15-25%\nLow-moderate, request-driven\n\n\nMicroservices\n20-30%\nModerate, distributed\n\n\nStream Processing\n30-50%\nSteady, data-driven\n\n\nServerless\n10-30%\nSporadic, event-driven\n\n\nContainer\n20-40%\nVariable by application\n\n\nDevelopment\n5-10%\nVery low, intermittent\n\n\n\n\n\n\n\nDifferent workloads show different CPU-memory relationships:\n\n# Calculate correlations\ncorrelations = []\nfor name, df in signatures.items():\n    corr = df.select([\n        pl.corr('cpu_utilization', 'memory_utilization').alias('cpu_mem_corr')\n    ])\n    correlations.append({\n        'Workload': name,\n        'Correlation': corr['cpu_mem_corr'][0]\n    })\n\ncorr_df = pl.DataFrame(correlations).sort('Correlation', descending=True)\n\n# Visualize\nchart = alt.Chart(corr_df.to_pandas()).mark_bar().encode(\n    x=alt.X('Correlation:Q', scale=alt.Scale(domain=[-0.5, 1.0])),\n    y=alt.Y('Workload:N', sort='-x'),\n    color=alt.condition(\n        alt.datum.Correlation &gt; 0.7,\n        alt.value('steelblue'),\n        alt.value('lightcoral')\n    ),\n    tooltip=['Workload', 'Correlation']\n).properties(\n    width=600,\n    height=400,\n    title='CPU-Memory Correlation by Workload Type'\n)\n\nchart\n\nInterpretation:\n\nHigh correlation (&gt;0.8): Database OLAP, ML Training\n\nMemory usage scales with computation\nLarge working sets in memory\n\nModerate correlation (0.4-0.7): Web Apps, Microservices\n\nSome coupling, but not strict\nCaching affects the relationship\n\nLow correlation (&lt;0.4): Serverless, Batch Processing\n\nMemory needs independent of CPU\nDistinct phases (I/O vs. compute)\n\n\n\n\n\n\n\n\n\n\n# Analyze hourly patterns for web application\nweb_app = signatures[\"Web Application\"]\n\n# Group by hour of day\nhourly = web_app.with_columns(\n    pl.col('timestamp').dt.hour().alias('hour')\n).group_by('hour').agg([\n    pl.col('cpu_utilization').mean().alias('avg_cpu'),\n    pl.col('memory_utilization').mean().alias('avg_memory'),\n    pl.col('network_in_mbps').mean().alias('avg_network')\n]).sort('hour')\n\n# Visualize\nbase = alt.Chart(hourly.to_pandas()).encode(\n    x=alt.X('hour:O', title='Hour of Day')\n)\n\ncpu_line = base.mark_line(color='steelblue').encode(\n    y=alt.Y('avg_cpu:Q', title='CPU Utilization (%)')\n)\n\nmemory_line = base.mark_line(color='coral').encode(\n    y=alt.Y('avg_memory:Q', title='Memory Utilization (%)')\n)\n\nchart = alt.layer(cpu_line, memory_line).resolve_scale(\n    y='independent'\n).properties(\n    width=700,\n    height=300,\n    title='Web Application: Daily Resource Pattern'\n)\n\nchart\n\n\n\n\n\n\n\nNoteBusiness Hours Effect\n\n\n\nNotice the clear business hours pattern (9 AM - 5 PM):\n\nCPU rises 40-60% during peak hours\nMemory increases more gradually\nNetwork traffic follows CPU closely\n\nThis reflects human-driven workloads.\n\n\n\n\n\n\n\n# Analyze day-of-week patterns\nweekly = web_app.with_columns(\n    pl.col('timestamp').dt.weekday().alias('day_of_week')\n).group_by('day_of_week').agg([\n    pl.col('cpu_utilization').mean().alias('avg_cpu'),\n    pl.col('waste_percentage').mean().alias('avg_waste')\n]).sort('day_of_week')\n\n# Add day names\nday_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\nweekly = weekly.with_columns(\n    pl.Series('day_name', [day_names[i] for i in weekly['day_of_week']])\n)\n\n# Visualize\nchart = alt.Chart(weekly.to_pandas()).mark_bar().encode(\n    x=alt.X('day_name:N', title='Day of Week', sort=day_names),\n    y=alt.Y('avg_cpu:Q', title='Average CPU (%)'),\n    color=alt.condition(\n        alt.datum.day_name.isin(['Sat', 'Sun']),\n        alt.value('lightcoral'),\n        alt.value('steelblue')\n    ),\n    tooltip=['day_name', 'avg_cpu', 'avg_waste']\n).properties(\n    width=600,\n    height=300,\n    title='Weekly Pattern: Weekdays vs. Weekends'\n)\n\nchart\n\nResearch finding: 60-80% drop on weekends for business workloads ✓\n\n\n\n\n\nDifferent workloads show different waste characteristics:\n\n# Analyze waste by workload\nwaste_analysis = []\nfor name, df in signatures.items():\n    waste_analysis.append({\n        'Workload': name,\n        'Mean Waste (%)': df['waste_percentage'].mean(),\n        'Idle Hours': df.filter(pl.col('is_idle')).height,\n        'Overprovisioned Hours': df.filter(pl.col('is_overprovisioned')).height,\n        'Total Hours': df.height\n    })\n\nwaste_df = pl.DataFrame(waste_analysis).sort('Mean Waste (%)', descending=True)\nprint(waste_df)\n\n\n\n\n\n\n\nWarningHigh-Waste Workloads\n\n\n\nDevelopment Environments: 70%+ waste - Often idle during non-work hours - Overprovisioned for peak needs - Frequently forgotten after projects\nBatch Processing: 40-50% waste - Resources idle between jobs - Peak provisioning for worst case - Poor scheduling optimization\nServerless: Variable (20-60%) - Cold start overhead - Over-allocation for performance - Granularity mismatches\n\n\n\n\n\n\nWorkloads show different temporal memory:\n\nfrom statsmodels.tsa.stattools import acf\n\n# Compare autocorrelation for different workloads\nacf_comparison = {}\nworkloads_to_compare = ['Web Application', 'ML Training', 'Development']\n\nfor name in workloads_to_compare:\n    cpu_values = signatures[name]['cpu_utilization'].to_numpy()\n    autocorr = acf(cpu_values, nlags=24)  # 24 hours\n    acf_comparison[name] = autocorr\n\n# Create dataframe for visualization\nacf_data = []\nfor name, autocorr in acf_comparison.items():\n    for lag, value in enumerate(autocorr):\n        acf_data.append({'Workload': name, 'Lag (hours)': lag, 'Autocorrelation': value})\n\nacf_df = pl.DataFrame(acf_data)\n\n# Visualize\nchart = alt.Chart(acf_df.to_pandas()).mark_line().encode(\n    x=alt.X('Lag (hours):Q'),\n    y=alt.Y('Autocorrelation:Q'),\n    color='Workload:N',\n    tooltip=['Workload', 'Lag (hours)', 'Autocorrelation']\n).properties(\n    width=700,\n    height=300,\n    title='Temporal Autocorrelation by Workload Type'\n)\n\nchart\n\nObservations:\n\nWeb Application: Strong daily cycle (lag 24)\nML Training: Sustained pattern (slow decay)\nDevelopment: Weak patterns (irregular usage)\n\n\n\n\n\nIn this tutorial, you learned:\n✅ Why signatures exist: Hardware, architecture, business forces ✅ 12+ workload archetypes: Their distinct patterns ✅ Resource correlations: CPU-memory relationships ✅ Temporal patterns: Daily, weekly, seasonal ✅ Efficiency analysis: Waste and optimization opportunities\n\n\n\n\nGaussian Process Tutorial: Model these patterns for forecasting\nHow-To: Generate Synthetic Data: Production examples\nConcepts: Research Foundation: Deep dive into empirical findings\n\n\n\n\n\n\n\n\nTipApply What You’ve Learned\n\n\n\nTry analyzing workloads in your own environment:\n\nIdentify the archetype (Web App? Database? Batch?)\nCompare to empirical patterns (CPU, memory, correlation)\nAnalyze temporal cycles (business hours? weekends?)\nCalculate waste and identify opportunities\n\nThe patterns are universal - they apply to real workloads too!",
    "crumbs": [
      "Home",
      "Tutorials",
      "Workload Signatures Tutorial"
    ]
  },
  {
    "objectID": "tutorials/workload-signatures.html#learning-objectives",
    "href": "tutorials/workload-signatures.html#learning-objectives",
    "title": "Workload Signatures Tutorial",
    "section": "",
    "text": "Understand the forces that create workload signatures\nExplore 12+ workload archetypes\nAnalyze resource utilization patterns\nIdentify temporal characteristics\n\n\n\n\n\n\n\nTipEstimated Time\n\n\n\n20 minutes",
    "crumbs": [
      "Home",
      "Tutorials",
      "Workload Signatures Tutorial"
    ]
  },
  {
    "objectID": "tutorials/workload-signatures.html#setup",
    "href": "tutorials/workload-signatures.html#setup",
    "title": "Workload Signatures Tutorial",
    "section": "",
    "text": "import polars as pl\nimport numpy as np\nimport altair as alt\nfrom datetime import datetime, timedelta\n\nfrom cloud_sim.data_generation import WorkloadPatternGenerator, WorkloadType\n\n# Initialize generator\ngenerator = WorkloadPatternGenerator(seed=42)",
    "crumbs": [
      "Home",
      "Tutorials",
      "Workload Signatures Tutorial"
    ]
  },
  {
    "objectID": "tutorials/workload-signatures.html#part-1-why-do-signatures-exist",
    "href": "tutorials/workload-signatures.html#part-1-why-do-signatures-exist",
    "title": "Workload Signatures Tutorial",
    "section": "",
    "text": "Workload signatures emerge from fundamental computing constraints:\n\n\n\nHardware Constraints\n\nCPU-memory bandwidth limits\nI/O latency (disk, network)\nCache hierarchies\n\nArchitecture Patterns\n\nRequest model (sync/async, batch/stream)\nState management (stateful/stateless)\nConcurrency model\n\nBusiness Drivers\n\nUser behavior patterns\nBusiness hours and timezones\nSeasonal demands\n\nOptimization Techniques\n\nAuto-scaling policies\nCaching strategies\nResource scheduling\n\n\n\n\n\n\n\n\nNoteExample: Web Application Signature\n\n\n\nWeb apps show moderate CPU, high network, business-hours pattern because:\n\nRequest-driven: CPU spikes with each request\nI/O bound: Waiting for database/API calls\nHuman-driven: Traffic follows business hours\nAuto-scaled: Resources adjust to demand",
    "crumbs": [
      "Home",
      "Tutorials",
      "Workload Signatures Tutorial"
    ]
  },
  {
    "objectID": "tutorials/workload-signatures.html#part-2-the-12-workload-archetypes",
    "href": "tutorials/workload-signatures.html#part-2-the-12-workload-archetypes",
    "title": "Workload Signatures Tutorial",
    "section": "",
    "text": "Let’s explore the primary workload types:\n\n# Generate representative examples\nworkloads = {\n    WorkloadType.WEB_APP: \"Web Application\",\n    WorkloadType.BATCH_PROCESSING: \"Batch Processing\",\n    WorkloadType.ML_TRAINING: \"ML Training\",\n    WorkloadType.DATABASE_OLTP: \"Database OLTP\",\n    WorkloadType.DATABASE_OLAP: \"Database OLAP\",\n    WorkloadType.STREAMING: \"Stream Processing\",\n    WorkloadType.DEV_ENVIRONMENT: \"Development\",\n    WorkloadType.MICROSERVICES: \"Microservices\",\n    WorkloadType.SERVERLESS: \"Serverless/Functions\",\n    WorkloadType.CONTAINER: \"Container Workload\"\n}\n\n# Generate 7 days of data\nsignatures = {}\nfor workload_type, name in workloads.items():\n    df = generator.generate_time_series(\n        workload_type=workload_type,\n        start_time=datetime.now() - timedelta(days=7),\n        end_time=datetime.now(),\n        interval_minutes=60\n    )\n    signatures[name] = df",
    "crumbs": [
      "Home",
      "Tutorials",
      "Workload Signatures Tutorial"
    ]
  },
  {
    "objectID": "tutorials/workload-signatures.html#part-3-signature-analysis",
    "href": "tutorials/workload-signatures.html#part-3-signature-analysis",
    "title": "Workload Signatures Tutorial",
    "section": "",
    "text": "# Compare average CPU utilization\ncpu_comparison = []\nfor name, df in signatures.items():\n    cpu_comparison.append({\n        'Workload': name,\n        'Mean CPU': df['cpu_utilization'].mean(),\n        'P50 CPU': df['cpu_utilization'].median(),\n        'P95 CPU': df['cpu_utilization'].quantile(0.95),\n        'Std Dev': df['cpu_utilization'].std()\n    })\n\ncpu_df = pl.DataFrame(cpu_comparison).sort('Mean CPU', descending=True)\nprint(cpu_df)\n\nExpected patterns:\n\n\n\nWorkload Type\nMean CPU\nCharacteristics\n\n\n\n\nML Training\n70-90%\nHigh, sustained utilization\n\n\nHPC\n80-95%\nVery high, consistent\n\n\nBatch Processing\n50-70%\nHigh during job execution\n\n\nDatabase OLAP\n40-60%\nModerate, query-driven\n\n\nDatabase OLTP\n40-60%\nModerate, transaction-driven\n\n\nWeb Application\n15-25%\nLow-moderate, request-driven\n\n\nMicroservices\n20-30%\nModerate, distributed\n\n\nStream Processing\n30-50%\nSteady, data-driven\n\n\nServerless\n10-30%\nSporadic, event-driven\n\n\nContainer\n20-40%\nVariable by application\n\n\nDevelopment\n5-10%\nVery low, intermittent\n\n\n\n\n\n\n\nDifferent workloads show different CPU-memory relationships:\n\n# Calculate correlations\ncorrelations = []\nfor name, df in signatures.items():\n    corr = df.select([\n        pl.corr('cpu_utilization', 'memory_utilization').alias('cpu_mem_corr')\n    ])\n    correlations.append({\n        'Workload': name,\n        'Correlation': corr['cpu_mem_corr'][0]\n    })\n\ncorr_df = pl.DataFrame(correlations).sort('Correlation', descending=True)\n\n# Visualize\nchart = alt.Chart(corr_df.to_pandas()).mark_bar().encode(\n    x=alt.X('Correlation:Q', scale=alt.Scale(domain=[-0.5, 1.0])),\n    y=alt.Y('Workload:N', sort='-x'),\n    color=alt.condition(\n        alt.datum.Correlation &gt; 0.7,\n        alt.value('steelblue'),\n        alt.value('lightcoral')\n    ),\n    tooltip=['Workload', 'Correlation']\n).properties(\n    width=600,\n    height=400,\n    title='CPU-Memory Correlation by Workload Type'\n)\n\nchart\n\nInterpretation:\n\nHigh correlation (&gt;0.8): Database OLAP, ML Training\n\nMemory usage scales with computation\nLarge working sets in memory\n\nModerate correlation (0.4-0.7): Web Apps, Microservices\n\nSome coupling, but not strict\nCaching affects the relationship\n\nLow correlation (&lt;0.4): Serverless, Batch Processing\n\nMemory needs independent of CPU\nDistinct phases (I/O vs. compute)",
    "crumbs": [
      "Home",
      "Tutorials",
      "Workload Signatures Tutorial"
    ]
  },
  {
    "objectID": "tutorials/workload-signatures.html#part-4-temporal-patterns",
    "href": "tutorials/workload-signatures.html#part-4-temporal-patterns",
    "title": "Workload Signatures Tutorial",
    "section": "",
    "text": "# Analyze hourly patterns for web application\nweb_app = signatures[\"Web Application\"]\n\n# Group by hour of day\nhourly = web_app.with_columns(\n    pl.col('timestamp').dt.hour().alias('hour')\n).group_by('hour').agg([\n    pl.col('cpu_utilization').mean().alias('avg_cpu'),\n    pl.col('memory_utilization').mean().alias('avg_memory'),\n    pl.col('network_in_mbps').mean().alias('avg_network')\n]).sort('hour')\n\n# Visualize\nbase = alt.Chart(hourly.to_pandas()).encode(\n    x=alt.X('hour:O', title='Hour of Day')\n)\n\ncpu_line = base.mark_line(color='steelblue').encode(\n    y=alt.Y('avg_cpu:Q', title='CPU Utilization (%)')\n)\n\nmemory_line = base.mark_line(color='coral').encode(\n    y=alt.Y('avg_memory:Q', title='Memory Utilization (%)')\n)\n\nchart = alt.layer(cpu_line, memory_line).resolve_scale(\n    y='independent'\n).properties(\n    width=700,\n    height=300,\n    title='Web Application: Daily Resource Pattern'\n)\n\nchart\n\n\n\n\n\n\n\nNoteBusiness Hours Effect\n\n\n\nNotice the clear business hours pattern (9 AM - 5 PM):\n\nCPU rises 40-60% during peak hours\nMemory increases more gradually\nNetwork traffic follows CPU closely\n\nThis reflects human-driven workloads.\n\n\n\n\n\n\n\n# Analyze day-of-week patterns\nweekly = web_app.with_columns(\n    pl.col('timestamp').dt.weekday().alias('day_of_week')\n).group_by('day_of_week').agg([\n    pl.col('cpu_utilization').mean().alias('avg_cpu'),\n    pl.col('waste_percentage').mean().alias('avg_waste')\n]).sort('day_of_week')\n\n# Add day names\nday_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\nweekly = weekly.with_columns(\n    pl.Series('day_name', [day_names[i] for i in weekly['day_of_week']])\n)\n\n# Visualize\nchart = alt.Chart(weekly.to_pandas()).mark_bar().encode(\n    x=alt.X('day_name:N', title='Day of Week', sort=day_names),\n    y=alt.Y('avg_cpu:Q', title='Average CPU (%)'),\n    color=alt.condition(\n        alt.datum.day_name.isin(['Sat', 'Sun']),\n        alt.value('lightcoral'),\n        alt.value('steelblue')\n    ),\n    tooltip=['day_name', 'avg_cpu', 'avg_waste']\n).properties(\n    width=600,\n    height=300,\n    title='Weekly Pattern: Weekdays vs. Weekends'\n)\n\nchart\n\nResearch finding: 60-80% drop on weekends for business workloads ✓",
    "crumbs": [
      "Home",
      "Tutorials",
      "Workload Signatures Tutorial"
    ]
  },
  {
    "objectID": "tutorials/workload-signatures.html#part-5-efficiency-waste-patterns",
    "href": "tutorials/workload-signatures.html#part-5-efficiency-waste-patterns",
    "title": "Workload Signatures Tutorial",
    "section": "",
    "text": "Different workloads show different waste characteristics:\n\n# Analyze waste by workload\nwaste_analysis = []\nfor name, df in signatures.items():\n    waste_analysis.append({\n        'Workload': name,\n        'Mean Waste (%)': df['waste_percentage'].mean(),\n        'Idle Hours': df.filter(pl.col('is_idle')).height,\n        'Overprovisioned Hours': df.filter(pl.col('is_overprovisioned')).height,\n        'Total Hours': df.height\n    })\n\nwaste_df = pl.DataFrame(waste_analysis).sort('Mean Waste (%)', descending=True)\nprint(waste_df)\n\n\n\n\n\n\n\nWarningHigh-Waste Workloads\n\n\n\nDevelopment Environments: 70%+ waste - Often idle during non-work hours - Overprovisioned for peak needs - Frequently forgotten after projects\nBatch Processing: 40-50% waste - Resources idle between jobs - Peak provisioning for worst case - Poor scheduling optimization\nServerless: Variable (20-60%) - Cold start overhead - Over-allocation for performance - Granularity mismatches",
    "crumbs": [
      "Home",
      "Tutorials",
      "Workload Signatures Tutorial"
    ]
  },
  {
    "objectID": "tutorials/workload-signatures.html#part-6-autocorrelation-analysis",
    "href": "tutorials/workload-signatures.html#part-6-autocorrelation-analysis",
    "title": "Workload Signatures Tutorial",
    "section": "",
    "text": "Workloads show different temporal memory:\n\nfrom statsmodels.tsa.stattools import acf\n\n# Compare autocorrelation for different workloads\nacf_comparison = {}\nworkloads_to_compare = ['Web Application', 'ML Training', 'Development']\n\nfor name in workloads_to_compare:\n    cpu_values = signatures[name]['cpu_utilization'].to_numpy()\n    autocorr = acf(cpu_values, nlags=24)  # 24 hours\n    acf_comparison[name] = autocorr\n\n# Create dataframe for visualization\nacf_data = []\nfor name, autocorr in acf_comparison.items():\n    for lag, value in enumerate(autocorr):\n        acf_data.append({'Workload': name, 'Lag (hours)': lag, 'Autocorrelation': value})\n\nacf_df = pl.DataFrame(acf_data)\n\n# Visualize\nchart = alt.Chart(acf_df.to_pandas()).mark_line().encode(\n    x=alt.X('Lag (hours):Q'),\n    y=alt.Y('Autocorrelation:Q'),\n    color='Workload:N',\n    tooltip=['Workload', 'Lag (hours)', 'Autocorrelation']\n).properties(\n    width=700,\n    height=300,\n    title='Temporal Autocorrelation by Workload Type'\n)\n\nchart\n\nObservations:\n\nWeb Application: Strong daily cycle (lag 24)\nML Training: Sustained pattern (slow decay)\nDevelopment: Weak patterns (irregular usage)",
    "crumbs": [
      "Home",
      "Tutorials",
      "Workload Signatures Tutorial"
    ]
  },
  {
    "objectID": "tutorials/workload-signatures.html#summary",
    "href": "tutorials/workload-signatures.html#summary",
    "title": "Workload Signatures Tutorial",
    "section": "",
    "text": "In this tutorial, you learned:\n✅ Why signatures exist: Hardware, architecture, business forces ✅ 12+ workload archetypes: Their distinct patterns ✅ Resource correlations: CPU-memory relationships ✅ Temporal patterns: Daily, weekly, seasonal ✅ Efficiency analysis: Waste and optimization opportunities",
    "crumbs": [
      "Home",
      "Tutorials",
      "Workload Signatures Tutorial"
    ]
  },
  {
    "objectID": "tutorials/workload-signatures.html#next-steps",
    "href": "tutorials/workload-signatures.html#next-steps",
    "title": "Workload Signatures Tutorial",
    "section": "",
    "text": "Gaussian Process Tutorial: Model these patterns for forecasting\nHow-To: Generate Synthetic Data: Production examples\nConcepts: Research Foundation: Deep dive into empirical findings\n\n\n\n\n\n\n\n\nTipApply What You’ve Learned\n\n\n\nTry analyzing workloads in your own environment:\n\nIdentify the archetype (Web App? Database? Batch?)\nCompare to empirical patterns (CPU, memory, correlation)\nAnalyze temporal cycles (business hours? weekends?)\nCalculate waste and identify opportunities\n\nThe patterns are universal - they apply to real workloads too!",
    "crumbs": [
      "Home",
      "Tutorials",
      "Workload Signatures Tutorial"
    ]
  },
  {
    "objectID": "tutorials/data-exploration.html",
    "href": "tutorials/data-exploration.html",
    "title": "Data Exploration Tutorial",
    "section": "",
    "text": "Welcome! In this tutorial, you’ll learn how to generate and explore synthetic cloud resource data using Cloud Resource Simulator’s empirically-grounded patterns.\n\n\nBy the end of this tutorial, you will be able to:\n\nGenerate realistic cloud workload data for different application types\nValidate simulated data against research findings\nVisualize resource utilization patterns\nIdentify inefficiencies and waste\n\n\n\n\n\n\n\nTipEstimated Time\n\n\n\n15 minutes\n\n\n\n\n\n\nFirst, let’s import the necessary libraries:\n\nimport polars as pl\nimport numpy as np\nimport altair as alt\nfrom datetime import datetime, timedelta\n\nfrom cloud_sim.data_generation import WorkloadPatternGenerator, WorkloadType\n\n\n\n\n\nThe WorkloadPatternGenerator creates time series data based on real-world research showing:\n\n13% average CPU utilization\n20% average memory utilization\n30-32% resource waste\n\nLet’s generate data for different workload archetypes:\n\n# Initialize generator with fixed seed for reproducibility\ngenerator = WorkloadPatternGenerator(seed=42)\n\n# Define workload types to explore\nworkloads = {\n    WorkloadType.WEB_APP: \"Web Application\",\n    WorkloadType.BATCH_PROCESSING: \"Batch Processing\",\n    WorkloadType.ML_TRAINING: \"ML Training\",\n    WorkloadType.DATABASE_OLTP: \"Database OLTP\",\n    WorkloadType.DEV_ENVIRONMENT: \"Development Environment\"\n}\n\n# Generate 7 days of hourly data for each workload\ndata_frames = {}\nfor workload_type, name in workloads.items():\n    df = generator.generate_time_series(\n        workload_type=workload_type,\n        start_time=datetime.now() - timedelta(days=7),\n        end_time=datetime.now(),\n        interval_minutes=60,\n        include_anomalies=True,\n        anomaly_rate=0.02\n    )\n    data_frames[name] = df\n\n    print(f\"\\n{name}:\")\n    print(f\"  CPU Utilization: {df['cpu_utilization'].mean():.1f}%\")\n    print(f\"  Memory Utilization: {df['memory_utilization'].mean():.1f}%\")\n    print(f\"  Waste Percentage: {df['waste_percentage'].mean():.1f}%\")\n\n\n\n\n\n\n\nNoteWhat Gets Generated?\n\n\n\nEach dataset includes:\n\nTemporal patterns: Daily cycles, weekly seasonality\nResource metrics: CPU, memory, network, disk I/O\nDerived metrics: Efficiency scores, waste estimates\nAnomalies: Realistic operational issues (configurable)\n\n\n\n\n\n\n\nLet’s verify our simulation matches empirical findings from cloud infrastructure research:\n\n# Combine all workloads\nall_data = pl.concat(list(data_frames.values()))\n\n# Compare simulation to research\ncomparison = pl.DataFrame({\n    \"Metric\": [\"CPU Utilization\", \"Memory Utilization\", \"Waste Percentage\"],\n    \"Research\": [13.0, 20.0, 31.0],\n    \"Simulation\": [\n        all_data['cpu_utilization'].mean(),\n        all_data['memory_utilization'].mean(),\n        all_data['waste_percentage'].mean()\n    ]\n})\n\nprint(comparison)\n\nExpected output:\n\n\n\nMetric\nResearch\nSimulation\n\n\n\n\nCPU Utilization\n13%\n12-14%\n\n\nMemory Utilization\n20%\n18-22%\n\n\nWaste Percentage\n31%\n29-33%\n\n\n\n\n\n\n\n\n\nTipWhy Close But Not Exact?\n\n\n\nSlight variations are expected because:\n\nWorkload mix: Different proportions of workload types\nTemporal patterns: Some periods have higher/lower utilization\nStochastic elements: Realistic noise and variations\n\nThe key is that overall patterns match research findings.\n\n\n\n\n\n\nLet’s explore how different workloads behave over time using Altair:\n\n# Prepare data for visualization\nviz_data = []\nfor name, df in data_frames.items():\n    subset = df.select(['timestamp', 'cpu_utilization', 'memory_utilization'])\n    subset = subset.with_columns(pl.lit(name).alias('workload'))\n    viz_data.append(subset)\n\ncombined = pl.concat(viz_data)\n\n# Create interactive time series plot\nchart = alt.Chart(combined.to_pandas()).mark_line().encode(\n    x=alt.X('timestamp:T', title='Time'),\n    y=alt.Y('cpu_utilization:Q', title='CPU Utilization (%)'),\n    color=alt.Color('workload:N', title='Workload Type'),\n    tooltip=['timestamp:T', 'cpu_utilization:Q', 'workload:N']\n).properties(\n    width=700,\n    height=400,\n    title='CPU Utilization Over Time by Workload Type'\n).interactive()\n\nchart\n\n\n\n\nWeb Application: Moderate utilization with daily patterns\nML Training: High, sustained utilization with occasional bursts\nBatch Processing: Periodic spikes as jobs run\nDatabase OLTP: Steady utilization during business hours\nDev Environment: Very low utilization (often idle)\n\n\n\n\n\n\nOne of the simulator’s key features is waste estimation. Let’s analyze inefficiencies:\n\n# Calculate waste statistics by workload\nwaste_stats = []\nfor name, df in data_frames.items():\n    stats = {\n        'workload': name,\n        'mean_waste': df['waste_percentage'].mean(),\n        'p50_waste': df['waste_percentage'].median(),\n        'p95_waste': df['waste_percentage'].quantile(0.95),\n        'idle_hours': df.filter(pl.col('is_idle')).height,\n        'overprovisioned_hours': df.filter(pl.col('is_overprovisioned')).height\n    }\n    waste_stats.append(stats)\n\nwaste_df = pl.DataFrame(waste_stats)\nprint(waste_df)\n\n\n\n\n\n\n\nWarningDevelopment Environments\n\n\n\nNotice how dev environments show 70%+ waste!\nThis matches research findings - development workloads are often:\n\nLeft running when not in use\nOverprovisioned for peak needs\nForgotten after projects complete\n\nThis represents a major cost optimization opportunity.\n\n\n\n\n\n\nCloud workloads show strong temporal patterns. Let’s verify this:\n\nfrom statsmodels.tsa.stattools import acf\n\n# Calculate autocorrelation for web app workload\nweb_app_df = data_frames[\"Web Application\"]\ncpu_values = web_app_df['cpu_utilization'].to_numpy()\n\n# Compute autocorrelation for first 24 lags (1 day)\nlags = 24\nautocorr = acf(cpu_values, nlags=lags)\n\n# Visualize\nacf_df = pl.DataFrame({\n    'lag': range(lags + 1),\n    'autocorrelation': autocorr\n})\n\nchart = alt.Chart(acf_df.to_pandas()).mark_bar().encode(\n    x=alt.X('lag:O', title='Lag (hours)'),\n    y=alt.Y('autocorrelation:Q', title='Autocorrelation'),\n    color=alt.condition(\n        alt.datum.autocorrelation &gt; 0.5,\n        alt.value('steelblue'),\n        alt.value('lightgray')\n    )\n).properties(\n    width=700,\n    height=300,\n    title='Autocorrelation Function - Web Application CPU'\n)\n\nchart\n\nResearch expectation: 0.7-0.8 autocorrelation for first 10 lags ✓\n\n\n\n\nIn this tutorial, you learned:\n✅ How to generate realistic cloud workload data ✅ Different workload archetypes and their patterns ✅ How to validate against empirical research ✅ Resource waste analysis techniques ✅ Temporal pattern exploration\n\n\n\n\nWorkload Signatures Tutorial: Deep dive into archetype characteristics\nGaussian Process Tutorial: Build forecasting models\nHow-To: Generate Synthetic Data: Production-ready examples\n\n\n\n\n\n\n\n\nTipWant to Experiment?\n\n\n\nTry modifying the code to:\n\nGenerate data for different time ranges (30 days, 90 days)\nAdjust the anomaly rate (0.01 = 1%, 0.05 = 5%)\nCompare weekend vs. weekday patterns\nAnalyze network I/O correlations with CPU\n\nThe framework is designed for exploration!",
    "crumbs": [
      "Home",
      "Tutorials",
      "Data Exploration Tutorial"
    ]
  },
  {
    "objectID": "tutorials/data-exploration.html#learning-objectives",
    "href": "tutorials/data-exploration.html#learning-objectives",
    "title": "Data Exploration Tutorial",
    "section": "",
    "text": "By the end of this tutorial, you will be able to:\n\nGenerate realistic cloud workload data for different application types\nValidate simulated data against research findings\nVisualize resource utilization patterns\nIdentify inefficiencies and waste\n\n\n\n\n\n\n\nTipEstimated Time\n\n\n\n15 minutes",
    "crumbs": [
      "Home",
      "Tutorials",
      "Data Exploration Tutorial"
    ]
  },
  {
    "objectID": "tutorials/data-exploration.html#setup",
    "href": "tutorials/data-exploration.html#setup",
    "title": "Data Exploration Tutorial",
    "section": "",
    "text": "First, let’s import the necessary libraries:\n\nimport polars as pl\nimport numpy as np\nimport altair as alt\nfrom datetime import datetime, timedelta\n\nfrom cloud_sim.data_generation import WorkloadPatternGenerator, WorkloadType",
    "crumbs": [
      "Home",
      "Tutorials",
      "Data Exploration Tutorial"
    ]
  },
  {
    "objectID": "tutorials/data-exploration.html#generate-realistic-workload-data",
    "href": "tutorials/data-exploration.html#generate-realistic-workload-data",
    "title": "Data Exploration Tutorial",
    "section": "",
    "text": "The WorkloadPatternGenerator creates time series data based on real-world research showing:\n\n13% average CPU utilization\n20% average memory utilization\n30-32% resource waste\n\nLet’s generate data for different workload archetypes:\n\n# Initialize generator with fixed seed for reproducibility\ngenerator = WorkloadPatternGenerator(seed=42)\n\n# Define workload types to explore\nworkloads = {\n    WorkloadType.WEB_APP: \"Web Application\",\n    WorkloadType.BATCH_PROCESSING: \"Batch Processing\",\n    WorkloadType.ML_TRAINING: \"ML Training\",\n    WorkloadType.DATABASE_OLTP: \"Database OLTP\",\n    WorkloadType.DEV_ENVIRONMENT: \"Development Environment\"\n}\n\n# Generate 7 days of hourly data for each workload\ndata_frames = {}\nfor workload_type, name in workloads.items():\n    df = generator.generate_time_series(\n        workload_type=workload_type,\n        start_time=datetime.now() - timedelta(days=7),\n        end_time=datetime.now(),\n        interval_minutes=60,\n        include_anomalies=True,\n        anomaly_rate=0.02\n    )\n    data_frames[name] = df\n\n    print(f\"\\n{name}:\")\n    print(f\"  CPU Utilization: {df['cpu_utilization'].mean():.1f}%\")\n    print(f\"  Memory Utilization: {df['memory_utilization'].mean():.1f}%\")\n    print(f\"  Waste Percentage: {df['waste_percentage'].mean():.1f}%\")\n\n\n\n\n\n\n\nNoteWhat Gets Generated?\n\n\n\nEach dataset includes:\n\nTemporal patterns: Daily cycles, weekly seasonality\nResource metrics: CPU, memory, network, disk I/O\nDerived metrics: Efficiency scores, waste estimates\nAnomalies: Realistic operational issues (configurable)",
    "crumbs": [
      "Home",
      "Tutorials",
      "Data Exploration Tutorial"
    ]
  },
  {
    "objectID": "tutorials/data-exploration.html#validate-against-research",
    "href": "tutorials/data-exploration.html#validate-against-research",
    "title": "Data Exploration Tutorial",
    "section": "",
    "text": "Let’s verify our simulation matches empirical findings from cloud infrastructure research:\n\n# Combine all workloads\nall_data = pl.concat(list(data_frames.values()))\n\n# Compare simulation to research\ncomparison = pl.DataFrame({\n    \"Metric\": [\"CPU Utilization\", \"Memory Utilization\", \"Waste Percentage\"],\n    \"Research\": [13.0, 20.0, 31.0],\n    \"Simulation\": [\n        all_data['cpu_utilization'].mean(),\n        all_data['memory_utilization'].mean(),\n        all_data['waste_percentage'].mean()\n    ]\n})\n\nprint(comparison)\n\nExpected output:\n\n\n\nMetric\nResearch\nSimulation\n\n\n\n\nCPU Utilization\n13%\n12-14%\n\n\nMemory Utilization\n20%\n18-22%\n\n\nWaste Percentage\n31%\n29-33%\n\n\n\n\n\n\n\n\n\nTipWhy Close But Not Exact?\n\n\n\nSlight variations are expected because:\n\nWorkload mix: Different proportions of workload types\nTemporal patterns: Some periods have higher/lower utilization\nStochastic elements: Realistic noise and variations\n\nThe key is that overall patterns match research findings.",
    "crumbs": [
      "Home",
      "Tutorials",
      "Data Exploration Tutorial"
    ]
  },
  {
    "objectID": "tutorials/data-exploration.html#visualize-utilization-patterns",
    "href": "tutorials/data-exploration.html#visualize-utilization-patterns",
    "title": "Data Exploration Tutorial",
    "section": "",
    "text": "Let’s explore how different workloads behave over time using Altair:\n\n# Prepare data for visualization\nviz_data = []\nfor name, df in data_frames.items():\n    subset = df.select(['timestamp', 'cpu_utilization', 'memory_utilization'])\n    subset = subset.with_columns(pl.lit(name).alias('workload'))\n    viz_data.append(subset)\n\ncombined = pl.concat(viz_data)\n\n# Create interactive time series plot\nchart = alt.Chart(combined.to_pandas()).mark_line().encode(\n    x=alt.X('timestamp:T', title='Time'),\n    y=alt.Y('cpu_utilization:Q', title='CPU Utilization (%)'),\n    color=alt.Color('workload:N', title='Workload Type'),\n    tooltip=['timestamp:T', 'cpu_utilization:Q', 'workload:N']\n).properties(\n    width=700,\n    height=400,\n    title='CPU Utilization Over Time by Workload Type'\n).interactive()\n\nchart\n\n\n\n\nWeb Application: Moderate utilization with daily patterns\nML Training: High, sustained utilization with occasional bursts\nBatch Processing: Periodic spikes as jobs run\nDatabase OLTP: Steady utilization during business hours\nDev Environment: Very low utilization (often idle)",
    "crumbs": [
      "Home",
      "Tutorials",
      "Data Exploration Tutorial"
    ]
  },
  {
    "objectID": "tutorials/data-exploration.html#analyze-resource-waste",
    "href": "tutorials/data-exploration.html#analyze-resource-waste",
    "title": "Data Exploration Tutorial",
    "section": "",
    "text": "One of the simulator’s key features is waste estimation. Let’s analyze inefficiencies:\n\n# Calculate waste statistics by workload\nwaste_stats = []\nfor name, df in data_frames.items():\n    stats = {\n        'workload': name,\n        'mean_waste': df['waste_percentage'].mean(),\n        'p50_waste': df['waste_percentage'].median(),\n        'p95_waste': df['waste_percentage'].quantile(0.95),\n        'idle_hours': df.filter(pl.col('is_idle')).height,\n        'overprovisioned_hours': df.filter(pl.col('is_overprovisioned')).height\n    }\n    waste_stats.append(stats)\n\nwaste_df = pl.DataFrame(waste_stats)\nprint(waste_df)\n\n\n\n\n\n\n\nWarningDevelopment Environments\n\n\n\nNotice how dev environments show 70%+ waste!\nThis matches research findings - development workloads are often:\n\nLeft running when not in use\nOverprovisioned for peak needs\nForgotten after projects complete\n\nThis represents a major cost optimization opportunity.",
    "crumbs": [
      "Home",
      "Tutorials",
      "Data Exploration Tutorial"
    ]
  },
  {
    "objectID": "tutorials/data-exploration.html#explore-temporal-autocorrelation",
    "href": "tutorials/data-exploration.html#explore-temporal-autocorrelation",
    "title": "Data Exploration Tutorial",
    "section": "",
    "text": "Cloud workloads show strong temporal patterns. Let’s verify this:\n\nfrom statsmodels.tsa.stattools import acf\n\n# Calculate autocorrelation for web app workload\nweb_app_df = data_frames[\"Web Application\"]\ncpu_values = web_app_df['cpu_utilization'].to_numpy()\n\n# Compute autocorrelation for first 24 lags (1 day)\nlags = 24\nautocorr = acf(cpu_values, nlags=lags)\n\n# Visualize\nacf_df = pl.DataFrame({\n    'lag': range(lags + 1),\n    'autocorrelation': autocorr\n})\n\nchart = alt.Chart(acf_df.to_pandas()).mark_bar().encode(\n    x=alt.X('lag:O', title='Lag (hours)'),\n    y=alt.Y('autocorrelation:Q', title='Autocorrelation'),\n    color=alt.condition(\n        alt.datum.autocorrelation &gt; 0.5,\n        alt.value('steelblue'),\n        alt.value('lightgray')\n    )\n).properties(\n    width=700,\n    height=300,\n    title='Autocorrelation Function - Web Application CPU'\n)\n\nchart\n\nResearch expectation: 0.7-0.8 autocorrelation for first 10 lags ✓",
    "crumbs": [
      "Home",
      "Tutorials",
      "Data Exploration Tutorial"
    ]
  },
  {
    "objectID": "tutorials/data-exploration.html#summary",
    "href": "tutorials/data-exploration.html#summary",
    "title": "Data Exploration Tutorial",
    "section": "",
    "text": "In this tutorial, you learned:\n✅ How to generate realistic cloud workload data ✅ Different workload archetypes and their patterns ✅ How to validate against empirical research ✅ Resource waste analysis techniques ✅ Temporal pattern exploration",
    "crumbs": [
      "Home",
      "Tutorials",
      "Data Exploration Tutorial"
    ]
  },
  {
    "objectID": "tutorials/data-exploration.html#next-steps",
    "href": "tutorials/data-exploration.html#next-steps",
    "title": "Data Exploration Tutorial",
    "section": "",
    "text": "Workload Signatures Tutorial: Deep dive into archetype characteristics\nGaussian Process Tutorial: Build forecasting models\nHow-To: Generate Synthetic Data: Production-ready examples\n\n\n\n\n\n\n\n\nTipWant to Experiment?\n\n\n\nTry modifying the code to:\n\nGenerate data for different time ranges (30 days, 90 days)\nAdjust the anomaly rate (0.01 = 1%, 0.05 = 5%)\nCompare weekend vs. weekday patterns\nAnalyze network I/O correlations with CPU\n\nThe framework is designed for exploration!",
    "crumbs": [
      "Home",
      "Tutorials",
      "Data Exploration Tutorial"
    ]
  },
  {
    "objectID": "concepts/index.html",
    "href": "concepts/index.html",
    "title": "Concepts",
    "section": "",
    "text": "Understand the theory, research, and design decisions behind Cloud Resource Simulator.\n\n\nOur simulator is built on extensive empirical research analyzing real-world cloud infrastructure patterns.\n\n\nComprehensive analysis of industry-wide cloud resource utilization statistics.\nKey findings: - 13% average CPU utilization across cloud infrastructure - 30-32% waste in cloud spending - Workload-specific inefficiency patterns - 35+ research citations\n\n\n\n\nEmpirical correlation matrices between cloud resource metrics.\nKey findings: - Temporal autocorrelation: 0.7-0.8 for first 10 lags - CPU-Memory correlations vary by workload (0.2-0.95) - Network-CPU high correlation in web apps (0.7-0.8)\n\n\n\n\nCurated catalog of HuggingFace datasets for cloud anomaly detection.\nTop recommendations: - AutonLab/Timeseries-PILE - Lemma-RCA-NEC/Cloud_Computing - Energy domain datasets as cloud infrastructure analogs\n\n\n\n\nAssessment of Stanford’s OpenTSLM for cloud time series forecasting.\nKey finding: Not suitable for cloud anomaly detection (medical domain focus, no pre-trained weights)\n\n\n\n\n\n\n\nComplete design specification for our GP-based forecasting system.\nTopics covered: - Sparse variational approximations - Composite periodic kernels - Student-t likelihood for robustness - 92% test coverage implementation\n\n\n\n\n\nThe Cloud Resource Simulator implements a multi-model approach:\n\nGaussian Processes: Time series forecasting with uncertainty quantification\nBayesian Hierarchical Models: Industry→Application→Resource hierarchy\nFoundation Models: Zero-shot forecasting (Chronos, TimesFM)\n\nCore principle: Empirically-grounded simulation based on real-world research, not assumptions.\n\n\n\n\nTutorials - Hands-on learning guides\nHow-To Guides - Task-oriented instructions\nAPI Reference - Complete API documentation",
    "crumbs": [
      "Home",
      "Concepts",
      "Concepts"
    ]
  },
  {
    "objectID": "concepts/index.html#research-foundation",
    "href": "concepts/index.html#research-foundation",
    "title": "Concepts",
    "section": "",
    "text": "Our simulator is built on extensive empirical research analyzing real-world cloud infrastructure patterns.\n\n\nComprehensive analysis of industry-wide cloud resource utilization statistics.\nKey findings: - 13% average CPU utilization across cloud infrastructure - 30-32% waste in cloud spending - Workload-specific inefficiency patterns - 35+ research citations\n\n\n\n\nEmpirical correlation matrices between cloud resource metrics.\nKey findings: - Temporal autocorrelation: 0.7-0.8 for first 10 lags - CPU-Memory correlations vary by workload (0.2-0.95) - Network-CPU high correlation in web apps (0.7-0.8)\n\n\n\n\nCurated catalog of HuggingFace datasets for cloud anomaly detection.\nTop recommendations: - AutonLab/Timeseries-PILE - Lemma-RCA-NEC/Cloud_Computing - Energy domain datasets as cloud infrastructure analogs\n\n\n\n\nAssessment of Stanford’s OpenTSLM for cloud time series forecasting.\nKey finding: Not suitable for cloud anomaly detection (medical domain focus, no pre-trained weights)",
    "crumbs": [
      "Home",
      "Concepts",
      "Concepts"
    ]
  },
  {
    "objectID": "concepts/index.html#design-documentation",
    "href": "concepts/index.html#design-documentation",
    "title": "Concepts",
    "section": "",
    "text": "Complete design specification for our GP-based forecasting system.\nTopics covered: - Sparse variational approximations - Composite periodic kernels - Student-t likelihood for robustness - 92% test coverage implementation",
    "crumbs": [
      "Home",
      "Concepts",
      "Concepts"
    ]
  },
  {
    "objectID": "concepts/index.html#framework-philosophy",
    "href": "concepts/index.html#framework-philosophy",
    "title": "Concepts",
    "section": "",
    "text": "The Cloud Resource Simulator implements a multi-model approach:\n\nGaussian Processes: Time series forecasting with uncertainty quantification\nBayesian Hierarchical Models: Industry→Application→Resource hierarchy\nFoundation Models: Zero-shot forecasting (Chronos, TimesFM)\n\nCore principle: Empirically-grounded simulation based on real-world research, not assumptions.",
    "crumbs": [
      "Home",
      "Concepts",
      "Concepts"
    ]
  },
  {
    "objectID": "concepts/index.html#see-also",
    "href": "concepts/index.html#see-also",
    "title": "Concepts",
    "section": "",
    "text": "Tutorials - Hands-on learning guides\nHow-To Guides - Task-oriented instructions\nAPI Reference - Complete API documentation",
    "crumbs": [
      "Home",
      "Concepts",
      "Concepts"
    ]
  },
  {
    "objectID": "concepts/design/gaussian-process-design.html",
    "href": "concepts/design/gaussian-process-design.html",
    "title": "Gaussian Process Modeling Design Narrative",
    "section": "",
    "text": "Purpose: This document explains the design decisions and modeling philosophy behind our Gaussian Process implementation for cloud resource anomaly detection.\n\n\n\n\n\nBuild production-ready Gaussian Process models for: - Time series forecasting with uncertainty quantification - Anomaly detection via prediction intervals - Pattern learning from operational cloud metrics\n\n\n\n\nTraining: 146,255 samples with 285 labeled anomalies (0.19%)\nTesting: 149,130 samples with 991 labeled anomalies (0.66%)\nKPI: IOPS (I/O operations per second) from production web server\nSource: HuggingFace Timeseries-PILE dataset\n\n\n\n\nThe dataset exhibits a two-scale periodic pattern: - SLOW component: Sawtooth envelope (~1250 timesteps ≈ 21 hours) - FAST component: Sinusoidal carrier (~250 timesteps ≈ 4 hours)\nOperational interpretation: Daily accumulation pattern with overnight resets, plus regular micro-cycles within operational windows.\n\n\n\n\n\n\n\nDesign Choice: Additive kernel structure (not multiplicative)\nK(x1, x2) = K_slow(x1, x2) + K_fast(x1, x2) + K_rbf(x1, x2)\nRationale: 1. Captures multi-scale patterns: Sawtooth × sinusoidal interaction requires two periodic components 2. Numerical stability: Additive structure more stable than multiplicative 3. Fixed lengthscales: Period lengths are domain-driven (1250, 250 steps), not learned 4. Learnable outputscales: Allow model to weight each component’s contribution\nImplementation: CompositePeriodicKernel in src/cloud_sim/ml_models/gaussian_process/kernels.py\nResearch Foundation: - Composite kernels: Rasmussen & Williams (2006), “Gaussian Processes for Machine Learning” - Domain knowledge: EDA revealed specific periods from autocorrelation analysis\n\n\n\n\nDesign Choice: Variational inference with inducing points\nComplexity reduction: O(n³) → O(nm²) where m &lt;&lt; n\nRationale: 1. Scalability: 146K samples require sparse approximation (exact GP infeasible) 2. Inducing points: M=200 points provide sufficient coverage while remaining tractable 3. Learn locations: Inducing points optimized during training (not fixed) 4. Cholesky variational distribution: Full covariance over inducing points\nImplementation: SparseGPModel in src/cloud_sim/ml_models/gaussian_process/models.py\nResearch Foundation: - Sparse GPs (SVGP): Hensman et al. (2013), “Gaussian Processes for Big Data” - GPyTorch library: Production-proven (Uber, Meta, Amazon)\n\n\n\n\nDesign Choice: Student-t likelihood with ν=4\nPhilosophy comparison:\n\n\n\n\n\n\n\n\nAspect\nRobust (Recommended)\nTraditional (Baseline)\n\n\n\n\nTraining Data\nALL 146,255 samples\n145,970 samples (exclude anomalies)\n\n\nLikelihood\nStudent-t (ν=4)\nGaussian\n\n\nPhilosophy\nOutliers are real data\nOutliers corrupt training\n\n\nRobustness\nHeavy tails handle extremes\nAssumes normality\n\n\nProduction\nTrained on operational reality\nTrained on sanitized data\n\n\n\nRationale: 1. Heavy tails: Student-t distribution naturally handles occasional spikes 2. Train on all data: Anomalies are part of operational reality 3. Automatic robustness: Outliers contribute less to parameter learning 4. ν=4: Balanced between robustness and efficiency\nMathematical formulation:\n\\[\np(y | \\mu, \\sigma, \\nu) = \\frac{\\Gamma(\\frac{\\nu+1}{2})}{\\Gamma(\\frac{\\nu}{2})\\sqrt{\\pi\\nu}\\sigma} \\left(1 + \\frac{1}{\\nu}\\left(\\frac{y-\\mu}{\\sigma}\\right)^2\\right)^{-\\frac{\\nu+1}{2}}\n\\]\nImplementation: Compatible with both StudentTLikelihood and GaussianLikelihood\nResearch Foundation: - Student-t Processes: Shah et al. (2014) - Empirical finding: 4× larger variance in anomalous periods (see EDA notebook)\n\n\n\n\n\n\n\nDesign Choice: Batch size 2048 with variational ELBO\nRationale: 1. Memory efficiency: Process large dataset in manageable chunks 2. ELBO objective: Variational lower bound on marginal likelihood 3. Adaptive optimization: Adam optimizer with lr=0.01 4. Convergence: 100 epochs typically sufficient\nTraining complexity: O(nm² * batches) per epoch\n\n\n\n\nDesign Choice: Maximum stability settings\nwith gpytorch.settings.cholesky_jitter(1e-3), \\\n     gpytorch.settings.cholesky_max_tries(10), \\\n     gpytorch.settings.cg_tolerance(1e-2):\n    # Training loop\nRationale: 1. Cholesky jitter: Add 1e-3 to diagonal for PSD guarantee 2. Max tries: Retry Cholesky decomposition up to 10 times 3. CG tolerance: Relaxed conjugate gradient convergence 4. Production-critical: Prevents training crashes from numerical issues\nLesson learned: Initial implementation crashed with NotPSDError. These settings prevent failures.\n\n\n\n\n\n\n\nDesign Choice: Both point accuracy AND uncertainty quality\nPoint accuracy metrics: - RMSE: Root mean squared error - MAE: Mean absolute error - R²: Variance explained\nUncertainty quality metrics: - Coverage: Fraction of points in prediction interval (should match nominal 95%/99%) - Sharpness: Average interval width (narrower is better IF well-calibrated)\nRationale: A good probabilistic forecast must be BOTH accurate AND well-calibrated.\n\n\n\n\nDesign Choice: Detection via prediction intervals\nMethod: Flag anomalies as points outside 95% (or 99%) interval\nanomalies_detected = (y_test &lt; lower_95) | (y_test &gt; upper_95)\nMetrics: - Precision: Fraction of detections that are true anomalies - Recall: Fraction of true anomalies detected - F1-Score: Harmonic mean balancing precision/recall - AUC-ROC: Overall discriminative ability\nThreshold tuning: 95% vs 99% interval trades precision for recall\n\n\n\n\n\n\n\nApple Silicon (MPS) limitation: - ❌ NOT USED for GP training - Reason: GPyTorch requires float64 for Cholesky decomposition, but MPS only supports float32 - Workaround: CPU locally, CUDA GPU in Colab for training\nRecommendation: Train on Google Colab with GPU, deploy models for CPU inference\n\n\n\n\nDesign Choice: Checkpoint includes full state\ncheckpoint = {\n    'model_state_dict': model.state_dict(),\n    'likelihood_state_dict': likelihood.state_dict(),\n    'inducing_points': inducing_points,\n    'losses': training_losses,\n    'final_nu': likelihood.deg_free.item(),  # If Student-t\n    'metadata': {...}\n}\nRationale: Reproducible loading without re-architecture decisions\n\n\n\n\nDesign Choice: Batched predictions (4096 samples per batch)\nRationale: 1. Memory management: 149K test samples cause exhaustion if processed at once 2. Progress tracking: Report every 10 batches 3. Numerical stability: Same jitter settings as training\nImplementation: See training.py for batched prediction pattern\n\n\n\n\n\n\n\nfrom cloud_sim.ml_models.gaussian_process import (\n    CompositePeriodicKernel,\n    SparseGPModel,\n    initialize_inducing_points,\n    train_gp_model,\n    save_model,\n    load_model,\n)\nimport gpytorch\n\n# Initialize inducing points\ninducing_points = initialize_inducing_points(\n    X_train=X_train_norm,\n    num_inducing=200,\n    method=\"evenly_spaced\"\n)\n\n# Create model\nmodel = SparseGPModel(\n    inducing_points=inducing_points,\n    slow_period=1250 / X_range,\n    fast_period=250 / X_range,\n    rbf_lengthscale=0.1\n)\n\n# Create likelihood (robust approach)\nlikelihood = gpytorch.likelihoods.StudentTLikelihood(\n    deg_free_prior=gpytorch.priors.NormalPrior(4.0, 1.0)\n)\n\n# Train\nlosses = train_gp_model(\n    model=model,\n    likelihood=likelihood,\n    X_train=X_train,\n    y_train=y_train,\n    n_epochs=100,\n    batch_size=2048\n)\n\n# Save\nsave_model(\n    model=model,\n    likelihood=likelihood,\n    save_path=\"../models/gp_robust_model.pth\",\n    losses=losses\n)\n\n\n\nfrom cloud_sim.ml_models.gaussian_process import (\n    compute_metrics,\n    compute_anomaly_metrics,\n    compute_prediction_intervals,\n)\n\n# Compute prediction intervals\nintervals = compute_prediction_intervals(\n    mean=predictions_mean,\n    std=predictions_std,\n    confidence_levels=[0.95, 0.99],\n    distribution=\"student_t\",\n    nu=4.0\n)\n\nlower_95, upper_95 = intervals[0.95]\nlower_99, upper_99 = intervals[0.99]\n\n# Evaluate accuracy + calibration\nmetrics = compute_metrics(\n    y_true=y_test,\n    y_pred=predictions_mean,\n    lower_95=lower_95,\n    upper_95=upper_95,\n    lower_99=lower_99,\n    upper_99=upper_99,\n    model_name=\"Robust GP\"\n)\n\n# Evaluate anomaly detection\nanomalies_pred = (y_test &lt; lower_95) | (y_test &gt; upper_95)\n\nanomaly_metrics = compute_anomaly_metrics(\n    y_true_anomaly=anomaly_labels,\n    y_pred_anomaly=anomalies_pred,\n    model_name=\"Robust GP\",\n    threshold_name=\"95% Interval\"\n)\n\n\n\n\n\n\n\nProblem encountered: Training crashed with NotPSDError on Cholesky decomposition\nSolution: Maximum jitter settings + retry logic\nTakeaway: Always use stability settings in production GP training\n\n\n\n\nProblem encountered: Kernel crashed predicting on 149K samples at once\nSolution: Process in batches of 4096 with progress tracking\nTakeaway: Never process full test set at once in production\n\n\n\n\nConventional wisdom: Remove outliers before training\nOur approach: Train on all data with robust likelihood\nResult: Better calibration, more realistic uncertainty\nTakeaway: Student-t likelihood eliminates need for pre-filtering\n\n\n\n\n\n\n\nsrc/cloud_sim/ml_models/gaussian_process/\n├── __init__.py            # Public API exports\n├── kernels.py             # CompositePeriodicKernel\n├── models.py              # SparseGPModel + inducing point utils\n├── training.py            # train_gp_model, save_model, load_model\n└── evaluation.py          # Comprehensive metrics\n\n\n\ntests/ml_models/\n├── __init__.py\n├── test_gaussian_process_kernels.py      # 18 tests, 100% coverage\n├── test_gaussian_process_models.py       # 19 tests, 100% coverage\n├── test_gaussian_process_evaluation.py   # 19 tests, 100% coverage\n└── test_gaussian_process_training.py     # 11 tests, 84% coverage\nOverall GP module coverage: 92% (exceeds 70% requirement)\n\n\n\n\n\n\n\n\nGPyTorch: https://gpytorch.ai/\nSparse GPs (SVGP): Hensman et al. (2013), “Gaussian Processes for Big Data”\nStudent-t Processes: Shah et al. (2014)\nComposite Kernels: Rasmussen & Williams (2006), “Gaussian Processes for Machine Learning”\n\n\n\n\n\nHuggingFace Timeseries-PILE: https://huggingface.co/datasets/AutonLab/Timeseries-PILE\nIOPS Web Server KPI: KPI-05f10d3a-239c-3bef-9bdc-a2feeb0037aa\n\n\n\n\n\nEDA Notebook: notebooks/03_iops_web_server_eda.ipynb (pattern discovery)\nGP Notebook (runbook): notebooks/04_gaussian_process_modeling.md (demonstrates library usage)\nResearch Foundation: docs/research/timeseries-anomaly-datasets-review.md\n\n\nNext Steps: - Deploy model via FastAPI endpoint - Integrate with monitoring dashboard - A/B test against existing anomaly detection - Collect feedback from operations team",
    "crumbs": [
      "Home",
      "Concepts",
      "Design",
      "Gaussian Process Modeling Design Narrative"
    ]
  },
  {
    "objectID": "concepts/design/gaussian-process-design.html#problem-context",
    "href": "concepts/design/gaussian-process-design.html#problem-context",
    "title": "Gaussian Process Modeling Design Narrative",
    "section": "",
    "text": "Build production-ready Gaussian Process models for: - Time series forecasting with uncertainty quantification - Anomaly detection via prediction intervals - Pattern learning from operational cloud metrics\n\n\n\n\nTraining: 146,255 samples with 285 labeled anomalies (0.19%)\nTesting: 149,130 samples with 991 labeled anomalies (0.66%)\nKPI: IOPS (I/O operations per second) from production web server\nSource: HuggingFace Timeseries-PILE dataset\n\n\n\n\nThe dataset exhibits a two-scale periodic pattern: - SLOW component: Sawtooth envelope (~1250 timesteps ≈ 21 hours) - FAST component: Sinusoidal carrier (~250 timesteps ≈ 4 hours)\nOperational interpretation: Daily accumulation pattern with overnight resets, plus regular micro-cycles within operational windows.",
    "crumbs": [
      "Home",
      "Concepts",
      "Design",
      "Gaussian Process Modeling Design Narrative"
    ]
  },
  {
    "objectID": "concepts/design/gaussian-process-design.html#architecture-decisions",
    "href": "concepts/design/gaussian-process-design.html#architecture-decisions",
    "title": "Gaussian Process Modeling Design Narrative",
    "section": "",
    "text": "Design Choice: Additive kernel structure (not multiplicative)\nK(x1, x2) = K_slow(x1, x2) + K_fast(x1, x2) + K_rbf(x1, x2)\nRationale: 1. Captures multi-scale patterns: Sawtooth × sinusoidal interaction requires two periodic components 2. Numerical stability: Additive structure more stable than multiplicative 3. Fixed lengthscales: Period lengths are domain-driven (1250, 250 steps), not learned 4. Learnable outputscales: Allow model to weight each component’s contribution\nImplementation: CompositePeriodicKernel in src/cloud_sim/ml_models/gaussian_process/kernels.py\nResearch Foundation: - Composite kernels: Rasmussen & Williams (2006), “Gaussian Processes for Machine Learning” - Domain knowledge: EDA revealed specific periods from autocorrelation analysis\n\n\n\n\nDesign Choice: Variational inference with inducing points\nComplexity reduction: O(n³) → O(nm²) where m &lt;&lt; n\nRationale: 1. Scalability: 146K samples require sparse approximation (exact GP infeasible) 2. Inducing points: M=200 points provide sufficient coverage while remaining tractable 3. Learn locations: Inducing points optimized during training (not fixed) 4. Cholesky variational distribution: Full covariance over inducing points\nImplementation: SparseGPModel in src/cloud_sim/ml_models/gaussian_process/models.py\nResearch Foundation: - Sparse GPs (SVGP): Hensman et al. (2013), “Gaussian Processes for Big Data” - GPyTorch library: Production-proven (Uber, Meta, Amazon)\n\n\n\n\nDesign Choice: Student-t likelihood with ν=4\nPhilosophy comparison:\n\n\n\n\n\n\n\n\nAspect\nRobust (Recommended)\nTraditional (Baseline)\n\n\n\n\nTraining Data\nALL 146,255 samples\n145,970 samples (exclude anomalies)\n\n\nLikelihood\nStudent-t (ν=4)\nGaussian\n\n\nPhilosophy\nOutliers are real data\nOutliers corrupt training\n\n\nRobustness\nHeavy tails handle extremes\nAssumes normality\n\n\nProduction\nTrained on operational reality\nTrained on sanitized data\n\n\n\nRationale: 1. Heavy tails: Student-t distribution naturally handles occasional spikes 2. Train on all data: Anomalies are part of operational reality 3. Automatic robustness: Outliers contribute less to parameter learning 4. ν=4: Balanced between robustness and efficiency\nMathematical formulation:\n\\[\np(y | \\mu, \\sigma, \\nu) = \\frac{\\Gamma(\\frac{\\nu+1}{2})}{\\Gamma(\\frac{\\nu}{2})\\sqrt{\\pi\\nu}\\sigma} \\left(1 + \\frac{1}{\\nu}\\left(\\frac{y-\\mu}{\\sigma}\\right)^2\\right)^{-\\frac{\\nu+1}{2}}\n\\]\nImplementation: Compatible with both StudentTLikelihood and GaussianLikelihood\nResearch Foundation: - Student-t Processes: Shah et al. (2014) - Empirical finding: 4× larger variance in anomalous periods (see EDA notebook)",
    "crumbs": [
      "Home",
      "Concepts",
      "Design",
      "Gaussian Process Modeling Design Narrative"
    ]
  },
  {
    "objectID": "concepts/design/gaussian-process-design.html#training-strategy-training.py",
    "href": "concepts/design/gaussian-process-design.html#training-strategy-training.py",
    "title": "Gaussian Process Modeling Design Narrative",
    "section": "",
    "text": "Design Choice: Batch size 2048 with variational ELBO\nRationale: 1. Memory efficiency: Process large dataset in manageable chunks 2. ELBO objective: Variational lower bound on marginal likelihood 3. Adaptive optimization: Adam optimizer with lr=0.01 4. Convergence: 100 epochs typically sufficient\nTraining complexity: O(nm² * batches) per epoch\n\n\n\n\nDesign Choice: Maximum stability settings\nwith gpytorch.settings.cholesky_jitter(1e-3), \\\n     gpytorch.settings.cholesky_max_tries(10), \\\n     gpytorch.settings.cg_tolerance(1e-2):\n    # Training loop\nRationale: 1. Cholesky jitter: Add 1e-3 to diagonal for PSD guarantee 2. Max tries: Retry Cholesky decomposition up to 10 times 3. CG tolerance: Relaxed conjugate gradient convergence 4. Production-critical: Prevents training crashes from numerical issues\nLesson learned: Initial implementation crashed with NotPSDError. These settings prevent failures.",
    "crumbs": [
      "Home",
      "Concepts",
      "Design",
      "Gaussian Process Modeling Design Narrative"
    ]
  },
  {
    "objectID": "concepts/design/gaussian-process-design.html#evaluation-strategy-evaluation.py",
    "href": "concepts/design/gaussian-process-design.html#evaluation-strategy-evaluation.py",
    "title": "Gaussian Process Modeling Design Narrative",
    "section": "",
    "text": "Design Choice: Both point accuracy AND uncertainty quality\nPoint accuracy metrics: - RMSE: Root mean squared error - MAE: Mean absolute error - R²: Variance explained\nUncertainty quality metrics: - Coverage: Fraction of points in prediction interval (should match nominal 95%/99%) - Sharpness: Average interval width (narrower is better IF well-calibrated)\nRationale: A good probabilistic forecast must be BOTH accurate AND well-calibrated.\n\n\n\n\nDesign Choice: Detection via prediction intervals\nMethod: Flag anomalies as points outside 95% (or 99%) interval\nanomalies_detected = (y_test &lt; lower_95) | (y_test &gt; upper_95)\nMetrics: - Precision: Fraction of detections that are true anomalies - Recall: Fraction of true anomalies detected - F1-Score: Harmonic mean balancing precision/recall - AUC-ROC: Overall discriminative ability\nThreshold tuning: 95% vs 99% interval trades precision for recall",
    "crumbs": [
      "Home",
      "Concepts",
      "Design",
      "Gaussian Process Modeling Design Narrative"
    ]
  },
  {
    "objectID": "concepts/design/gaussian-process-design.html#production-deployment-considerations",
    "href": "concepts/design/gaussian-process-design.html#production-deployment-considerations",
    "title": "Gaussian Process Modeling Design Narrative",
    "section": "",
    "text": "Apple Silicon (MPS) limitation: - ❌ NOT USED for GP training - Reason: GPyTorch requires float64 for Cholesky decomposition, but MPS only supports float32 - Workaround: CPU locally, CUDA GPU in Colab for training\nRecommendation: Train on Google Colab with GPU, deploy models for CPU inference\n\n\n\n\nDesign Choice: Checkpoint includes full state\ncheckpoint = {\n    'model_state_dict': model.state_dict(),\n    'likelihood_state_dict': likelihood.state_dict(),\n    'inducing_points': inducing_points,\n    'losses': training_losses,\n    'final_nu': likelihood.deg_free.item(),  # If Student-t\n    'metadata': {...}\n}\nRationale: Reproducible loading without re-architecture decisions\n\n\n\n\nDesign Choice: Batched predictions (4096 samples per batch)\nRationale: 1. Memory management: 149K test samples cause exhaustion if processed at once 2. Progress tracking: Report every 10 batches 3. Numerical stability: Same jitter settings as training\nImplementation: See training.py for batched prediction pattern",
    "crumbs": [
      "Home",
      "Concepts",
      "Design",
      "Gaussian Process Modeling Design Narrative"
    ]
  },
  {
    "objectID": "concepts/design/gaussian-process-design.html#library-usage-examples",
    "href": "concepts/design/gaussian-process-design.html#library-usage-examples",
    "title": "Gaussian Process Modeling Design Narrative",
    "section": "",
    "text": "from cloud_sim.ml_models.gaussian_process import (\n    CompositePeriodicKernel,\n    SparseGPModel,\n    initialize_inducing_points,\n    train_gp_model,\n    save_model,\n    load_model,\n)\nimport gpytorch\n\n# Initialize inducing points\ninducing_points = initialize_inducing_points(\n    X_train=X_train_norm,\n    num_inducing=200,\n    method=\"evenly_spaced\"\n)\n\n# Create model\nmodel = SparseGPModel(\n    inducing_points=inducing_points,\n    slow_period=1250 / X_range,\n    fast_period=250 / X_range,\n    rbf_lengthscale=0.1\n)\n\n# Create likelihood (robust approach)\nlikelihood = gpytorch.likelihoods.StudentTLikelihood(\n    deg_free_prior=gpytorch.priors.NormalPrior(4.0, 1.0)\n)\n\n# Train\nlosses = train_gp_model(\n    model=model,\n    likelihood=likelihood,\n    X_train=X_train,\n    y_train=y_train,\n    n_epochs=100,\n    batch_size=2048\n)\n\n# Save\nsave_model(\n    model=model,\n    likelihood=likelihood,\n    save_path=\"../models/gp_robust_model.pth\",\n    losses=losses\n)\n\n\n\nfrom cloud_sim.ml_models.gaussian_process import (\n    compute_metrics,\n    compute_anomaly_metrics,\n    compute_prediction_intervals,\n)\n\n# Compute prediction intervals\nintervals = compute_prediction_intervals(\n    mean=predictions_mean,\n    std=predictions_std,\n    confidence_levels=[0.95, 0.99],\n    distribution=\"student_t\",\n    nu=4.0\n)\n\nlower_95, upper_95 = intervals[0.95]\nlower_99, upper_99 = intervals[0.99]\n\n# Evaluate accuracy + calibration\nmetrics = compute_metrics(\n    y_true=y_test,\n    y_pred=predictions_mean,\n    lower_95=lower_95,\n    upper_95=upper_95,\n    lower_99=lower_99,\n    upper_99=upper_99,\n    model_name=\"Robust GP\"\n)\n\n# Evaluate anomaly detection\nanomalies_pred = (y_test &lt; lower_95) | (y_test &gt; upper_95)\n\nanomaly_metrics = compute_anomaly_metrics(\n    y_true_anomaly=anomaly_labels,\n    y_pred_anomaly=anomalies_pred,\n    model_name=\"Robust GP\",\n    threshold_name=\"95% Interval\"\n)",
    "crumbs": [
      "Home",
      "Concepts",
      "Design",
      "Gaussian Process Modeling Design Narrative"
    ]
  },
  {
    "objectID": "concepts/design/gaussian-process-design.html#lessons-learned",
    "href": "concepts/design/gaussian-process-design.html#lessons-learned",
    "title": "Gaussian Process Modeling Design Narrative",
    "section": "",
    "text": "Problem encountered: Training crashed with NotPSDError on Cholesky decomposition\nSolution: Maximum jitter settings + retry logic\nTakeaway: Always use stability settings in production GP training\n\n\n\n\nProblem encountered: Kernel crashed predicting on 149K samples at once\nSolution: Process in batches of 4096 with progress tracking\nTakeaway: Never process full test set at once in production\n\n\n\n\nConventional wisdom: Remove outliers before training\nOur approach: Train on all data with robust likelihood\nResult: Better calibration, more realistic uncertainty\nTakeaway: Student-t likelihood eliminates need for pre-filtering",
    "crumbs": [
      "Home",
      "Concepts",
      "Design",
      "Gaussian Process Modeling Design Narrative"
    ]
  },
  {
    "objectID": "concepts/design/gaussian-process-design.html#code-organization",
    "href": "concepts/design/gaussian-process-design.html#code-organization",
    "title": "Gaussian Process Modeling Design Narrative",
    "section": "",
    "text": "src/cloud_sim/ml_models/gaussian_process/\n├── __init__.py            # Public API exports\n├── kernels.py             # CompositePeriodicKernel\n├── models.py              # SparseGPModel + inducing point utils\n├── training.py            # train_gp_model, save_model, load_model\n└── evaluation.py          # Comprehensive metrics\n\n\n\ntests/ml_models/\n├── __init__.py\n├── test_gaussian_process_kernels.py      # 18 tests, 100% coverage\n├── test_gaussian_process_models.py       # 19 tests, 100% coverage\n├── test_gaussian_process_evaluation.py   # 19 tests, 100% coverage\n└── test_gaussian_process_training.py     # 11 tests, 84% coverage\nOverall GP module coverage: 92% (exceeds 70% requirement)",
    "crumbs": [
      "Home",
      "Concepts",
      "Design",
      "Gaussian Process Modeling Design Narrative"
    ]
  },
  {
    "objectID": "concepts/design/gaussian-process-design.html#references",
    "href": "concepts/design/gaussian-process-design.html#references",
    "title": "Gaussian Process Modeling Design Narrative",
    "section": "",
    "text": "GPyTorch: https://gpytorch.ai/\nSparse GPs (SVGP): Hensman et al. (2013), “Gaussian Processes for Big Data”\nStudent-t Processes: Shah et al. (2014)\nComposite Kernels: Rasmussen & Williams (2006), “Gaussian Processes for Machine Learning”\n\n\n\n\n\nHuggingFace Timeseries-PILE: https://huggingface.co/datasets/AutonLab/Timeseries-PILE\nIOPS Web Server KPI: KPI-05f10d3a-239c-3bef-9bdc-a2feeb0037aa\n\n\n\n\n\nEDA Notebook: notebooks/03_iops_web_server_eda.ipynb (pattern discovery)\nGP Notebook (runbook): notebooks/04_gaussian_process_modeling.md (demonstrates library usage)\nResearch Foundation: docs/research/timeseries-anomaly-datasets-review.md\n\n\nNext Steps: - Deploy model via FastAPI endpoint - Integrate with monitoring dashboard - A/B test against existing anomaly detection - Collect feedback from operations team",
    "crumbs": [
      "Home",
      "Concepts",
      "Design",
      "Gaussian Process Modeling Design Narrative"
    ]
  },
  {
    "objectID": "concepts/design/library-first-refactoring-plan.html",
    "href": "concepts/design/library-first-refactoring-plan.html",
    "title": "Library-First Refactoring Plan",
    "section": "",
    "text": "Current State: - 75 commits with messy iteration history - Library code exists: src/cloud_sim/{data_generation,ml_models} - Notebooks mix implementation AND educational content - GP model code lives IN notebook (04_gaussian_process_modeling.md) - No TimesFM/Chronos integration yet - ETL module empty after Alibaba/Google cleanup\nExecution Environments: 1. Local: Development, light inference, visualization (M4 CPU, no GPU) 2. Google Colab: GP training, TimesFM, large ETL (Free GPU, Pro $10/mo) 3. HuggingFace Inference API: Production inference (pay-per-use)\n\n\n\n\n\n\nNew files to create:\nsrc/cloud_sim/ml_models/gaussian_process/\n├── __init__.py\n├── kernels.py        # CompositePeriodicKernel, custom kernels\n├── models.py         # SparseGPModel, base classes\n├── training.py       # train_gp, save_model, load_model utilities\n└── evaluation.py     # compute_metrics, compute_anomaly_metrics\nExtract from: notebooks/04_gaussian_process_modeling.md lines 259-937 - CompositePeriodicKernel class → kernels.py - SparseGPModel class → models.py - Training logic → training.py (with batched prediction fixes) - compute_metrics, compute_anomaly_metrics → evaluation.py\n\n\n\nNew files to create:\nsrc/cloud_sim/ml_models/foundation/\n├── __init__.py\n├── timesfm.py        # TimesFM wrapper (Google)\n├── chronos.py        # Chronos wrapper (Amazon)\n└── base.py           # Abstract base class for foundation models\nCapabilities: - Load pre-trained models from HF - Inference API support - Fine-tuning hooks (Colab-only)\n\n\n\nNew files to create:\nsrc/cloud_sim/etl/\n├── __init__.py\n└── cloudzero_loader.py   # Production data loader\nDesign: - Load CloudZero production data samples - Convert to standardized format (Polars DataFrames) - Integrate with HuggingFace dataset publishing\n\n\n\nAdd optional dependency groups:\n[project.optional-dependencies]\ngpu = [\n    \"torch&gt;=2.0.0\",\n    \"gpytorch&gt;=1.11.0\",\n]\n\nfoundation = [\n    \"timesfm @ git+https://github.com/google-research/timesfm\",\n    \"chronos-forecasting&gt;=1.0.0\",\n    \"transformers&gt;=4.35.0\",\n]\n\nall = [\n    \"cloud-resource-simulator[gpu,foundation]\",\n]\n\n\n\n\n\n\n\nnotebooks/\n├── README.md                        # Update with new structure\n├── quickstart/\n│   ├── 01_installation.md          # Setup instructions\n│   ├── 02_basic_usage.md           # Simple library usage\n│   └── 03_data_generation.md       # WorkloadPatternGenerator demo\n├── guides/\n│   ├── workload_patterns.md        # Educational (keep current 02)\n│   ├── gp_inference.md             # Use pre-trained GP models\n│   └── foundation_models.md        # TimesFM/Chronos inference\n├── eda/\n│   ├── iops_web_server_analysis.md # Keep current 03 (research)\n│   └── cloudzero_data_exploration.md # NEW: CloudZero data\n└── training/ (Colab-specific)\n    ├── README.md                    # Colab setup instructions\n    ├── train_gp_colab.ipynb        # GPU training workflow\n    ├── train_timesfm_colab.ipynb   # Foundation model fine-tuning\n    └── etl_pipeline_colab.ipynb    # Large-scale data processing\n\n\n\n01_data_exploration.md → quickstart/03_data_generation.md - Remove implementation details - Focus on library usage: from cloud_sim.data_generation import WorkloadPatternGenerator - Show basic examples\n02_workload_signatures_guide.md → guides/workload_patterns.md - Keep as-is (already educational) - No implementation code to extract\n03_iops_web_server_eda.md → eda/iops_web_server_analysis.md - Keep as-is (research notebook) - Uses HuggingFace dataset (good runbook example)\n04_gaussian_process_modeling.md → Refactor: 1. Extract implementation → src/cloud_sim/ml_models/gaussian_process/ 2. Create guides/gp_inference.md - Load pre-trained models, run inference 3. Create training/train_gp_colab.ipynb - Full training workflow for Colab\n\n\n\nquickstart/01_installation.md:\n# Installation\n\n## Local Development\n`uv pip install -e \".[dev]\"`\n\n## GPU/ML Work (Colab)\n`!pip install cloud-resource-simulator[gpu,foundation]`\n\n## CloudZero Integration\nRequires production data access...\nguides/gp_inference.md:\nfrom cloud_sim.ml_models.gaussian_process import SparseGPModel, load_model\n\n# Load pre-trained model\nmodel = load_model(\"models/gp_robust_model.pth\")\n\n# Run inference\npredictions = model.predict(X_test)\ntraining/train_gp_colab.ipynb:\n# GPU-enabled training in Colab\n# 1. Mount Drive\n# 2. Install with GPU support\n# 3. Train on full dataset\n# 4. Save to Drive/HF Hub\n\n\n\n\n\n\n\nGoal: Squash 75 commits into ~15 clean commits\nTarget structure:\n1. feat: establish project foundation (d0d1e1f - keep as-is)\n2. docs: establish research foundation (fe20c8e - keep as-is)\n3. feat: define simulation architecture (f5709bb - keep as-is)\n4. feat: implement core library (91cf3e3 + squash 10 related commits)\n5. feat: add PyMC Bayesian models (f59f45d + squash related)\n6. feat: add workload taxonomy (squash commits)\n7. docs: add research notebooks (squash all notebook commits)\n8. feat: add GP modeling support (squash GP commits)\n9. test: add comprehensive test suite (squash test commits)\n10. ci: configure GitHub Actions (squash CI commits)\n11. refactor: remove Alibaba/Google datasets (04c055f - keep as-is)\n12. refactor: extract GP implementation to library (NEW)\n13. refactor: convert notebooks to runbooks (NEW)\n14. feat: add foundation model support (NEW)\n15. feat: add CloudZero ETL (NEW)\n\n\n\nOption A: Interactive rebase from foundation\ngit rebase -i d0d1e1f~1  # Interactive from project start\n# Mark commits as 'squash' or 'fixup' to consolidate\nOption B: Reset and recommit (nuclear option)\ngit checkout -b clean-history d0d1e1f  # Branch from foundation\ngit cherry-pick fe20c8e f5709bb 91cf3e3  # Keep foundation commits\n# Then create new clean commits for Phase 1 & 2 work\nRecommendation: Option A (interactive rebase) - Less destructive, preserves authorship\n\n\n\n\n\n\n\nSupported: - Library development - Runbook execution (inference only) - Small-scale testing - Visualization\nNot Supported: - GP training (CPU too slow, MPS incompatibility) - Large ETL pipelines (memory limits) - TimesFM experiments (requires GPU)\n\n\n\nSetup:\n# In Colab notebook\n!pip install cloud-resource-simulator[gpu,foundation]\n\n# Mount Drive for model persistence\nfrom google.colab import drive\ndrive.mount('/content/drive')\nWorkflow: 1. Train GP models in training/train_gp_colab.ipynb 2. Save trained models to Google Drive 3. Download to models/ directory for local inference 4. OR upload to HuggingFace Model Hub\nCost: Free tier (12hr session, T4 GPU) or Pro ($10/mo, better GPUs)\n\n\n\nFor: - Published trained models - Public inference endpoints - Sharing with community\nWorkflow: 1. Train in Colab 2. Push to HF Model Hub: model.push_to_hub(\"username/cloud-gp-model\") 3. Enable Inference API 4. Use in runbooks: from huggingface_hub import InferenceClient\n\n\n\n\n\n\n\n\nTest extracted GP modules: tests/test_gp_models.py\nTest foundation model wrappers: tests/test_foundation_models.py\nTest CloudZero loader: tests/test_cloudzero_etl.py\nRunbook execution tests: Update test_notebooks.py for new structure\n\n\n\n\n\nRun tests on GPU runner for GP tests (GitHub Actions with GPU)\nOR skip GPU tests in CI, document manual testing in Colab\nAdd notebook linting for runbooks\n\n\n\n\n\n\n\n\nA: Google Colab Pro ($10/mo) for GPU access - Local M4 too slow + MPS compatibility issues - Colab provides T4/V100 GPUs, persistent storage via Drive - Can save models to HF Hub for sharing\n\n\n\nA: Three-tier approach: 1. Development: Save to models/ directory (gitignored, local only) 2. Sharing: Push to HuggingFace Model Hub (public/private) 3. Production: HF Inference API or Colab-hosted\n\n\n\nA: Hybrid approach: - Sample data: Local testing with CloudZero production samples - Full pipelines: Colab for large-scale processing - Document both workflows in runbooks\n\n\n\nA: Use cases differ: - GP: Anomaly detection, uncertainty quantification, small-scale - TimesFM: Zero-shot forecasting, Google’s latest, requires GPU - Chronos: Probabilistic forecasting, Amazon’s model, lighter weight\nSupport all three, let runbooks demonstrate each\n\n\n\nA: Do Phase 1-2 first, THEN rebase: 1. Extract implementation to library (Phase 1) 2. Convert notebooks to runbooks (Phase 2) 3. Test everything works 4. Interactive rebase to clean history (Phase 3) 5. Force push to clean remote history\nWhy: Easier to debug if rebase goes wrong, can always revert\n\n\n\n\n\n\n\n\nCreate GP module structure\nExtract code from notebook 04\nAdd training utilities\nUpdate tests\nVerify notebook still works with imports\n\n\n\n\n\nAdd foundation model wrappers (stub implementations first)\nAdd CloudZero ETL module\nUpdate pyproject.toml dependencies\nCreate Colab training notebooks\n\n\n\n\n\nRestructure notebooks/ directory\nRefactor existing notebooks\nCreate quickstart guides\nCreate Colab-specific training notebooks\nUpdate documentation\n\n\n\n\n\nInteractive rebase to squash commits\nForce push clean history\nUpdate README/docs to reflect new structure\nVerify CI still passes\nCreate release tag\n\n\n\n\n\n\nRisk 1: Rebase breaks something - Mitigation: Create backup branch before rebase - Recovery: git reflog to restore pre-rebase state\nRisk 2: Model training costs escalate - Mitigation: Start with Colab free tier, monitor usage - Recovery: Can train locally (slow but works)\nRisk 3: HF Inference API limits - Mitigation: Document rate limits, provide local inference fallback - Recovery: Self-host models on Colab\nRisk 4: CloudZero data structure changes - Mitigation: Abstract ETL interface, version loaders - Recovery: Create adapters for different schemas\n\n\n\n\n✅ Library Quality: - All implementation code in src/cloud_sim/ - No implementation in runbooks (only imports) - &gt;70% test coverage maintained\n✅ Runbook Quality: - Can execute all runbooks locally (inference only) - Colab notebooks have clear GPU setup - Educational content preserved\n✅ History Quality: - Clean commit history (&lt;20 commits) - Each commit buildable and testable - Clear commit messages\n✅ Documentation Quality: - README reflects new structure - Installation instructions for local vs Colab - Clear separation of concerns\n\n\n\n\n\nCreate feature branch: git checkout -b refactor/library-first\nStart Phase 1: Extract GP module\nCommit incrementally with tests\nWhen stable, proceed to Phase 2-4\nFinal interactive rebase on feature branch\nReview before force-pushing to main\n\nEstimated Effort: 15-20 hours over 2-4 weeks\n\n\n\n\n\n\nPrimary Agents for This Work: 1. ai-modeling-developer: 🎯 LEAD IMPLEMENTATION AGENT - Test-strategy-first, enforces 70% coverage, research-grounded code 2. workflow-orchestrator: Coordinates all phases, manages agent handoffs 3. repository-manager: Git operations, commits, branch management, interactive rebase 4. workflow-designer: Creates implementation workflows, process documentation 5. huggingface-hub: HuggingFace integration, model/dataset operations (when ready)\nSupporting Agents: 6. professional-document-architect: Update READMEs, documentation 7. llm-ai-agents-and-eng-research: Research latest AI/ML techniques 8. work-completion-summary: Audio summaries after each phase completion\n\n\n\n\n\nLead Agent: ai-modeling-developer - Proposes test strategy BEFORE extraction - Extracts code with parallel test development - Validates 70% coverage before any commits - Coordinates with repository-manager for commits\nExecution:\nai-modeling-developer:\n  # For each module extraction:\n  1. Analyze code to be extracted (GP models, foundation stubs, ETL)\n  2. Propose test strategy:\n     - Unit tests for kernels, models, training utilities\n     - Integration tests for end-to-end GP workflow\n     - Coverage target: &gt;70% for new modules\n  3. Get user approval on test strategy\n  4. Extract code + write comprehensive tests in parallel\n  5. Run coverage validation: uv run pytest --cov=src/cloud_sim\n  6. If coverage ≥70%: Delegate to repository-manager for commit\n  7. If coverage &lt;70%: Add tests until threshold met\n\nworkflow-orchestrator:\n  - Coordinates sequential extraction (GP → Foundation → ETL)\n  - Manages handoffs between ai-modeling-developer and repository-manager\n\nrepository-manager:\n  - Commit after coverage validation\n  - Format: \"refactor(ml_models): extract GP implementation from notebook (72% coverage)\"\n\n\n\nLead Agent: workflow-designer - Designs new notebook structure - Creates quickstart guides - Documents Colab workflows\nExecution:\nworkflow-designer:\n  - Create notebook restructuring plan\n  - Design Colab training workflow documentation\n  - Create installation guides\n\nworkflow-orchestrator:\n  - Coordinate notebook file moves\n  - Update notebook references\n  - Create new runbook templates\n\nrepository-manager:\n  - Commit notebook restructuring: \"refactor(notebooks): convert to runbook architecture\"\n  - Commit new guides: \"docs(notebooks): add quickstart and training guides\"\n\n\n\nLead Agent: repository-manager - Execute interactive rebase - Handle merge conflicts - Verify commit integrity\nExecution:\nrepository-manager:\n  - Create backup branch: backup/pre-rebase\n  - Interactive rebase: git rebase -i d0d1e1f~1\n  - Squash 75 commits → ~15 clean commits\n  - Verify each commit builds: git rebase --exec \"uv run pytest tests/\"\n  - Force push after verification\n\n\n\nLead Agent: professional-document-architect - Rewrite project history - Update README files - Create migration guide\nExecution:\nprofessional-document-architect:\n  - Update .claude-project-memory.md with new architecture\n  - Rewrite main README.md with new structure\n  - Create MIGRATION.md for existing users\n  - Update CLAUDE.md with new module structure\n\nrepository-manager:\n  - Commit: \"docs: update documentation for library-first architecture\"\n\n\n\nLead Agent: work-completion-summary - Generate audio summary of changes - Highlight key decisions and next steps\n\n\n\n\nWeek 1: Can run in parallel: - GP extraction + Foundation model stubs + CloudZero ETL - Use single workflow-orchestrator invocation with multiple subtasks\nWeek 2-3: Sequential required: - Notebook restructuring must follow library extraction (imports need to work) - But can parallelize: quickstart guides + Colab notebooks + EDA moves\nWeek 4: Must be sequential: - Interactive rebase is atomic operation - Documentation updates must follow rebase (commit SHAs change)\n\n\n\nFor each phase:\n# Phase 1 Example\nworkflow_orchestrator.execute({\n    \"phase\": \"Library Extraction\",\n    \"tasks\": [\n        {\n            \"name\": \"Extract GP module\",\n            \"files\": [\"notebooks/04_gaussian_process_modeling.md\"],\n            \"output\": [\"src/cloud_sim/ml_models/gaussian_process/\"],\n            \"agent\": \"general-purpose\"  # For code extraction\n        },\n        {\n            \"name\": \"Commit extraction\",\n            \"agent\": \"repository-manager\",\n            \"commit_message\": \"refactor(ml_models): extract GP implementation from notebook\"\n        }\n    ]\n})\n\n\n\nai-modeling-developer (CRITICAL): - ✅ Test strategy proposed and approved BEFORE implementation - ✅ All new code has ≥70% test coverage - ✅ Tests include research-validated parameters - ✅ No commits without coverage validation - ✅ All tests passing before handoff to repository-manager - ✅ Polars-only (no pandas) enforced - ✅ No print statements in notebooks\nworkflow-orchestrator: - ✅ All subtasks completed - ✅ Context properly passed between agents - ✅ TodoWrite tracking maintained\nrepository-manager: - ✅ All commits follow conventional format - ✅ Each commit builds and passes tests - ✅ SSH signing verified on all commits - ✅ Interactive rebase completes without conflicts - ✅ Commit messages include coverage percentage\nworkflow-designer: - ✅ Clear workflow documentation created - ✅ Colab setup instructions are executable - ✅ Runbook structure is intuitive\nprofessional-document-architect: - ✅ Documentation reflects new architecture - ✅ Migration path clearly explained - ✅ No broken links or outdated references\n\n\n\nIf workflow-orchestrator fails: - Review TodoWrite progress - Complete failed subtask manually - Resume from next incomplete task\nIf repository-manager rebase fails: - git rebase --abort - Restore from backup: git reset --hard backup/pre-rebase - Review conflict manually - Retry with adjusted strategy\nIf documentation updates incomplete: - Use professional-document-architect for final pass - Verify all cross-references - Run link checker\n\n\n\n\n\nAfter implementation: - [ ] Update .claude-project-memory.md with new architecture - [ ] Rewrite project history in documentation - [ ] Update main README.md with new structure - [ ] Create migration guide for existing users - [ ] Update agent coordination documentation with lessons learned"
  },
  {
    "objectID": "concepts/design/library-first-refactoring-plan.html#context-analysis",
    "href": "concepts/design/library-first-refactoring-plan.html#context-analysis",
    "title": "Library-First Refactoring Plan",
    "section": "",
    "text": "Current State: - 75 commits with messy iteration history - Library code exists: src/cloud_sim/{data_generation,ml_models} - Notebooks mix implementation AND educational content - GP model code lives IN notebook (04_gaussian_process_modeling.md) - No TimesFM/Chronos integration yet - ETL module empty after Alibaba/Google cleanup\nExecution Environments: 1. Local: Development, light inference, visualization (M4 CPU, no GPU) 2. Google Colab: GP training, TimesFM, large ETL (Free GPU, Pro $10/mo) 3. HuggingFace Inference API: Production inference (pay-per-use)"
  },
  {
    "objectID": "concepts/design/library-first-refactoring-plan.html#phase-1-extract-implementation-from-notebooks-to-library",
    "href": "concepts/design/library-first-refactoring-plan.html#phase-1-extract-implementation-from-notebooks-to-library",
    "title": "Library-First Refactoring Plan",
    "section": "",
    "text": "New files to create:\nsrc/cloud_sim/ml_models/gaussian_process/\n├── __init__.py\n├── kernels.py        # CompositePeriodicKernel, custom kernels\n├── models.py         # SparseGPModel, base classes\n├── training.py       # train_gp, save_model, load_model utilities\n└── evaluation.py     # compute_metrics, compute_anomaly_metrics\nExtract from: notebooks/04_gaussian_process_modeling.md lines 259-937 - CompositePeriodicKernel class → kernels.py - SparseGPModel class → models.py - Training logic → training.py (with batched prediction fixes) - compute_metrics, compute_anomaly_metrics → evaluation.py\n\n\n\nNew files to create:\nsrc/cloud_sim/ml_models/foundation/\n├── __init__.py\n├── timesfm.py        # TimesFM wrapper (Google)\n├── chronos.py        # Chronos wrapper (Amazon)\n└── base.py           # Abstract base class for foundation models\nCapabilities: - Load pre-trained models from HF - Inference API support - Fine-tuning hooks (Colab-only)\n\n\n\nNew files to create:\nsrc/cloud_sim/etl/\n├── __init__.py\n└── cloudzero_loader.py   # Production data loader\nDesign: - Load CloudZero production data samples - Convert to standardized format (Polars DataFrames) - Integrate with HuggingFace dataset publishing\n\n\n\nAdd optional dependency groups:\n[project.optional-dependencies]\ngpu = [\n    \"torch&gt;=2.0.0\",\n    \"gpytorch&gt;=1.11.0\",\n]\n\nfoundation = [\n    \"timesfm @ git+https://github.com/google-research/timesfm\",\n    \"chronos-forecasting&gt;=1.0.0\",\n    \"transformers&gt;=4.35.0\",\n]\n\nall = [\n    \"cloud-resource-simulator[gpu,foundation]\",\n]"
  },
  {
    "objectID": "concepts/design/library-first-refactoring-plan.html#phase-2-convert-notebooks-to-runbooks",
    "href": "concepts/design/library-first-refactoring-plan.html#phase-2-convert-notebooks-to-runbooks",
    "title": "Library-First Refactoring Plan",
    "section": "",
    "text": "notebooks/\n├── README.md                        # Update with new structure\n├── quickstart/\n│   ├── 01_installation.md          # Setup instructions\n│   ├── 02_basic_usage.md           # Simple library usage\n│   └── 03_data_generation.md       # WorkloadPatternGenerator demo\n├── guides/\n│   ├── workload_patterns.md        # Educational (keep current 02)\n│   ├── gp_inference.md             # Use pre-trained GP models\n│   └── foundation_models.md        # TimesFM/Chronos inference\n├── eda/\n│   ├── iops_web_server_analysis.md # Keep current 03 (research)\n│   └── cloudzero_data_exploration.md # NEW: CloudZero data\n└── training/ (Colab-specific)\n    ├── README.md                    # Colab setup instructions\n    ├── train_gp_colab.ipynb        # GPU training workflow\n    ├── train_timesfm_colab.ipynb   # Foundation model fine-tuning\n    └── etl_pipeline_colab.ipynb    # Large-scale data processing\n\n\n\n01_data_exploration.md → quickstart/03_data_generation.md - Remove implementation details - Focus on library usage: from cloud_sim.data_generation import WorkloadPatternGenerator - Show basic examples\n02_workload_signatures_guide.md → guides/workload_patterns.md - Keep as-is (already educational) - No implementation code to extract\n03_iops_web_server_eda.md → eda/iops_web_server_analysis.md - Keep as-is (research notebook) - Uses HuggingFace dataset (good runbook example)\n04_gaussian_process_modeling.md → Refactor: 1. Extract implementation → src/cloud_sim/ml_models/gaussian_process/ 2. Create guides/gp_inference.md - Load pre-trained models, run inference 3. Create training/train_gp_colab.ipynb - Full training workflow for Colab\n\n\n\nquickstart/01_installation.md:\n# Installation\n\n## Local Development\n`uv pip install -e \".[dev]\"`\n\n## GPU/ML Work (Colab)\n`!pip install cloud-resource-simulator[gpu,foundation]`\n\n## CloudZero Integration\nRequires production data access...\nguides/gp_inference.md:\nfrom cloud_sim.ml_models.gaussian_process import SparseGPModel, load_model\n\n# Load pre-trained model\nmodel = load_model(\"models/gp_robust_model.pth\")\n\n# Run inference\npredictions = model.predict(X_test)\ntraining/train_gp_colab.ipynb:\n# GPU-enabled training in Colab\n# 1. Mount Drive\n# 2. Install with GPU support\n# 3. Train on full dataset\n# 4. Save to Drive/HF Hub"
  },
  {
    "objectID": "concepts/design/library-first-refactoring-plan.html#phase-3-interactive-rebase-to-clean-history",
    "href": "concepts/design/library-first-refactoring-plan.html#phase-3-interactive-rebase-to-clean-history",
    "title": "Library-First Refactoring Plan",
    "section": "",
    "text": "Goal: Squash 75 commits into ~15 clean commits\nTarget structure:\n1. feat: establish project foundation (d0d1e1f - keep as-is)\n2. docs: establish research foundation (fe20c8e - keep as-is)\n3. feat: define simulation architecture (f5709bb - keep as-is)\n4. feat: implement core library (91cf3e3 + squash 10 related commits)\n5. feat: add PyMC Bayesian models (f59f45d + squash related)\n6. feat: add workload taxonomy (squash commits)\n7. docs: add research notebooks (squash all notebook commits)\n8. feat: add GP modeling support (squash GP commits)\n9. test: add comprehensive test suite (squash test commits)\n10. ci: configure GitHub Actions (squash CI commits)\n11. refactor: remove Alibaba/Google datasets (04c055f - keep as-is)\n12. refactor: extract GP implementation to library (NEW)\n13. refactor: convert notebooks to runbooks (NEW)\n14. feat: add foundation model support (NEW)\n15. feat: add CloudZero ETL (NEW)\n\n\n\nOption A: Interactive rebase from foundation\ngit rebase -i d0d1e1f~1  # Interactive from project start\n# Mark commits as 'squash' or 'fixup' to consolidate\nOption B: Reset and recommit (nuclear option)\ngit checkout -b clean-history d0d1e1f  # Branch from foundation\ngit cherry-pick fe20c8e f5709bb 91cf3e3  # Keep foundation commits\n# Then create new clean commits for Phase 1 & 2 work\nRecommendation: Option A (interactive rebase) - Less destructive, preserves authorship"
  },
  {
    "objectID": "concepts/design/library-first-refactoring-plan.html#phase-4-execution-environment-support",
    "href": "concepts/design/library-first-refactoring-plan.html#phase-4-execution-environment-support",
    "title": "Library-First Refactoring Plan",
    "section": "",
    "text": "Supported: - Library development - Runbook execution (inference only) - Small-scale testing - Visualization\nNot Supported: - GP training (CPU too slow, MPS incompatibility) - Large ETL pipelines (memory limits) - TimesFM experiments (requires GPU)\n\n\n\nSetup:\n# In Colab notebook\n!pip install cloud-resource-simulator[gpu,foundation]\n\n# Mount Drive for model persistence\nfrom google.colab import drive\ndrive.mount('/content/drive')\nWorkflow: 1. Train GP models in training/train_gp_colab.ipynb 2. Save trained models to Google Drive 3. Download to models/ directory for local inference 4. OR upload to HuggingFace Model Hub\nCost: Free tier (12hr session, T4 GPU) or Pro ($10/mo, better GPUs)\n\n\n\nFor: - Published trained models - Public inference endpoints - Sharing with community\nWorkflow: 1. Train in Colab 2. Push to HF Model Hub: model.push_to_hub(\"username/cloud-gp-model\") 3. Enable Inference API 4. Use in runbooks: from huggingface_hub import InferenceClient"
  },
  {
    "objectID": "concepts/design/library-first-refactoring-plan.html#phase-5-testing-validation",
    "href": "concepts/design/library-first-refactoring-plan.html#phase-5-testing-validation",
    "title": "Library-First Refactoring Plan",
    "section": "",
    "text": "Test extracted GP modules: tests/test_gp_models.py\nTest foundation model wrappers: tests/test_foundation_models.py\nTest CloudZero loader: tests/test_cloudzero_etl.py\nRunbook execution tests: Update test_notebooks.py for new structure\n\n\n\n\n\nRun tests on GPU runner for GP tests (GitHub Actions with GPU)\nOR skip GPU tests in CI, document manual testing in Colab\nAdd notebook linting for runbooks"
  },
  {
    "objectID": "concepts/design/library-first-refactoring-plan.html#decision-points-recommendations",
    "href": "concepts/design/library-first-refactoring-plan.html#decision-points-recommendations",
    "title": "Library-First Refactoring Plan",
    "section": "",
    "text": "A: Google Colab Pro ($10/mo) for GPU access - Local M4 too slow + MPS compatibility issues - Colab provides T4/V100 GPUs, persistent storage via Drive - Can save models to HF Hub for sharing\n\n\n\nA: Three-tier approach: 1. Development: Save to models/ directory (gitignored, local only) 2. Sharing: Push to HuggingFace Model Hub (public/private) 3. Production: HF Inference API or Colab-hosted\n\n\n\nA: Hybrid approach: - Sample data: Local testing with CloudZero production samples - Full pipelines: Colab for large-scale processing - Document both workflows in runbooks\n\n\n\nA: Use cases differ: - GP: Anomaly detection, uncertainty quantification, small-scale - TimesFM: Zero-shot forecasting, Google’s latest, requires GPU - Chronos: Probabilistic forecasting, Amazon’s model, lighter weight\nSupport all three, let runbooks demonstrate each\n\n\n\nA: Do Phase 1-2 first, THEN rebase: 1. Extract implementation to library (Phase 1) 2. Convert notebooks to runbooks (Phase 2) 3. Test everything works 4. Interactive rebase to clean history (Phase 3) 5. Force push to clean remote history\nWhy: Easier to debug if rebase goes wrong, can always revert"
  },
  {
    "objectID": "concepts/design/library-first-refactoring-plan.html#implementation-order",
    "href": "concepts/design/library-first-refactoring-plan.html#implementation-order",
    "title": "Library-First Refactoring Plan",
    "section": "",
    "text": "Create GP module structure\nExtract code from notebook 04\nAdd training utilities\nUpdate tests\nVerify notebook still works with imports\n\n\n\n\n\nAdd foundation model wrappers (stub implementations first)\nAdd CloudZero ETL module\nUpdate pyproject.toml dependencies\nCreate Colab training notebooks\n\n\n\n\n\nRestructure notebooks/ directory\nRefactor existing notebooks\nCreate quickstart guides\nCreate Colab-specific training notebooks\nUpdate documentation\n\n\n\n\n\nInteractive rebase to squash commits\nForce push clean history\nUpdate README/docs to reflect new structure\nVerify CI still passes\nCreate release tag"
  },
  {
    "objectID": "concepts/design/library-first-refactoring-plan.html#risk-mitigation",
    "href": "concepts/design/library-first-refactoring-plan.html#risk-mitigation",
    "title": "Library-First Refactoring Plan",
    "section": "",
    "text": "Risk 1: Rebase breaks something - Mitigation: Create backup branch before rebase - Recovery: git reflog to restore pre-rebase state\nRisk 2: Model training costs escalate - Mitigation: Start with Colab free tier, monitor usage - Recovery: Can train locally (slow but works)\nRisk 3: HF Inference API limits - Mitigation: Document rate limits, provide local inference fallback - Recovery: Self-host models on Colab\nRisk 4: CloudZero data structure changes - Mitigation: Abstract ETL interface, version loaders - Recovery: Create adapters for different schemas"
  },
  {
    "objectID": "concepts/design/library-first-refactoring-plan.html#success-metrics",
    "href": "concepts/design/library-first-refactoring-plan.html#success-metrics",
    "title": "Library-First Refactoring Plan",
    "section": "",
    "text": "✅ Library Quality: - All implementation code in src/cloud_sim/ - No implementation in runbooks (only imports) - &gt;70% test coverage maintained\n✅ Runbook Quality: - Can execute all runbooks locally (inference only) - Colab notebooks have clear GPU setup - Educational content preserved\n✅ History Quality: - Clean commit history (&lt;20 commits) - Each commit buildable and testable - Clear commit messages\n✅ Documentation Quality: - README reflects new structure - Installation instructions for local vs Colab - Clear separation of concerns"
  },
  {
    "objectID": "concepts/design/library-first-refactoring-plan.html#next-steps-after-approval",
    "href": "concepts/design/library-first-refactoring-plan.html#next-steps-after-approval",
    "title": "Library-First Refactoring Plan",
    "section": "",
    "text": "Create feature branch: git checkout -b refactor/library-first\nStart Phase 1: Extract GP module\nCommit incrementally with tests\nWhen stable, proceed to Phase 2-4\nFinal interactive rebase on feature branch\nReview before force-pushing to main\n\nEstimated Effort: 15-20 hours over 2-4 weeks"
  },
  {
    "objectID": "concepts/design/library-first-refactoring-plan.html#agent-coordination-strategy",
    "href": "concepts/design/library-first-refactoring-plan.html#agent-coordination-strategy",
    "title": "Library-First Refactoring Plan",
    "section": "",
    "text": "Primary Agents for This Work: 1. ai-modeling-developer: 🎯 LEAD IMPLEMENTATION AGENT - Test-strategy-first, enforces 70% coverage, research-grounded code 2. workflow-orchestrator: Coordinates all phases, manages agent handoffs 3. repository-manager: Git operations, commits, branch management, interactive rebase 4. workflow-designer: Creates implementation workflows, process documentation 5. huggingface-hub: HuggingFace integration, model/dataset operations (when ready)\nSupporting Agents: 6. professional-document-architect: Update READMEs, documentation 7. llm-ai-agents-and-eng-research: Research latest AI/ML techniques 8. work-completion-summary: Audio summaries after each phase completion\n\n\n\n\n\nLead Agent: ai-modeling-developer - Proposes test strategy BEFORE extraction - Extracts code with parallel test development - Validates 70% coverage before any commits - Coordinates with repository-manager for commits\nExecution:\nai-modeling-developer:\n  # For each module extraction:\n  1. Analyze code to be extracted (GP models, foundation stubs, ETL)\n  2. Propose test strategy:\n     - Unit tests for kernels, models, training utilities\n     - Integration tests for end-to-end GP workflow\n     - Coverage target: &gt;70% for new modules\n  3. Get user approval on test strategy\n  4. Extract code + write comprehensive tests in parallel\n  5. Run coverage validation: uv run pytest --cov=src/cloud_sim\n  6. If coverage ≥70%: Delegate to repository-manager for commit\n  7. If coverage &lt;70%: Add tests until threshold met\n\nworkflow-orchestrator:\n  - Coordinates sequential extraction (GP → Foundation → ETL)\n  - Manages handoffs between ai-modeling-developer and repository-manager\n\nrepository-manager:\n  - Commit after coverage validation\n  - Format: \"refactor(ml_models): extract GP implementation from notebook (72% coverage)\"\n\n\n\nLead Agent: workflow-designer - Designs new notebook structure - Creates quickstart guides - Documents Colab workflows\nExecution:\nworkflow-designer:\n  - Create notebook restructuring plan\n  - Design Colab training workflow documentation\n  - Create installation guides\n\nworkflow-orchestrator:\n  - Coordinate notebook file moves\n  - Update notebook references\n  - Create new runbook templates\n\nrepository-manager:\n  - Commit notebook restructuring: \"refactor(notebooks): convert to runbook architecture\"\n  - Commit new guides: \"docs(notebooks): add quickstart and training guides\"\n\n\n\nLead Agent: repository-manager - Execute interactive rebase - Handle merge conflicts - Verify commit integrity\nExecution:\nrepository-manager:\n  - Create backup branch: backup/pre-rebase\n  - Interactive rebase: git rebase -i d0d1e1f~1\n  - Squash 75 commits → ~15 clean commits\n  - Verify each commit builds: git rebase --exec \"uv run pytest tests/\"\n  - Force push after verification\n\n\n\nLead Agent: professional-document-architect - Rewrite project history - Update README files - Create migration guide\nExecution:\nprofessional-document-architect:\n  - Update .claude-project-memory.md with new architecture\n  - Rewrite main README.md with new structure\n  - Create MIGRATION.md for existing users\n  - Update CLAUDE.md with new module structure\n\nrepository-manager:\n  - Commit: \"docs: update documentation for library-first architecture\"\n\n\n\nLead Agent: work-completion-summary - Generate audio summary of changes - Highlight key decisions and next steps\n\n\n\n\nWeek 1: Can run in parallel: - GP extraction + Foundation model stubs + CloudZero ETL - Use single workflow-orchestrator invocation with multiple subtasks\nWeek 2-3: Sequential required: - Notebook restructuring must follow library extraction (imports need to work) - But can parallelize: quickstart guides + Colab notebooks + EDA moves\nWeek 4: Must be sequential: - Interactive rebase is atomic operation - Documentation updates must follow rebase (commit SHAs change)\n\n\n\nFor each phase:\n# Phase 1 Example\nworkflow_orchestrator.execute({\n    \"phase\": \"Library Extraction\",\n    \"tasks\": [\n        {\n            \"name\": \"Extract GP module\",\n            \"files\": [\"notebooks/04_gaussian_process_modeling.md\"],\n            \"output\": [\"src/cloud_sim/ml_models/gaussian_process/\"],\n            \"agent\": \"general-purpose\"  # For code extraction\n        },\n        {\n            \"name\": \"Commit extraction\",\n            \"agent\": \"repository-manager\",\n            \"commit_message\": \"refactor(ml_models): extract GP implementation from notebook\"\n        }\n    ]\n})\n\n\n\nai-modeling-developer (CRITICAL): - ✅ Test strategy proposed and approved BEFORE implementation - ✅ All new code has ≥70% test coverage - ✅ Tests include research-validated parameters - ✅ No commits without coverage validation - ✅ All tests passing before handoff to repository-manager - ✅ Polars-only (no pandas) enforced - ✅ No print statements in notebooks\nworkflow-orchestrator: - ✅ All subtasks completed - ✅ Context properly passed between agents - ✅ TodoWrite tracking maintained\nrepository-manager: - ✅ All commits follow conventional format - ✅ Each commit builds and passes tests - ✅ SSH signing verified on all commits - ✅ Interactive rebase completes without conflicts - ✅ Commit messages include coverage percentage\nworkflow-designer: - ✅ Clear workflow documentation created - ✅ Colab setup instructions are executable - ✅ Runbook structure is intuitive\nprofessional-document-architect: - ✅ Documentation reflects new architecture - ✅ Migration path clearly explained - ✅ No broken links or outdated references\n\n\n\nIf workflow-orchestrator fails: - Review TodoWrite progress - Complete failed subtask manually - Resume from next incomplete task\nIf repository-manager rebase fails: - git rebase --abort - Restore from backup: git reset --hard backup/pre-rebase - Review conflict manually - Retry with adjusted strategy\nIf documentation updates incomplete: - Use professional-document-architect for final pass - Verify all cross-references - Run link checker"
  },
  {
    "objectID": "concepts/design/library-first-refactoring-plan.html#related-documentation-to-update",
    "href": "concepts/design/library-first-refactoring-plan.html#related-documentation-to-update",
    "title": "Library-First Refactoring Plan",
    "section": "",
    "text": "After implementation: - [ ] Update .claude-project-memory.md with new architecture - [ ] Rewrite project history in documentation - [ ] Update main README.md with new structure - [ ] Create migration guide for existing users - [ ] Update agent coordination documentation with lessons learned"
  },
  {
    "objectID": "concepts/research/cloud-resource-patterns-research.html",
    "href": "concepts/research/cloud-resource-patterns-research.html",
    "title": "Cloud Resource Usage Patterns and Signatures: Technical Research Report",
    "section": "",
    "text": "Date: January 18, 2025 Reference Count: 35 authoritative sources cited\n\n\nCloud resource utilization remains critically inefficient across the industry, with only 13% of provisioned CPUs and 20% of memory being actually utilized [1]. Organizations waste approximately 30-32% of their cloud spending, totaling $225.9 billion in 2024 [2]. This research report provides comprehensive technical details on resource usage patterns, waste indicators, and optimization benchmarks across different workload types to enable realistic cloud resource simulations.\n\n\n\n\n\nRecent industry studies reveal alarmingly low resource utilization across cloud environments: - CPU Utilization: Only 13% of provisioned CPUs are actually utilized [1] - Memory Utilization: Only 20% of provisioned memory is actively used [1] - Improved rates for scale: Clusters with 1,000+ CPUs average 17% utilization [1] - Spot Instance reluctance: Organizations remain hesitant to use Spot Instances despite cost benefits [1]\n\n\n\nCloud waste represents a massive financial burden: - 32% of cloud expenditure is wasted, equating to $225.9 billion in 2024 [2] - $135 billion in wasted cloud resources expected in 2024 [2] - $44.5 billion projected infrastructure waste for 2025 [3] - 30-40% average waste due to overprovisioning alone [4]\n\n\n\n\n\n\nWeb applications exhibit distinct temporal patterns based on their usage characteristics [5]:\n\n\n\nPattern: Consistent 24/7 resource usage\nExamples: Email services, CRM systems, ERP applications\nResource needs: Fairly predictable and known\nOptimization approach: Right-sizing based on steady-state usage\n\n\n\n\n\nPattern: Regular traffic spikes at specific times (daily/weekly/monthly)\nExamples: Bill payment systems, tax and accounting tools\nPeak variations: Can see 3-5x traffic during peak periods\nOptimization: Serverless computing ideal for these patterns [5]\n\n\n\n\n\nPattern: Exponential traffic increases without warning\nExamples: Social networks, online games, streaming platforms\nScaling requirements: Auto-scaling essential for handling spikes [5]\nResource multiplication: Can require 10-100x resources during viral events\n\n\n\n\n\nGPU utilization in ML workloads shows significant optimization opportunities [6]:\n\n\n\nTarget utilization: &gt;80% during active training phases [6]\nCurrent reality: Many jobs operate at ≤50% GPU utilization [7]\nMemory utilization impact: Lower batch sizes result in 3.4GB/48GB (7%) usage [6]\nOptimized batch size: Can achieve 100% GPU utilization with proper tuning [6]\n\n\n\n\n\nBatch size 64: ~3.4GB GPU memory usage out of 48GB available [6]\nBatch size 128: ~5GB GPU memory usage, 100% GPU utilization achieved [6]\nPerformance gains: 20x training performance improvements possible [8]\nIndustry achievement: 99%+ GPU utilization demonstrated in MLPerf benchmarks [8]\n\n\n\n\n\nMemory allocation stability: Should remain constant throughout training [6]\nGradual increases: May indicate memory leaks requiring attention [6]\nDistributed training: All GPUs should show similar utilization patterns [6]\nImbalance indicators: Significant variations suggest load distribution issues [6]\n\n\n\n\n\nDatabase workloads show distinct resource consumption patterns [9]:\n\n\n\nCPU monitoring intervals: Enhanced monitoring at 1, 5, 10, 15, 30, or 60 seconds [9]\nLoad average threshold: Heavy load when exceeds number of vCPUs [9]\nMemory components: Performance Schema tracks usage by event type [9]\nBaseline establishment: DevOps Guru uses ML to detect anomalies [9]\n\n\n\n\n\nCPU Utilization: Percentage of processing capacity used\nDB Connections: Active client sessions connected\nFreeable Memory: Available RAM in megabytes\nIOPS correlation: Compare Read/Write IOPS with CPU for pattern identification [9]\n\n\n\n\n\nBatch processing exhibits unique resource signatures: - Periodic spikes: Regular resource usage at scheduled intervals - Idle periods: Extended low-utilization between batch runs - Memory patterns: Step-function increases during data loading - CPU bursts: 100% utilization during processing, near-zero between jobs\n\n\n\n\n\n\nTypical daily resource consumption follows predictable cycles [5]:\n\n\n\nMorning ramp: 30-50% increase from 7-9 AM\nPeak hours: 100% baseline load 10 AM - 3 PM\nAfternoon decline: 20-30% reduction after 5 PM\nOvernight minimum: 10-20% of peak usage\n\n\n\n\n\nWork hours peak: 9 AM - 6 PM local time\nLunch dip: 15-20% reduction 12-1 PM\nEvening spike: 20% increase 7-9 PM (remote workers)\nWeekend reduction: 80-90% lower than weekdays\n\n\n\n\n\nWeekly cycles show consistent trends [5]: - Monday surge: 15-25% higher than weekend baseline - Mid-week peak: Tuesday-Thursday highest utilization - Friday decline: 10-15% reduction from peak - Weekend trough: 60-80% reduction for business applications\n\n\n\nSeasonal variations impact different sectors [5]: - Retail peaks: 300-500% increases during holiday seasons - Tax software: 1000% increases during filing deadlines - Education platforms: 200% increases during semester starts - Streaming services: 150% increases during major events\n\n\n\n\n\n\nAdvanced detection methods identify memory leaks through specific patterns [10]:\n\n\n\nContinuous growth: Steady memory increase without leveling [10]\nNon-decreasing usage: Memory never drops during idle periods [10]\nStair-step pattern: Periodic jumps without corresponding releases [10]\nDetection accuracy: 85% precision, 91% recall achieved [10]\n\n\n\n\n\nLBR Algorithm: Uses system memory utilization metrics [10]\nPrecogMF: 85% accuracy with 80% compute time reduction [10]\nPattern analysis: Steady, spike, or stair growth patterns [10]\nMitigation impact: 100x reduction in VM reboots achieved [10]\n\n\n\n\n\nZombie resources represent significant hidden costs [11]:\n\n\n\nIdle VMs: Testing instances never terminated, costing $100/month each [11]\nUnused load balancers: No connected resources but still incurring charges [11]\nDormant databases: Holding unused data without queries [11]\nOrphaned snapshots: Backups never deleted after migrations [11]\nReserved IPs: Static addresses for non-existent projects [11]\n\n\n\n\n\nZero utilization: Resources at 0% usage for &gt;7 days\nNo network traffic: No inbound/outbound connections for &gt;30 days\nOrphaned state: Resources with no parent or dependent resources\nAge indicators: Resources older than 90 days with minimal activity\n\n\n\n\n\nOver-provisioning manifests in specific patterns [4]:\n\n\n\nAverage utilization &lt;20%: Clear over-provisioning indicator\nPeak utilization &lt;40%: Never approaching capacity limits\nBurst headroom &gt;60%: Excessive safety margins\nInstance size mismatch: Using XL when Medium sufficient\n\n\n\n\n\nAverage usage &lt;30%: Significant over-allocation\nPeak usage &lt;50%: Never utilizing half of allocation\nNo swap usage: Despite low memory utilization\nCache dominance: 70%+ memory used for caching only\n\n\n\n\n\n\n\n\nIndustry standards for resource utilization from FinOps Framework [12]:\n\n\n\nSteady-state workloads: 80% utilization upper waterline [12]\nVariable workloads: 60-70% average utilization target\nDevelopment environments: 40-50% acceptable utilization\nCurrent reality: Most organizations at only 50% utilization [12]\n\n\n\n\n\nCoverage targets: 70-80% of steady-state usage covered\nSavings thresholds: &gt;90% savings per dollar of commitment [12]\nESR by spend: $10M+ spend achieves 54.3% median ESR [12]\nUnused potential: 50% of organizations use no discount instruments [12]\n\n\n\n\n\nQuantified improvement potential based on benchmarks [12]:\n\n\n\nUtilization improvement: 15% cost reduction achievable [12]\nStorage optimization: 30% reduction from S3 Standard baseline [12]\nRight-sizing: 20-40% savings from proper instance selection\nCommitment discounts: 25-55% savings with proper coverage\n\n\n\n\n\nCurrent organizational challenges in resource management [13]:\n\n\n\n43% have real-time data on idle resources [13]\n39% can see unused/orphaned resources [13]\n33% visibility into over/under-provisioned workloads [13]\n55% base commitments on guesswork [13]\n\n\n\n\n\n30% know where cloud budget is actually spent [13]\n30% can accurately attribute cloud costs [13]\n20% have little/no idea of business cost relationships [13]\n31 days average to identify and eliminate waste [13]\n\n\n\n\n\n\n\n\nAverage time to detect various issues manually [13]: - Idle resources: 31 days to identify and eliminate - Orphaned resources: 31 days to detect and remove - Over-provisioning: 25 days to detect and rightsize - Memory leaks: Weeks to months without monitoring\n\n\n\nImproved detection with modern tools: - Real-time alerts: Immediate detection of anomalies - ML-based detection: &lt;24 hours for pattern recognition - Automated remediation: Minutes to hours for action - Continuous monitoring: Ongoing optimization cycles\n\n\n\n\n\n\n\n\n\nTrigger metrics: CPU &gt;60% over 5-minute window [5]\nScale-out delay: 2-5 minutes typical\nScale-in delay: 10-15 minutes to avoid flapping\nEffectiveness: Good for gradual changes, lags on bursts [5]\n\n\n\n\n\nTraining data: 24+ hours of usage patterns required [5]\nForecast window: Up to 48 hours advance planning [5]\nUse cases: E-commerce peaks, streaming events [5]\nAccuracy: 85-90% prediction accuracy achievable\n\n\n\n\n\n\n\n\nCollect 2-4 weeks of utilization data\nIdentify peak usage periods (95th percentile)\nAdd 20-30% headroom for safety\nSelect instance size matching requirements\nMonitor and adjust based on actual usage\n\n\n\n\n\n\n\n\nGarbage collection tuning: Reduce memory footprint 20-30%\nConnection pooling: Limit concurrent connections\nCache sizing: Right-size caches based on hit rates\nHeap limits: Set appropriate JVM/runtime limits\n\n\n\n\n\nBuffer pool sizing: 70-80% of available memory\nQuery optimization: Reduce memory-intensive operations\nConnection limits: Prevent memory exhaustion\nIndex optimization: Reduce memory requirements\n\n\n\n\n\n\n\n\nResults from memory leak detection deployment [10]: - Period: September 2020 - December 2023 - VM reboot reduction: Nearly 100x decrease - Allocation error reduction: Over 30x decrease - Outage prevention: Zero severe outages from memory leaks since 2020 - Detection accuracy: 85% precision, 91% recall\n\n\n\nIndustry achievements in GPU optimization [8]: - Alluxio implementation: 99%+ GPU utilization achieved - Performance gain: 20x training performance improvement - Latency reduction: 45x faster than S3 Standard - Customer growth: 50%+ including Salesforce and Geely\n\n\n\n\n\n\nFor accurate simulations, use these baseline parameters:\n\n\n\nBase CPU: 10-20% idle, 40-60% normal, 80-90% peak\nMemory: 30-40% base, 60-70% normal, 85% peak\nNetwork: 100 Mbps base, 1 Gbps peak for standard apps\nStorage IOPS: 100-500 base, 2000-5000 peak\n\n\n\n\n\nGPU utilization: 0% idle, 50% poorly optimized, 80%+ optimized\nGPU memory: Scales with batch size (7% to 90% range)\nCPU coordination: 20-30% during GPU training\nNetwork (distributed): 10 Gbps+ for model parallel training\n\n\n\n\n\nCPU: 20% idle, 50% normal, 90% peak transactions\nMemory: 70-80% steady for buffer pools\nIOPS: 500-1000 normal, 10,000+ for heavy workloads\nConnection pool: 50-200 concurrent connections typical\n\n\n\n\n\nAdd realistic variations to simulations: - Random spikes: ±20% random variation every 5 minutes - Gradual drift: ±5% per hour for organic growth - Burst events: 200-500% spikes lasting 1-15 minutes - Maintenance windows: 50% reduction for 2-4 hours weekly\n\n\n\nInclude failure scenarios: - Memory leaks: 0.5-2% memory growth per hour - CPU pegging: Stuck at 100% for extended periods - Network issues: 50% packet loss or 10x latency - Cascade failures: 30% resource increase when peers fail\n\n\n\n\n[1] Data Center Dynamics. (2024). “Study: Only 13% of provisioned CPUs and 20% of memory utilized in cloud computing.” DCD. https://www.datacenterdynamics.com/en/news/only-13-of-provisioned-cpus-and-20-of-memory-utilized-in-cloud-computing-report/\n[2] CloudZero. (2025). “90+ Cloud Computing Statistics: A 2025 Market Snapshot.” CloudZero Blog. https://www.cloudzero.com/blog/cloud-computing-statistics/\n[3] Harness. (2025). “$44.5 Billion in Infrastructure Cloud Waste Projected for 2025.” PR Newswire. https://www.prnewswire.com/news-releases/44-5-billion-in-infrastructure-cloud-waste-projected-for-2025-due-to-finops-and-developer-disconnect-finds-finops-in-focus-report-from-harness-302385580.html\n[4] ProsperOps. (2024). “How To Identify and Reduce Cloud Waste.” ProsperOps Blog. https://www.prosperops.com/blog/how-to-identify-and-prevent-cloud-waste/\n[5] Aqua Security. (2024). “Cloud Workloads: Types, Common Tasks & Security Best Practices.” Aqua Cloud Native Academy. https://www.aquasec.com/cloud-native-academy/cspm/cloud-workload/\n[6] Alluxio. (2024). “GPU Utilization: What Is It and How to Maximize It.” Alluxio Blog. https://www.alluxio.io/blog/maximize-gpu-utilization-for-model-training\n[7] Microsoft Research. (2024). “An Empirical Study on Low GPU Utilization of Deep Learning Jobs.” ICSE 2024 Proceedings. https://www.microsoft.com/en-us/research/publication/an-empirical-study-on-low-gpu-utilization-of-deep-learning-jobs/\n[8] Alluxio. (2024). “MLPerf Storage v2.0 Results Showing 99%+ GPU Utilization.” Alluxio Performance Benchmarks. https://www.alluxio.io/blog/maximize-gpu-utilization-for-model-training\n[9] AWS. (2024). “View CPU and memory usage for Aurora MySQL-Compatible DB clusters.” AWS Knowledge Center. https://repost.aws/knowledge-center/rds-aurora-mysql-view-cpu-memory\n[10] Microsoft Azure. (2024). “Advancing memory leak detection with AIOps—introducing RESIN.” Azure Blog. https://azure.microsoft.com/en-us/blog/advancing-memory-leak-detection-with-aiops-introducing-resin/\n[11] AST Consulting. (2024). “Zombie Resources in the Cloud: What They Are and How to Banish Them.” AST Consulting FinOps. https://astconsulting.in/finops/zombie-resources-in-the-cloud\n[12] FinOps Foundation. (2024). “Resource Utilization & Efficiency Framework Capability.” FinOps.org. https://www.finops.org/framework/capabilities/utilization-efficiency/\n[13] Williams, D. (2024). “FinOps is Stuck — Cloud Waste is Out of Control; But There’s a Fix.” Medium. https://medium.com/@dpwilliams03/finops-is-stuck-cloud-waste-is-out-of-control-but-theres-a-fix-c28e1155b86c"
  },
  {
    "objectID": "concepts/research/cloud-resource-patterns-research.html#executive-summary",
    "href": "concepts/research/cloud-resource-patterns-research.html#executive-summary",
    "title": "Cloud Resource Usage Patterns and Signatures: Technical Research Report",
    "section": "",
    "text": "Cloud resource utilization remains critically inefficient across the industry, with only 13% of provisioned CPUs and 20% of memory being actually utilized [1]. Organizations waste approximately 30-32% of their cloud spending, totaling $225.9 billion in 2024 [2]. This research report provides comprehensive technical details on resource usage patterns, waste indicators, and optimization benchmarks across different workload types to enable realistic cloud resource simulations."
  },
  {
    "objectID": "concepts/research/cloud-resource-patterns-research.html#overall-cloud-resource-utilization-statistics",
    "href": "concepts/research/cloud-resource-patterns-research.html#overall-cloud-resource-utilization-statistics",
    "title": "Cloud Resource Usage Patterns and Signatures: Technical Research Report",
    "section": "",
    "text": "Recent industry studies reveal alarmingly low resource utilization across cloud environments: - CPU Utilization: Only 13% of provisioned CPUs are actually utilized [1] - Memory Utilization: Only 20% of provisioned memory is actively used [1] - Improved rates for scale: Clusters with 1,000+ CPUs average 17% utilization [1] - Spot Instance reluctance: Organizations remain hesitant to use Spot Instances despite cost benefits [1]\n\n\n\nCloud waste represents a massive financial burden: - 32% of cloud expenditure is wasted, equating to $225.9 billion in 2024 [2] - $135 billion in wasted cloud resources expected in 2024 [2] - $44.5 billion projected infrastructure waste for 2025 [3] - 30-40% average waste due to overprovisioning alone [4]"
  },
  {
    "objectID": "concepts/research/cloud-resource-patterns-research.html#workload-specific-usage-patterns",
    "href": "concepts/research/cloud-resource-patterns-research.html#workload-specific-usage-patterns",
    "title": "Cloud Resource Usage Patterns and Signatures: Technical Research Report",
    "section": "",
    "text": "Web applications exhibit distinct temporal patterns based on their usage characteristics [5]:\n\n\n\nPattern: Consistent 24/7 resource usage\nExamples: Email services, CRM systems, ERP applications\nResource needs: Fairly predictable and known\nOptimization approach: Right-sizing based on steady-state usage\n\n\n\n\n\nPattern: Regular traffic spikes at specific times (daily/weekly/monthly)\nExamples: Bill payment systems, tax and accounting tools\nPeak variations: Can see 3-5x traffic during peak periods\nOptimization: Serverless computing ideal for these patterns [5]\n\n\n\n\n\nPattern: Exponential traffic increases without warning\nExamples: Social networks, online games, streaming platforms\nScaling requirements: Auto-scaling essential for handling spikes [5]\nResource multiplication: Can require 10-100x resources during viral events\n\n\n\n\n\nGPU utilization in ML workloads shows significant optimization opportunities [6]:\n\n\n\nTarget utilization: &gt;80% during active training phases [6]\nCurrent reality: Many jobs operate at ≤50% GPU utilization [7]\nMemory utilization impact: Lower batch sizes result in 3.4GB/48GB (7%) usage [6]\nOptimized batch size: Can achieve 100% GPU utilization with proper tuning [6]\n\n\n\n\n\nBatch size 64: ~3.4GB GPU memory usage out of 48GB available [6]\nBatch size 128: ~5GB GPU memory usage, 100% GPU utilization achieved [6]\nPerformance gains: 20x training performance improvements possible [8]\nIndustry achievement: 99%+ GPU utilization demonstrated in MLPerf benchmarks [8]\n\n\n\n\n\nMemory allocation stability: Should remain constant throughout training [6]\nGradual increases: May indicate memory leaks requiring attention [6]\nDistributed training: All GPUs should show similar utilization patterns [6]\nImbalance indicators: Significant variations suggest load distribution issues [6]\n\n\n\n\n\nDatabase workloads show distinct resource consumption patterns [9]:\n\n\n\nCPU monitoring intervals: Enhanced monitoring at 1, 5, 10, 15, 30, or 60 seconds [9]\nLoad average threshold: Heavy load when exceeds number of vCPUs [9]\nMemory components: Performance Schema tracks usage by event type [9]\nBaseline establishment: DevOps Guru uses ML to detect anomalies [9]\n\n\n\n\n\nCPU Utilization: Percentage of processing capacity used\nDB Connections: Active client sessions connected\nFreeable Memory: Available RAM in megabytes\nIOPS correlation: Compare Read/Write IOPS with CPU for pattern identification [9]\n\n\n\n\n\nBatch processing exhibits unique resource signatures: - Periodic spikes: Regular resource usage at scheduled intervals - Idle periods: Extended low-utilization between batch runs - Memory patterns: Step-function increases during data loading - CPU bursts: 100% utilization during processing, near-zero between jobs"
  },
  {
    "objectID": "concepts/research/cloud-resource-patterns-research.html#temporal-usage-patterns",
    "href": "concepts/research/cloud-resource-patterns-research.html#temporal-usage-patterns",
    "title": "Cloud Resource Usage Patterns and Signatures: Technical Research Report",
    "section": "",
    "text": "Typical daily resource consumption follows predictable cycles [5]:\n\n\n\nMorning ramp: 30-50% increase from 7-9 AM\nPeak hours: 100% baseline load 10 AM - 3 PM\nAfternoon decline: 20-30% reduction after 5 PM\nOvernight minimum: 10-20% of peak usage\n\n\n\n\n\nWork hours peak: 9 AM - 6 PM local time\nLunch dip: 15-20% reduction 12-1 PM\nEvening spike: 20% increase 7-9 PM (remote workers)\nWeekend reduction: 80-90% lower than weekdays\n\n\n\n\n\nWeekly cycles show consistent trends [5]: - Monday surge: 15-25% higher than weekend baseline - Mid-week peak: Tuesday-Thursday highest utilization - Friday decline: 10-15% reduction from peak - Weekend trough: 60-80% reduction for business applications\n\n\n\nSeasonal variations impact different sectors [5]: - Retail peaks: 300-500% increases during holiday seasons - Tax software: 1000% increases during filing deadlines - Education platforms: 200% increases during semester starts - Streaming services: 150% increases during major events"
  },
  {
    "objectID": "concepts/research/cloud-resource-patterns-research.html#resource-waste-indicators-and-signatures",
    "href": "concepts/research/cloud-resource-patterns-research.html#resource-waste-indicators-and-signatures",
    "title": "Cloud Resource Usage Patterns and Signatures: Technical Research Report",
    "section": "",
    "text": "Advanced detection methods identify memory leaks through specific patterns [10]:\n\n\n\nContinuous growth: Steady memory increase without leveling [10]\nNon-decreasing usage: Memory never drops during idle periods [10]\nStair-step pattern: Periodic jumps without corresponding releases [10]\nDetection accuracy: 85% precision, 91% recall achieved [10]\n\n\n\n\n\nLBR Algorithm: Uses system memory utilization metrics [10]\nPrecogMF: 85% accuracy with 80% compute time reduction [10]\nPattern analysis: Steady, spike, or stair growth patterns [10]\nMitigation impact: 100x reduction in VM reboots achieved [10]\n\n\n\n\n\nZombie resources represent significant hidden costs [11]:\n\n\n\nIdle VMs: Testing instances never terminated, costing $100/month each [11]\nUnused load balancers: No connected resources but still incurring charges [11]\nDormant databases: Holding unused data without queries [11]\nOrphaned snapshots: Backups never deleted after migrations [11]\nReserved IPs: Static addresses for non-existent projects [11]\n\n\n\n\n\nZero utilization: Resources at 0% usage for &gt;7 days\nNo network traffic: No inbound/outbound connections for &gt;30 days\nOrphaned state: Resources with no parent or dependent resources\nAge indicators: Resources older than 90 days with minimal activity\n\n\n\n\n\nOver-provisioning manifests in specific patterns [4]:\n\n\n\nAverage utilization &lt;20%: Clear over-provisioning indicator\nPeak utilization &lt;40%: Never approaching capacity limits\nBurst headroom &gt;60%: Excessive safety margins\nInstance size mismatch: Using XL when Medium sufficient\n\n\n\n\n\nAverage usage &lt;30%: Significant over-allocation\nPeak usage &lt;50%: Never utilizing half of allocation\nNo swap usage: Despite low memory utilization\nCache dominance: 70%+ memory used for caching only"
  },
  {
    "objectID": "concepts/research/cloud-resource-patterns-research.html#industry-benchmarks-and-standards",
    "href": "concepts/research/cloud-resource-patterns-research.html#industry-benchmarks-and-standards",
    "title": "Cloud Resource Usage Patterns and Signatures: Technical Research Report",
    "section": "",
    "text": "Industry standards for resource utilization from FinOps Framework [12]:\n\n\n\nSteady-state workloads: 80% utilization upper waterline [12]\nVariable workloads: 60-70% average utilization target\nDevelopment environments: 40-50% acceptable utilization\nCurrent reality: Most organizations at only 50% utilization [12]\n\n\n\n\n\nCoverage targets: 70-80% of steady-state usage covered\nSavings thresholds: &gt;90% savings per dollar of commitment [12]\nESR by spend: $10M+ spend achieves 54.3% median ESR [12]\nUnused potential: 50% of organizations use no discount instruments [12]\n\n\n\n\n\nQuantified improvement potential based on benchmarks [12]:\n\n\n\nUtilization improvement: 15% cost reduction achievable [12]\nStorage optimization: 30% reduction from S3 Standard baseline [12]\nRight-sizing: 20-40% savings from proper instance selection\nCommitment discounts: 25-55% savings with proper coverage\n\n\n\n\n\nCurrent organizational challenges in resource management [13]:\n\n\n\n43% have real-time data on idle resources [13]\n39% can see unused/orphaned resources [13]\n33% visibility into over/under-provisioned workloads [13]\n55% base commitments on guesswork [13]\n\n\n\n\n\n30% know where cloud budget is actually spent [13]\n30% can accurately attribute cloud costs [13]\n20% have little/no idea of business cost relationships [13]\n31 days average to identify and eliminate waste [13]"
  },
  {
    "objectID": "concepts/research/cloud-resource-patterns-research.html#problem-detection-timeframes",
    "href": "concepts/research/cloud-resource-patterns-research.html#problem-detection-timeframes",
    "title": "Cloud Resource Usage Patterns and Signatures: Technical Research Report",
    "section": "",
    "text": "Average time to detect various issues manually [13]: - Idle resources: 31 days to identify and eliminate - Orphaned resources: 31 days to detect and remove - Over-provisioning: 25 days to detect and rightsize - Memory leaks: Weeks to months without monitoring\n\n\n\nImproved detection with modern tools: - Real-time alerts: Immediate detection of anomalies - ML-based detection: &lt;24 hours for pattern recognition - Automated remediation: Minutes to hours for action - Continuous monitoring: Ongoing optimization cycles"
  },
  {
    "objectID": "concepts/research/cloud-resource-patterns-research.html#optimization-techniques-and-best-practices",
    "href": "concepts/research/cloud-resource-patterns-research.html#optimization-techniques-and-best-practices",
    "title": "Cloud Resource Usage Patterns and Signatures: Technical Research Report",
    "section": "",
    "text": "Trigger metrics: CPU &gt;60% over 5-minute window [5]\nScale-out delay: 2-5 minutes typical\nScale-in delay: 10-15 minutes to avoid flapping\nEffectiveness: Good for gradual changes, lags on bursts [5]\n\n\n\n\n\nTraining data: 24+ hours of usage patterns required [5]\nForecast window: Up to 48 hours advance planning [5]\nUse cases: E-commerce peaks, streaming events [5]\nAccuracy: 85-90% prediction accuracy achievable\n\n\n\n\n\n\n\n\nCollect 2-4 weeks of utilization data\nIdentify peak usage periods (95th percentile)\nAdd 20-30% headroom for safety\nSelect instance size matching requirements\nMonitor and adjust based on actual usage\n\n\n\n\n\n\n\n\nGarbage collection tuning: Reduce memory footprint 20-30%\nConnection pooling: Limit concurrent connections\nCache sizing: Right-size caches based on hit rates\nHeap limits: Set appropriate JVM/runtime limits\n\n\n\n\n\nBuffer pool sizing: 70-80% of available memory\nQuery optimization: Reduce memory-intensive operations\nConnection limits: Prevent memory exhaustion\nIndex optimization: Reduce memory requirements"
  },
  {
    "objectID": "concepts/research/cloud-resource-patterns-research.html#real-world-case-studies",
    "href": "concepts/research/cloud-resource-patterns-research.html#real-world-case-studies",
    "title": "Cloud Resource Usage Patterns and Signatures: Technical Research Report",
    "section": "",
    "text": "Results from memory leak detection deployment [10]: - Period: September 2020 - December 2023 - VM reboot reduction: Nearly 100x decrease - Allocation error reduction: Over 30x decrease - Outage prevention: Zero severe outages from memory leaks since 2020 - Detection accuracy: 85% precision, 91% recall\n\n\n\nIndustry achievements in GPU optimization [8]: - Alluxio implementation: 99%+ GPU utilization achieved - Performance gain: 20x training performance improvement - Latency reduction: 45x faster than S3 Standard - Customer growth: 50%+ including Salesforce and Geely"
  },
  {
    "objectID": "concepts/research/cloud-resource-patterns-research.html#simulation-parameters-for-realistic-modeling",
    "href": "concepts/research/cloud-resource-patterns-research.html#simulation-parameters-for-realistic-modeling",
    "title": "Cloud Resource Usage Patterns and Signatures: Technical Research Report",
    "section": "",
    "text": "For accurate simulations, use these baseline parameters:\n\n\n\nBase CPU: 10-20% idle, 40-60% normal, 80-90% peak\nMemory: 30-40% base, 60-70% normal, 85% peak\nNetwork: 100 Mbps base, 1 Gbps peak for standard apps\nStorage IOPS: 100-500 base, 2000-5000 peak\n\n\n\n\n\nGPU utilization: 0% idle, 50% poorly optimized, 80%+ optimized\nGPU memory: Scales with batch size (7% to 90% range)\nCPU coordination: 20-30% during GPU training\nNetwork (distributed): 10 Gbps+ for model parallel training\n\n\n\n\n\nCPU: 20% idle, 50% normal, 90% peak transactions\nMemory: 70-80% steady for buffer pools\nIOPS: 500-1000 normal, 10,000+ for heavy workloads\nConnection pool: 50-200 concurrent connections typical\n\n\n\n\n\nAdd realistic variations to simulations: - Random spikes: ±20% random variation every 5 minutes - Gradual drift: ±5% per hour for organic growth - Burst events: 200-500% spikes lasting 1-15 minutes - Maintenance windows: 50% reduction for 2-4 hours weekly\n\n\n\nInclude failure scenarios: - Memory leaks: 0.5-2% memory growth per hour - CPU pegging: Stuck at 100% for extended periods - Network issues: 50% packet loss or 10x latency - Cascade failures: 30% resource increase when peers fail"
  },
  {
    "objectID": "concepts/research/cloud-resource-patterns-research.html#references",
    "href": "concepts/research/cloud-resource-patterns-research.html#references",
    "title": "Cloud Resource Usage Patterns and Signatures: Technical Research Report",
    "section": "",
    "text": "[1] Data Center Dynamics. (2024). “Study: Only 13% of provisioned CPUs and 20% of memory utilized in cloud computing.” DCD. https://www.datacenterdynamics.com/en/news/only-13-of-provisioned-cpus-and-20-of-memory-utilized-in-cloud-computing-report/\n[2] CloudZero. (2025). “90+ Cloud Computing Statistics: A 2025 Market Snapshot.” CloudZero Blog. https://www.cloudzero.com/blog/cloud-computing-statistics/\n[3] Harness. (2025). “$44.5 Billion in Infrastructure Cloud Waste Projected for 2025.” PR Newswire. https://www.prnewswire.com/news-releases/44-5-billion-in-infrastructure-cloud-waste-projected-for-2025-due-to-finops-and-developer-disconnect-finds-finops-in-focus-report-from-harness-302385580.html\n[4] ProsperOps. (2024). “How To Identify and Reduce Cloud Waste.” ProsperOps Blog. https://www.prosperops.com/blog/how-to-identify-and-prevent-cloud-waste/\n[5] Aqua Security. (2024). “Cloud Workloads: Types, Common Tasks & Security Best Practices.” Aqua Cloud Native Academy. https://www.aquasec.com/cloud-native-academy/cspm/cloud-workload/\n[6] Alluxio. (2024). “GPU Utilization: What Is It and How to Maximize It.” Alluxio Blog. https://www.alluxio.io/blog/maximize-gpu-utilization-for-model-training\n[7] Microsoft Research. (2024). “An Empirical Study on Low GPU Utilization of Deep Learning Jobs.” ICSE 2024 Proceedings. https://www.microsoft.com/en-us/research/publication/an-empirical-study-on-low-gpu-utilization-of-deep-learning-jobs/\n[8] Alluxio. (2024). “MLPerf Storage v2.0 Results Showing 99%+ GPU Utilization.” Alluxio Performance Benchmarks. https://www.alluxio.io/blog/maximize-gpu-utilization-for-model-training\n[9] AWS. (2024). “View CPU and memory usage for Aurora MySQL-Compatible DB clusters.” AWS Knowledge Center. https://repost.aws/knowledge-center/rds-aurora-mysql-view-cpu-memory\n[10] Microsoft Azure. (2024). “Advancing memory leak detection with AIOps—introducing RESIN.” Azure Blog. https://azure.microsoft.com/en-us/blog/advancing-memory-leak-detection-with-aiops-introducing-resin/\n[11] AST Consulting. (2024). “Zombie Resources in the Cloud: What They Are and How to Banish Them.” AST Consulting FinOps. https://astconsulting.in/finops/zombie-resources-in-the-cloud\n[12] FinOps Foundation. (2024). “Resource Utilization & Efficiency Framework Capability.” FinOps.org. https://www.finops.org/framework/capabilities/utilization-efficiency/\n[13] Williams, D. (2024). “FinOps is Stuck — Cloud Waste is Out of Control; But There’s a Fix.” Medium. https://medium.com/@dpwilliams03/finops-is-stuck-cloud-waste-is-out-of-control-but-theres-a-fix-c28e1155b86c"
  },
  {
    "objectID": "concepts/research/timeseries-anomaly-datasets-review.html",
    "href": "concepts/research/timeseries-anomaly-datasets-review.html",
    "title": "Hugging Face Datasets for Cloud Resource Anomaly Detection",
    "section": "",
    "text": "Bottom line: Hugging Face offers several strong datasets for cloud resource anomaly detection modeling, though direct cloud infrastructure datasets are limited. The best options combine benchmark collections with labeled anomalies, real cloud platform data, and energy/resource domain datasets that closely parallel cloud behavior.\nTop recommendations: For comprehensive benchmarking, use AutonLab/Timeseries-PILE (1,980 labeled anomaly time series including web server data). For actual cloud infrastructure, use Lemma-RCA-NEC/Cloud_Computing_Original (real cloud platform with 6 fault types). For algorithm development with strong seasonality, use pryshlyak/seasonal_time_series_for_anomaly_detection (explicit weekly patterns). Energy domain datasets (electricity demand, solar generation, grid monitoring) provide excellent cloud analogs with resource consumption patterns, daily/weekly cycles, and efficiency concerns matching cloud workloads.\nKey insight: While dedicated multi-metric cloud datasets (CPU+memory+network combined) are scarce, energy and infrastructure monitoring datasets exhibit remarkably similar characteristics—resource consumption over time, strong temporal patterns from user/load behavior, anomalies from inefficiency or external events, and operational monitoring requirements. These domains provide robust training and evaluation data for cloud-focused anomaly detection research.\n\n\n\nThe following table compares the most relevant datasets for cloud resource anomaly detection, ordered by direct applicability to cloud infrastructure monitoring:\n\n\n\n\n\n\n\n\n\n\n\n\nDataset\nDomain\nScale\nSeasonality\nAnomaly Labels\nCloud Relevance\nBest For\n\n\n\n\nLemma-RCA-NEC/Cloud_Computing_Original\nReal cloud platform\nHundreds of entities\nDaily/weekly cycles\n✅ Yes (6 fault types)\n🟢 High - Actual cloud data\nReal cloud fault detection, RCA\n\n\nAutonLab/Timeseries-PILE\nMulti-domain benchmark\n1,980 labeled series (20GB)\nVaries by domain\n✅ Yes (comprehensive)\n🟢 High - Web server subset\nComprehensive benchmarking\n\n\npryshlyak/seasonal_time_series\nSynthetic seasonal\n67,700 points (3 months)\nStrong weekly patterns\n✅ Yes (3 types)\n🟡 Medium - Generic patterns\nSeasonal algorithm development\n\n\npatrickfleith/CATS\nSimulated system\n5M points, 17 variables\nProcess-driven cycles\n✅ Yes (200 precise)\n🟡 Medium - Multivariate\nMultivariate method testing\n\n\nEDS-lab/electricity-demand\nSmart meters\nMulti-building, hourly\nDaily/weekly/seasonal + weather\n⚠️ No (find natural)\n🟡 Medium - Resource consumption\nResource usage patterns\n\n\nETDataset/ett\nElectricity transformers\n2 years, 6 variables\nDaily/seasonal + trends\n⚠️ No (predict failures)\n🟡 Medium - Infrastructure\nMulti-timescale patterns\n\n\nopenclimatefix/uk_pv\nSolar generation\n30,000+ systems (15 years)\nDaily/seasonal strong\n⚠️ Partial (bad_data.csv)\n🟡 Medium - External impacts\nLarge-scale peer comparison\n\n\nelectricity_load_diagrams\nGrid substations\n320 substations (4 years)\nDaily/weekly/seasonal\n⚠️ No (find operational)\n🟡 Medium - Infrastructure\nMulti-entity monitoring\n\n\nTime-MQA/TSQA\nQA-formatted\n37K anomaly QA pairs\nVaries\n✅ Yes (QA format)\n🟡 Medium - AIOps included\nLLM-based approaches\n\n\n\nLegend: 🟢 High relevance (direct cloud data or web servers) | 🟡 Medium relevance (analogous resource behavior) | ✅ Labeled | ⚠️ Unlabeled or partial\n\n\nNeed actual cloud data? → Lemma-RCA-NEC/Cloud_Computing_Original + AutonLab/Timeseries-PILE (web server subset)\nNeed labeled anomalies for supervised learning? → AutonLab/Timeseries-PILE (1,980 series) or patrickfleith/CATS (200 precise labels)\nDeveloping seasonal anomaly detection? → pryshlyak/seasonal_time_series (clean weekly patterns) → validate on EDS-lab/electricity-demand\nTesting multivariate methods? → patrickfleith/CATS (controlled 17-var) → ETDataset/ett (real-world 6-var)\nNeed large-scale evaluation? → openclimatefix/uk_pv (30K+ systems) or electricity_load_diagrams (320 entities)\nExploring LLM-based detection? → Time-MQA/TSQA (37K QA pairs with AIOps domain)\n\n\n\n\n\nHugging Face hosts several high-quality datasets suitable for time series anomaly detection modeling analogous to cloud resource usage behavior, though dedicated cloud infrastructure datasets remain limited. The platform offers a mix of real-world operational data, energy consumption patterns, and purpose-built anomaly detection benchmarks—all exhibiting the seasonality, outliers, and temporal dynamics needed for robust anomaly detection research. The most promising datasets span industrial monitoring, energy systems, and large-scale benchmark collections with labeled anomalies.\n\n\n\nThe datasets below most closely match cloud resource usage patterns with seasonal behavior and labeled anomalies, ordered by relevance to your use case.\n\n\nThis massive collection aggregates 13 million unique time series across 13 domains, specifically designed for foundation model training and evaluation. The dataset’s TSB-UAD (Time-Series Benchmark for Univariate Anomaly Detection) component contains 1,980 labeled time series from 18 anomaly detection datasets, making it the most comprehensive anomaly detection resource on Hugging Face.\nDataset characteristics: The collection spans 20.085 GB with 1.23 billion timestamps total, including data from healthcare, engineering, finance, environment, and critically, web servers—the domain most analogous to cloud infrastructure. The TSB-UAD subset provides both synthetic and real-world anomalies with high variability in types, ratios, and sizes. The dataset also includes the Informer forecasting collection with electricity transformer temperature data, traffic patterns, and weather data—all exhibiting resource-like temporal dynamics.\nSeasonality patterns: Temporal patterns vary by subdataset but comprehensively cover daily cycles, weekly patterns, and seasonal variations. Web server data within TSB-UAD exhibits usage patterns directly comparable to cloud resources. The electricity and traffic datasets demonstrate clear periodic behavior with load-dependent variations.\nAnomaly types: The labeled anomalies span point anomalies (sudden spikes), collective anomalies (sustained unusual patterns), and contextual anomalies (unusual given temporal context). These mirror the anomaly types in cloud environments: point anomalies resemble sudden resource spikes, collective anomalies match degraded performance periods, and contextual anomalies reflect unusual usage given time-of-day expectations.\nWhy suitable for cloud resource modeling: Web server metrics inherently parallel cloud resource behavior—both exhibit request-driven load patterns, have clear daily/weekly seasonality from user activity, and experience anomalies from traffic surges, system failures, or external events. The diversity of 18 source datasets prevents overfitting to specific patterns while the large scale (1,980 labeled series) enables robust model training and evaluation.\nPreprocessing considerations: The dataset comes standardized for the MOMENT foundation model framework but remains accessible via standard tools. Use the TSB-UAD subset specifically for anomaly detection tasks. The data varies in length and amplitude across sources, so normalization by time series is recommended. The collection provides ready-to-use train/test splits, eliminating common temporal leakage issues. Downloaded 28,200 times, indicating strong community validation.\n\n\n\nThis dataset provides actual system metrics and logs from a cloud computing platform with six real fault types injected across hundreds of system entities—making it the most directly applicable to cloud resource anomaly detection on Hugging Face.\nDataset characteristics: The data comes in JSON format containing both system metrics (performance measurements) and logs (event data) from a production cloud computing environment. The dataset captures real operational conditions with hundreds of interconnected system entities, reflecting the complexity of actual cloud infrastructure. While the exact size isn’t specified, the multimodal nature (metrics + logs) provides rich context for anomaly detection.\nFault types labeled: The dataset includes six critical cloud failure modes: cryptojacking (unauthorized resource usage), silent pod degradation (gradual performance decay), malware attacks, GitOps mistakes (configuration errors), configuration change failures, and bug infections. These represent real-world cloud anomalies spanning security breaches, performance degradation, and operational errors—precisely the types of inefficiencies and external impacts relevant to your use case.\nSeasonality patterns: Cloud computing platforms naturally exhibit strong temporal patterns from user activity. Workloads typically show pronounced daily cycles (business hours vs. night), weekly patterns (weekday vs. weekend usage), and potential seasonal variations from business cycles or world events. The time-series format with timestamps enables analysis of these periodic patterns.\nWhy suitable: This dataset directly addresses the cloud resource use case unlike other datasets that require analogy. The fault types mirror real cloud anomalies: cryptojacking represents inefficient resource usage, pod degradation shows performance issues, and the others reflect external impacts from attacks or human errors. The multi-entity structure parallels distributed cloud architectures with many interconnected services.\nPreprocessing considerations: The JSON format requires parsing into time-series structure. Given hundreds of entities, feature selection or dimensionality reduction may be necessary. The dataset is designed for Root Cause Analysis (RCA), providing attribution for which entities are affected by each fault—valuable for understanding anomaly propagation in distributed systems. Note the CC-BY-ND-4.0 license restricts derivative works. The dataset viewer has errors, so programmatic access via the datasets library is recommended.\n\n\n\nThis dataset was explicitly designed for seasonal anomaly detection, making it ideal for developing and testing algorithms that leverage periodic patterns. Based on the Numenta Anomaly Benchmark but restructured to emphasize weekly seasonality, it provides clean labeled data for methodical algorithm development.\nDataset characteristics: The dataset contains 67,700 rows with 5-minute sampling intervals spanning three months. Data is organized by day of week (seven separate CSVs for Monday-Sunday with 3,745 rows each) plus weekly aggregations (2,017 rows each). This structure directly supports periodicity-based anomaly detection approaches. The format is minimal—timestamp and value columns only—keeping focus on temporal patterns.\nSeasonality patterns: The dataset exhibits strong weekly periodicity with distinct patterns for each weekday, directly analogous to cloud resources that experience different usage on weekdays versus weekends. The 5-minute granularity captures intra-day variations like morning startup, lunch dips, and evening shutdowns common in business applications. The three-month span covers sufficient cycles for learning robust seasonal patterns.\nAnomaly characteristics: Training data includes seven weekday files and one normal week file with no anomalies—ideal for unsupervised learning. Testing data contains three types: collective anomaly downward (Monday file, like sustained performance degradation), collective anomaly upward (Wednesday file, like traffic surge), and point anomaly (Saturday file, like sudden spike). These anomaly types directly map to cloud resource scenarios: downward collective anomalies represent underutilization or failures, upward anomalies represent unusual demand, and point anomalies represent isolated incidents.\nWhy suitable: The explicit seasonal structure mirrors cloud workloads where different days exhibit different patterns—weekday business traffic differs from weekend consumer traffic. The clean separation of training (normal) and testing (anomalous) data supports supervised, semi-supervised, and unsupervised approaches. The dataset’s design for auto-encoder training generalizes to any anomaly detection technique exploiting periodicity.\nPreprocessing considerations: Data is pre-cleaned with no missing values and positive-only values. The day-of-week split enables training separate models per period—the thesis approach—or pooling for general models. Timestamps are artificial (January-March 2024) so absolute dates don’t matter, only relative temporal positions. Best for developing seasonal algorithms before applying to messier real-world data. Consider this a controlled environment for algorithm validation.\n\n\n\nFor researchers needing multivariate anomaly detection with precise ground truth, CATS provides 200 exactly labeled anomalies across 17 variables in a 5-million-point simulated system—offering unprecedented control for rigorous algorithm evaluation.\nDataset characteristics: CATS simulates a complex dynamical system (analogous to industrial control systems or building management) with 17 variables split into 4 control commands, 3 environmental stimuli, and 10 telemetry readings (temperature, pressure, voltage, current, position, velocity, acceleration, humidity). The 1Hz sampling rate provides second-by-second resolution over an extended operational period. The first 1 million points contain only nominal behavior (ideal for unsupervised training), while the next 4 million mix normal and anomalous segments.\nSeasonality patterns: The simulated system exhibits operational cycles from the control logic and environmental interactions—periodic patterns from processes like heating/cooling cycles, position movements, or state machines. While not traditional daily/weekly seasonality, these represent the process-driven periodicity found in automated systems, directly relevant to auto-scaling cloud resources or scheduled batch jobs.\nAnomaly characteristics: All 200 anomalies include precise metadata: exact start/end times, root cause channel, affected channels, and anomaly category. This eliminates the ground truth ambiguity plaguing many benchmark datasets. The anomalies have controlled injection, meaning researchers can isolate algorithm performance from data quality issues. Metadata supports root cause analysis—identifying not just that an anomaly occurred but which variable caused it and which variables were affected.\nWhy suitable for cloud modeling: Modern cloud environments are increasingly multivariate—CPU, memory, network, disk I/O, latency, and error rates all interact. CATS’s multivariate structure with known dependencies mirrors this. The control commands parallel API calls or configuration changes that affect system state. Environmental stimuli represent external load or conditions. Telemetry readings parallel observability metrics. The pure signal with no noise provides a baseline; researchers can add custom noise levels to test robustness—valuable for understanding algorithm behavior under varying data quality.\nPreprocessing considerations: The dataset is pristine with no missing data—unrealistic for production but perfect for controlled experiments. Add synthetic noise or missing data windows to test real-world resilience. The root cause labels enable evaluation of not just detection but attribution algorithms. Use the first 1 million points for semi-supervised approaches (novelty detection) or include contamination for unsupervised scenarios. The multivariate nature requires techniques handling variable interactions—VAE, LSTM-VAE, or graph neural networks.\n\n\n\n\nThese datasets from energy domains provide excellent analogs to cloud resources given their strong seasonality, resource-like behavior, and operational monitoring nature.\n\n\nSmart meter electricity consumption closely parallels cloud resource consumption—both represent resource usage over time, exhibit strong temporal patterns, and experience demand variations from user behavior and external conditions.\nDataset characteristics: This harmonized collection aggregates multiple smart meter datasets with hourly sampling across residential and commercial buildings. The data comes in three components: demand.parquet (consumption time series with unique_id, timestamp, y in kWh), metadata.parquet (building_class, cluster_size, location), and weather.parquet (25+ weather variables including temperature, humidity, precipitation, solar radiation, wind speed). The multi-building structure provides numerous parallel time series for comparative analysis.\nSeasonality patterns: Electricity demand shows pronounced patterns directly analogous to cloud resources. Daily cycles reflect business hours for commercial buildings or home activity for residential—matching cloud application usage. Weekly cycles distinguish weekdays from weekends—mirroring reduced weekend traffic for business applications. Seasonal variations from heating/cooling loads parallel seasonal e-commerce patterns (holiday shopping) or tax season spikes. The strong correlation with weather (temperature especially) demonstrates how external factors drive consumption—just as world events or viral content drive cloud traffic.\nAnomaly opportunities: While unlabeled, natural anomalies abound: equipment malfunctions (sudden drops or spikes), unusual consumption (vacant building with high usage suggesting waste), meter reading errors (negative values or impossibly high readings), or weather-adjusted anomalies (high usage on mild day). The weather covariates enable sophisticated contextual anomaly detection—flagging consumption unusual for the conditions, analogous to detecting high cloud resource usage during low user activity periods.\nWhy suitable: The parallels are strong: both are resource consumption metrics with strong time-of-day/day-of-week patterns, both have a “correct” expected baseline with deviations indicating issues, both are influenced by external factors (weather vs. user behavior), and both seek to identify inefficient usage. The building metadata enables clustering similar usage profiles—like grouping similar microservices or customer workloads.\nPreprocessing for anomaly detection: The rich weather covariates suggest multivariate anomaly detection incorporating context. Normalize consumption by building capacity or size for fair comparison. Use the multiple buildings for peer-comparison approaches—flagging buildings as anomalous if they deviate from similar buildings. The location data enables spatial analysis. Test algorithms for weather-adjusted anomaly detection—a valuable capability for cloud resources when expected load varies by time or external factors. Licensed under BSD 3-clause for flexible use.\n\n\n\nThis dataset provides 2 years of electricity transformer operational data from critical infrastructure—equipment failure here has severe consequences, making anomaly detection crucial.\nDataset characteristics: Four variants (ETT-h1, ETT-h2, ETT-m1, ETT-m2) from two transformers at two stations provide both hourly (17,520 points) and 15-minute (70,080 points) resolution data spanning 2016-2018. The target variable is Oil Temperature (OT), a critical safety indicator—overheating damages transformers. Six load features provide context: High/Middle/Low UseFul Load and High/Middle/Low UseLess Load, capturing the transformer’s operating conditions.\nSeasonality patterns: The dataset explicitly exhibits short-term periodical patterns (daily load cycles from grid usage), long-term periodical patterns (seasonal variations from weather-dependent demand), long-term trends (equipment aging/degradation), and complex irregular patterns. This combination of pattern types makes it excellent for testing robust algorithms that must handle multiple timescales—exactly the challenge in cloud resources with daily patterns, weekly patterns, and long-term growth trends.\nAnomaly detection value: Oil temperature anomalies indicate transformer malfunction risk—overheating from excess load, cooling system failure, or internal electrical issues. These parallel cloud resource anomalies: excess load, insufficient capacity, or component failures. False predictions can damage equipment—the same high-stakes environment as cloud anomaly detection where false alarms cause alert fatigue and missed detections cause outages.\nWhy suitable: The load features + temperature structure mirrors cloud metrics (CPU/memory/network load + response time/error rate). Both domains involve resource allocation, capacity planning, and failure prevention. The multi-transformer setup provides multiple parallel series for comparative analysis. The predictive maintenance application—detecting issues before failure—directly parallels cloud workload management.\nPreprocessing considerations: Pre-split into train/val/test (12/4/4 months) for consistent evaluation. The multivariate structure (6 load features + temperature) enables testing correlation-based anomaly detection—flagging temperature unusual given load conditions. The 15-minute variants provide higher resolution for detecting rapid-onset anomalies. Use the long-term trends to test algorithms robust to non-stationarity. Well-documented and widely used in time series research (from the Informer paper), ensuring reproducibility. CC-BY-4.0 license.\n\n\n\nWith over 30,000 solar PV systems tracked from 2010-2025, this dataset provides exceptional scale for anomaly detection research, plus partial ground truth via labeled bad data periods.\nDataset characteristics: The dataset covers 15 years of domestic solar installations across Great Britain with two resolution levels: 30-minute intervals (30,000+ systems, high quality cumulative energy) and 5-minute intervals (1,309 systems, instantaneous but noisy). Systems range from 0.47 kW to 250 kW capacity. Metadata includes latitude/longitude, panel orientation, tilt angle, and capacity. Critically, bad_data.csv identifies known periods of data quality issues—providing partial ground truth for anomaly detection evaluation.\nSeasonality patterns: Solar generation exhibits the strongest possible seasonal patterns: zero nighttime generation, predictable daily curves (sunrise ramp, midday peak, sunset decline), seasonal variation (long summer days, short winter days), and weather dependency (cloud cover causes rapid generation drops). Geographic spread across Britain provides diverse weather patterns. These characteristics parallel cloud resources with predictable baseline patterns disrupted by external events—like solar generation disrupted by clouds, cloud resources are disrupted by traffic events.\nAnomaly characteristics: The bad_data.csv provides labeled anomalies including: equipment malfunctions, negative generation values (sensor errors), excessive values (&gt;750× capacity, clearly impossible), nighttime generation (indicates clock errors), and zero generation during sunny periods (equipment failure). Beyond these labeled cases, natural unlabeled anomalies exist from inverter failures, panel degradation, shading issues, and soiling (dirt reducing output).\nWhy suitable: The massive scale (30,000+ systems) enables testing at cloud-scale where thousands of services or instances require monitoring. The partial ground truth balances labeled data for validation with unlabeled realistic data for unsupervised approaches. The geospatial dimension enables peer-comparison anomaly detection—flagging systems underperforming compared to nearby systems with similar conditions. The real-world messiness (missing data, quality issues) mirrors production cloud data.\nPreprocessing considerations: Extensive data cleaning guidance is provided. Use bad_data.csv as ground truth test set. Normalize generation by kWp capacity for fair comparison across system sizes. The location data enables clustering by geographic region or weather patterns. Handle missing midnight readings and gaps documented in the dataset. The 5-minute data is noisy—test algorithm robustness to noise. Gated dataset requiring access approval, but readily granted for research. DOI: 10.57967/hf/0878.\n\n\n\nThis dataset provides 4 years of Portuguese electricity grid substation data with 370 substations sampled every 15 minutes—a scale and operational focus directly relevant to infrastructure monitoring.\nDataset characteristics: Originally 15-minute sampling resampled to hourly, spanning 2011-2014. The LSTNet benchmark configuration uses 320 substations active during 2012-2014, providing consistent time series for algorithm comparison. The data captures actual grid load in kW across a national electricity system—critical infrastructure where anomalies indicate equipment issues, unexpected demand, or grid instability.\nSeasonality patterns: Electricity grid load exhibits textbook periodicity: 24-hour daily cycles (morning ramp-up, evening peak, overnight minimum), weekly patterns (weekday business/industrial load vs. weekend residential-dominated load), and seasonal variations (summer air conditioning, winter heating). These patterns mirror cloud infrastructure serving business applications with clear usage rhythms.\nAnomaly opportunities: Though unlabeled, natural anomalies include equipment failures (sudden drops), unexpected demand (unusual spikes), grid instability (rapid fluctuations), or seasonal anomalies (unusual load for the weather). The dataset documents known quirks: daylight saving time transitions create 23-hour days (March) and 25-hour days (October)—realistic complications that anomaly detectors must handle. No missing values simplifies initial development.\nWhy suitable: Multiple substations enable comparative anomaly detection—identifying substations behaving differently from peers. The hourly resolution matches common cloud metric collection intervals. The 4-year span covers sufficient seasonal cycles for learning robust patterns. The infrastructure monitoring domain parallels cloud monitoring—both involve critical systems requiring high availability where anomalies indicate serious issues.\nPreprocessing considerations: Account for daylight saving transitions where March has zeros 1-2am (23 hours) and October aggregates 1-2am (25 hours). The resampling to hourly from 15-minute data smooths some noise but may miss rapid anomalies—consider accessing original 15-minute data if available. Use the LSTNet configuration (320 substations, 2012-2014) for benchmark comparison. The dataset is clean and well-documented, used in multiple research papers for validation. Source: UCI Machine Learning Repository.\n\n\n\n\n\n\nThis unique dataset reformulates time series tasks as question-answering pairs, enabling LLM-based approaches to anomaly detection—a frontier research direction.\nDataset characteristics: Approximately 200,000 QA pairs across 12 real-world domains with 37,000 instances specifically for anomaly detection. The dataset includes healthcare (EEG, ECG), finance, energy, IoT, environment, transport, web traffic, and critically AIOps (cloud monitoring) domain. Multiple formats are provided: 6,919 true/false questions, 11,281 multiple-choice questions, and 12,510 open-ended questions. Pre-trained models are available (Mistral 7B, Qwen-2.5 7B, Llama-3 8B).\nWhy relevant: The AIOps domain explicitly includes cloud monitoring data formatted for question answering. This enables approaches where models reason about time series rather than just pattern-match—answering questions like “Is this usage pattern anomalous given it’s Monday morning?” or “What caused this traffic spike?” The multi-task format (forecasting, imputation, anomaly detection, classification, open-ended reasoning) enables transfer learning where representations learned from one task improve others.\nUse case: For researchers exploring LLM-based anomaly detection or multi-task time series models, this provides ready-to-use training data. The context enhancement (auxiliary textual descriptions) helps models understand domain semantics—e.g., explaining that Monday mornings typically have high traffic helps contextual anomaly detection. Downloaded 262 times despite recent publication (ACL 2025), indicating strong interest.\nPreprocessing considerations: The QA format requires different architectures than traditional time series models—use sequence-to-sequence models or LLMs fine-tuned on time series. The dataset is designed for continued pre-training of foundation models. Consider how to convert raw time series into this format if generating additional training data. The multiple-choice and true/false formats enable classification-based evaluation.\n\n\n\n\n\n\nPrimary choice: Lemma-RCA-NEC/Cloud_Computing_Original provides actual cloud platform metrics with real fault types, though limited documentation requires hands-on exploration. Supplement with AutonLab/Timeseries-PILE’s web server data for additional cloud-analogous patterns.\n\n\n\nPrimary choice: pryshlyak/seasonal_time_series_for_anomaly_detection offers clean, explicitly seasonal data ideal for developing and validating periodicity-based algorithms. Once proven on this controlled dataset, test on messier real-world data like EDS-lab/electricity-demand or electricity_load_diagrams.\n\n\n\nPrimary choice: AutonLab/Timeseries-PILE provides the most extensive benchmark with 1,980 labeled anomaly time series from 18 datasets. The TSB-UAD subset specifically addresses dataset quality issues plaguing older benchmarks. Use this for broad evaluation across diverse patterns and anomaly types.\n\n\n\nPrimary choice: patrickfleith/controlled-anomalies-time-series-dataset (CATS) offers 17 variables with precise root cause labels—ideal for developing and testing multivariate algorithms. Follow with ETDataset/ett for real-world multivariate data at scale.\n\n\n\nPrimary choice: openclimatefix/uk_pv with 30,000+ systems tests algorithmic scalability and peer-comparison approaches. Alternatively, electricity_load_diagrams with 320 substations provides a smaller but still substantial multi-entity dataset.\n\n\n\n\n\n\nMost datasets benefit from per-time-series normalization (z-score standardization) to account for different scales—one building’s 10 kW load differs from another’s 1000 kW load, but both may show similar relative patterns. For datasets with metadata (capacity, size), consider normalizing by these physical properties. For multivariate datasets like CATS or ETT, normalize each variable independently to prevent high-magnitude variables dominating.\n\n\n\nAlgorithms exploiting seasonality require sufficient cycles for learning—at least 2-3 complete periods. For daily patterns, 2-3 days suffices; for weekly patterns, 2-3 weeks; for seasonal patterns, 1-2 years. Use techniques like seasonal decomposition (STL) to explicitly model and remove seasonality, with residuals analyzed for anomalies. Alternatively, use day-of-week and hour-of-day encodings as features. The pryshlyak dataset’s pre-split by weekday demonstrates one approach.\n\n\n\nAlways use temporal splits, not random splits—time series have temporal dependencies making random splits leak information. The typical split is chronological: first 60-70% training, remaining testing. Ensure training data precedes test data temporally. For semi-supervised approaches, training should contain only normal data (or &lt;5% contamination). For unsupervised approaches, training may include realistic anomaly rates.\n\n\n\nReal-world datasets like openclimatefix/uk_pv have missing readings common in production. Strategies include: forward-fill for short gaps (carry last valid value), interpolation for medium gaps (linear or spline), masking and imputation modeling for long gaps, or excluding time series with excessive missingness. Test algorithm robustness to missing data patterns. CATS’s completeness provides a baseline; add synthetic gaps to test handling.\n\n\n\nMost energy and resource datasets lack anomaly labels—use these for unsupervised approaches (isolation forest, LOF, autoencoders) or semi-supervised approaches (one-class SVM, VAE). Alternatively, use domain knowledge to label obvious anomalies (e.g., negative electricity generation, usage 10× typical) or leverage labeled datasets for training with transfer learning to unlabeled domains.\n\n\n\nSliding window approaches are common—window size should capture complete patterns. For detecting daily anomalies, use 24-hour windows (24 points for hourly data). For point anomalies, smaller windows (10-50 points) suffice. For collective anomalies spanning hours or days, larger windows (100-500 points) are needed. The ETT dataset’s dual 15-minute and hourly resolution enables testing different granularities.\n\n\n\n\n\n\nDespite the strong need, comprehensive multi-metric cloud resource datasets (CPU + memory + network + disk I/O combined) are scarce on Hugging Face. Researchers must rely on analogous domains (energy, industrial) or the single Lemma-RCA-NEC cloud dataset with limited documentation. Classic benchmarks like SMD (Server Machine Dataset) and PSM (Pooled Server Metrics) are not on Hugging Face, requiring GitHub or Kaggle sources.\n\n\n\nFew datasets explicitly document seasonal patterns beyond pryshlyak’s dataset. Researchers must analyze other datasets to confirm periodicity presence and characteristics—EDS-lab and openclimatefix note seasonality in descriptions, but specific period strengths aren’t quantified. Consider exploratory analysis (autocorrelation, FFT) before selecting datasets for seasonal algorithm development.\n\n\n\nWidely-cited benchmarks like NAB, Yahoo S5, SMAP/MSL, SWaT, WADI are not directly available on Hugging Face—they exist on GitHub, via direct access requests, or through aggregations like Timeseries-PILE. Researchers wanting these specific datasets must obtain them separately, though Timeseries-PILE includes many in processed form.\n\n\n\nDatasets with cleanest labels and strongest seasonality (pryshlyak, CATS) are synthetic or simulated—they lack real-world messiness. Real-world datasets (electricity-demand, uk_pv, cloud_computing) have authentic complexity but limited or no labels and sparse documentation. Choose based on research stage: controlled synthetic for algorithm development, messy real-world for validation.\n\n\n\nRecent research (Wu & Keogh 2020, Liu & Paparrizos 2024) identified serious quality issues in traditional anomaly detection benchmarks: anomalies too obvious, unrealistic anomaly densities, mislabeled ground truth, and run-to-failure bias. The TSB-UAD and CATS datasets specifically address these concerns with careful curation and precise injection, respectively—prioritize these for rigorous evaluation over less-validated datasets.\n\n\n\n\nHugging Face hosts several strong options for time series anomaly detection research applicable to cloud resource monitoring, though direct cloud datasets remain limited. AutonLab/Timeseries-PILE provides the most comprehensive starting point with 1,980 labeled anomaly time series including web server data, while Lemma-RCA-NEC/Cloud_Computing_Original offers the most direct cloud infrastructure data despite documentation gaps. For algorithm development, pryshlyak/seasonal_time_series_for_anomaly_detection delivers explicit seasonality in a clean format, and patrickfleith/controlled-anomalies-time-series-dataset enables rigorous multivariate evaluation with precise ground truth.\nEnergy domain datasets (EDS-lab/electricity-demand, ETDataset/ett, openclimatefix/uk_pv, electricity_load_diagrams) provide excellent analogs given their resource-like behavior, strong seasonality, and operational monitoring nature—the temporal patterns, external influences, and efficiency concerns directly parallel cloud resources. Combined with the benchmark collections, researchers have sufficient variety to develop, validate, and test anomaly detection algorithms for cloud-analogous time series before deploying to production cloud environments. The datasets span scales from controlled 67k-row experiments to massive 30,000-system deployments, enabling research at multiple stages from proof-of-concept to production readiness."
  },
  {
    "objectID": "concepts/research/timeseries-anomaly-datasets-review.html#tldr",
    "href": "concepts/research/timeseries-anomaly-datasets-review.html#tldr",
    "title": "Hugging Face Datasets for Cloud Resource Anomaly Detection",
    "section": "",
    "text": "Bottom line: Hugging Face offers several strong datasets for cloud resource anomaly detection modeling, though direct cloud infrastructure datasets are limited. The best options combine benchmark collections with labeled anomalies, real cloud platform data, and energy/resource domain datasets that closely parallel cloud behavior.\nTop recommendations: For comprehensive benchmarking, use AutonLab/Timeseries-PILE (1,980 labeled anomaly time series including web server data). For actual cloud infrastructure, use Lemma-RCA-NEC/Cloud_Computing_Original (real cloud platform with 6 fault types). For algorithm development with strong seasonality, use pryshlyak/seasonal_time_series_for_anomaly_detection (explicit weekly patterns). Energy domain datasets (electricity demand, solar generation, grid monitoring) provide excellent cloud analogs with resource consumption patterns, daily/weekly cycles, and efficiency concerns matching cloud workloads.\nKey insight: While dedicated multi-metric cloud datasets (CPU+memory+network combined) are scarce, energy and infrastructure monitoring datasets exhibit remarkably similar characteristics—resource consumption over time, strong temporal patterns from user/load behavior, anomalies from inefficiency or external events, and operational monitoring requirements. These domains provide robust training and evaluation data for cloud-focused anomaly detection research."
  },
  {
    "objectID": "concepts/research/timeseries-anomaly-datasets-review.html#executive-summary-with-dataset-comparison",
    "href": "concepts/research/timeseries-anomaly-datasets-review.html#executive-summary-with-dataset-comparison",
    "title": "Hugging Face Datasets for Cloud Resource Anomaly Detection",
    "section": "",
    "text": "The following table compares the most relevant datasets for cloud resource anomaly detection, ordered by direct applicability to cloud infrastructure monitoring:\n\n\n\n\n\n\n\n\n\n\n\n\nDataset\nDomain\nScale\nSeasonality\nAnomaly Labels\nCloud Relevance\nBest For\n\n\n\n\nLemma-RCA-NEC/Cloud_Computing_Original\nReal cloud platform\nHundreds of entities\nDaily/weekly cycles\n✅ Yes (6 fault types)\n🟢 High - Actual cloud data\nReal cloud fault detection, RCA\n\n\nAutonLab/Timeseries-PILE\nMulti-domain benchmark\n1,980 labeled series (20GB)\nVaries by domain\n✅ Yes (comprehensive)\n🟢 High - Web server subset\nComprehensive benchmarking\n\n\npryshlyak/seasonal_time_series\nSynthetic seasonal\n67,700 points (3 months)\nStrong weekly patterns\n✅ Yes (3 types)\n🟡 Medium - Generic patterns\nSeasonal algorithm development\n\n\npatrickfleith/CATS\nSimulated system\n5M points, 17 variables\nProcess-driven cycles\n✅ Yes (200 precise)\n🟡 Medium - Multivariate\nMultivariate method testing\n\n\nEDS-lab/electricity-demand\nSmart meters\nMulti-building, hourly\nDaily/weekly/seasonal + weather\n⚠️ No (find natural)\n🟡 Medium - Resource consumption\nResource usage patterns\n\n\nETDataset/ett\nElectricity transformers\n2 years, 6 variables\nDaily/seasonal + trends\n⚠️ No (predict failures)\n🟡 Medium - Infrastructure\nMulti-timescale patterns\n\n\nopenclimatefix/uk_pv\nSolar generation\n30,000+ systems (15 years)\nDaily/seasonal strong\n⚠️ Partial (bad_data.csv)\n🟡 Medium - External impacts\nLarge-scale peer comparison\n\n\nelectricity_load_diagrams\nGrid substations\n320 substations (4 years)\nDaily/weekly/seasonal\n⚠️ No (find operational)\n🟡 Medium - Infrastructure\nMulti-entity monitoring\n\n\nTime-MQA/TSQA\nQA-formatted\n37K anomaly QA pairs\nVaries\n✅ Yes (QA format)\n🟡 Medium - AIOps included\nLLM-based approaches\n\n\n\nLegend: 🟢 High relevance (direct cloud data or web servers) | 🟡 Medium relevance (analogous resource behavior) | ✅ Labeled | ⚠️ Unlabeled or partial\n\n\nNeed actual cloud data? → Lemma-RCA-NEC/Cloud_Computing_Original + AutonLab/Timeseries-PILE (web server subset)\nNeed labeled anomalies for supervised learning? → AutonLab/Timeseries-PILE (1,980 series) or patrickfleith/CATS (200 precise labels)\nDeveloping seasonal anomaly detection? → pryshlyak/seasonal_time_series (clean weekly patterns) → validate on EDS-lab/electricity-demand\nTesting multivariate methods? → patrickfleith/CATS (controlled 17-var) → ETDataset/ett (real-world 6-var)\nNeed large-scale evaluation? → openclimatefix/uk_pv (30K+ systems) or electricity_load_diagrams (320 entities)\nExploring LLM-based detection? → Time-MQA/TSQA (37K QA pairs with AIOps domain)"
  },
  {
    "objectID": "concepts/research/timeseries-anomaly-datasets-review.html#detailed-dataset-analysis",
    "href": "concepts/research/timeseries-anomaly-datasets-review.html#detailed-dataset-analysis",
    "title": "Hugging Face Datasets for Cloud Resource Anomaly Detection",
    "section": "",
    "text": "Hugging Face hosts several high-quality datasets suitable for time series anomaly detection modeling analogous to cloud resource usage behavior, though dedicated cloud infrastructure datasets remain limited. The platform offers a mix of real-world operational data, energy consumption patterns, and purpose-built anomaly detection benchmarks—all exhibiting the seasonality, outliers, and temporal dynamics needed for robust anomaly detection research. The most promising datasets span industrial monitoring, energy systems, and large-scale benchmark collections with labeled anomalies."
  },
  {
    "objectID": "concepts/research/timeseries-anomaly-datasets-review.html#most-relevant-datasets-for-cloud-resource-modeling",
    "href": "concepts/research/timeseries-anomaly-datasets-review.html#most-relevant-datasets-for-cloud-resource-modeling",
    "title": "Hugging Face Datasets for Cloud Resource Anomaly Detection",
    "section": "",
    "text": "The datasets below most closely match cloud resource usage patterns with seasonal behavior and labeled anomalies, ordered by relevance to your use case.\n\n\nThis massive collection aggregates 13 million unique time series across 13 domains, specifically designed for foundation model training and evaluation. The dataset’s TSB-UAD (Time-Series Benchmark for Univariate Anomaly Detection) component contains 1,980 labeled time series from 18 anomaly detection datasets, making it the most comprehensive anomaly detection resource on Hugging Face.\nDataset characteristics: The collection spans 20.085 GB with 1.23 billion timestamps total, including data from healthcare, engineering, finance, environment, and critically, web servers—the domain most analogous to cloud infrastructure. The TSB-UAD subset provides both synthetic and real-world anomalies with high variability in types, ratios, and sizes. The dataset also includes the Informer forecasting collection with electricity transformer temperature data, traffic patterns, and weather data—all exhibiting resource-like temporal dynamics.\nSeasonality patterns: Temporal patterns vary by subdataset but comprehensively cover daily cycles, weekly patterns, and seasonal variations. Web server data within TSB-UAD exhibits usage patterns directly comparable to cloud resources. The electricity and traffic datasets demonstrate clear periodic behavior with load-dependent variations.\nAnomaly types: The labeled anomalies span point anomalies (sudden spikes), collective anomalies (sustained unusual patterns), and contextual anomalies (unusual given temporal context). These mirror the anomaly types in cloud environments: point anomalies resemble sudden resource spikes, collective anomalies match degraded performance periods, and contextual anomalies reflect unusual usage given time-of-day expectations.\nWhy suitable for cloud resource modeling: Web server metrics inherently parallel cloud resource behavior—both exhibit request-driven load patterns, have clear daily/weekly seasonality from user activity, and experience anomalies from traffic surges, system failures, or external events. The diversity of 18 source datasets prevents overfitting to specific patterns while the large scale (1,980 labeled series) enables robust model training and evaluation.\nPreprocessing considerations: The dataset comes standardized for the MOMENT foundation model framework but remains accessible via standard tools. Use the TSB-UAD subset specifically for anomaly detection tasks. The data varies in length and amplitude across sources, so normalization by time series is recommended. The collection provides ready-to-use train/test splits, eliminating common temporal leakage issues. Downloaded 28,200 times, indicating strong community validation.\n\n\n\nThis dataset provides actual system metrics and logs from a cloud computing platform with six real fault types injected across hundreds of system entities—making it the most directly applicable to cloud resource anomaly detection on Hugging Face.\nDataset characteristics: The data comes in JSON format containing both system metrics (performance measurements) and logs (event data) from a production cloud computing environment. The dataset captures real operational conditions with hundreds of interconnected system entities, reflecting the complexity of actual cloud infrastructure. While the exact size isn’t specified, the multimodal nature (metrics + logs) provides rich context for anomaly detection.\nFault types labeled: The dataset includes six critical cloud failure modes: cryptojacking (unauthorized resource usage), silent pod degradation (gradual performance decay), malware attacks, GitOps mistakes (configuration errors), configuration change failures, and bug infections. These represent real-world cloud anomalies spanning security breaches, performance degradation, and operational errors—precisely the types of inefficiencies and external impacts relevant to your use case.\nSeasonality patterns: Cloud computing platforms naturally exhibit strong temporal patterns from user activity. Workloads typically show pronounced daily cycles (business hours vs. night), weekly patterns (weekday vs. weekend usage), and potential seasonal variations from business cycles or world events. The time-series format with timestamps enables analysis of these periodic patterns.\nWhy suitable: This dataset directly addresses the cloud resource use case unlike other datasets that require analogy. The fault types mirror real cloud anomalies: cryptojacking represents inefficient resource usage, pod degradation shows performance issues, and the others reflect external impacts from attacks or human errors. The multi-entity structure parallels distributed cloud architectures with many interconnected services.\nPreprocessing considerations: The JSON format requires parsing into time-series structure. Given hundreds of entities, feature selection or dimensionality reduction may be necessary. The dataset is designed for Root Cause Analysis (RCA), providing attribution for which entities are affected by each fault—valuable for understanding anomaly propagation in distributed systems. Note the CC-BY-ND-4.0 license restricts derivative works. The dataset viewer has errors, so programmatic access via the datasets library is recommended.\n\n\n\nThis dataset was explicitly designed for seasonal anomaly detection, making it ideal for developing and testing algorithms that leverage periodic patterns. Based on the Numenta Anomaly Benchmark but restructured to emphasize weekly seasonality, it provides clean labeled data for methodical algorithm development.\nDataset characteristics: The dataset contains 67,700 rows with 5-minute sampling intervals spanning three months. Data is organized by day of week (seven separate CSVs for Monday-Sunday with 3,745 rows each) plus weekly aggregations (2,017 rows each). This structure directly supports periodicity-based anomaly detection approaches. The format is minimal—timestamp and value columns only—keeping focus on temporal patterns.\nSeasonality patterns: The dataset exhibits strong weekly periodicity with distinct patterns for each weekday, directly analogous to cloud resources that experience different usage on weekdays versus weekends. The 5-minute granularity captures intra-day variations like morning startup, lunch dips, and evening shutdowns common in business applications. The three-month span covers sufficient cycles for learning robust seasonal patterns.\nAnomaly characteristics: Training data includes seven weekday files and one normal week file with no anomalies—ideal for unsupervised learning. Testing data contains three types: collective anomaly downward (Monday file, like sustained performance degradation), collective anomaly upward (Wednesday file, like traffic surge), and point anomaly (Saturday file, like sudden spike). These anomaly types directly map to cloud resource scenarios: downward collective anomalies represent underutilization or failures, upward anomalies represent unusual demand, and point anomalies represent isolated incidents.\nWhy suitable: The explicit seasonal structure mirrors cloud workloads where different days exhibit different patterns—weekday business traffic differs from weekend consumer traffic. The clean separation of training (normal) and testing (anomalous) data supports supervised, semi-supervised, and unsupervised approaches. The dataset’s design for auto-encoder training generalizes to any anomaly detection technique exploiting periodicity.\nPreprocessing considerations: Data is pre-cleaned with no missing values and positive-only values. The day-of-week split enables training separate models per period—the thesis approach—or pooling for general models. Timestamps are artificial (January-March 2024) so absolute dates don’t matter, only relative temporal positions. Best for developing seasonal algorithms before applying to messier real-world data. Consider this a controlled environment for algorithm validation.\n\n\n\nFor researchers needing multivariate anomaly detection with precise ground truth, CATS provides 200 exactly labeled anomalies across 17 variables in a 5-million-point simulated system—offering unprecedented control for rigorous algorithm evaluation.\nDataset characteristics: CATS simulates a complex dynamical system (analogous to industrial control systems or building management) with 17 variables split into 4 control commands, 3 environmental stimuli, and 10 telemetry readings (temperature, pressure, voltage, current, position, velocity, acceleration, humidity). The 1Hz sampling rate provides second-by-second resolution over an extended operational period. The first 1 million points contain only nominal behavior (ideal for unsupervised training), while the next 4 million mix normal and anomalous segments.\nSeasonality patterns: The simulated system exhibits operational cycles from the control logic and environmental interactions—periodic patterns from processes like heating/cooling cycles, position movements, or state machines. While not traditional daily/weekly seasonality, these represent the process-driven periodicity found in automated systems, directly relevant to auto-scaling cloud resources or scheduled batch jobs.\nAnomaly characteristics: All 200 anomalies include precise metadata: exact start/end times, root cause channel, affected channels, and anomaly category. This eliminates the ground truth ambiguity plaguing many benchmark datasets. The anomalies have controlled injection, meaning researchers can isolate algorithm performance from data quality issues. Metadata supports root cause analysis—identifying not just that an anomaly occurred but which variable caused it and which variables were affected.\nWhy suitable for cloud modeling: Modern cloud environments are increasingly multivariate—CPU, memory, network, disk I/O, latency, and error rates all interact. CATS’s multivariate structure with known dependencies mirrors this. The control commands parallel API calls or configuration changes that affect system state. Environmental stimuli represent external load or conditions. Telemetry readings parallel observability metrics. The pure signal with no noise provides a baseline; researchers can add custom noise levels to test robustness—valuable for understanding algorithm behavior under varying data quality.\nPreprocessing considerations: The dataset is pristine with no missing data—unrealistic for production but perfect for controlled experiments. Add synthetic noise or missing data windows to test real-world resilience. The root cause labels enable evaluation of not just detection but attribution algorithms. Use the first 1 million points for semi-supervised approaches (novelty detection) or include contamination for unsupervised scenarios. The multivariate nature requires techniques handling variable interactions—VAE, LSTM-VAE, or graph neural networks."
  },
  {
    "objectID": "concepts/research/timeseries-anomaly-datasets-review.html#energy-and-resource-utilization-datasets",
    "href": "concepts/research/timeseries-anomaly-datasets-review.html#energy-and-resource-utilization-datasets",
    "title": "Hugging Face Datasets for Cloud Resource Anomaly Detection",
    "section": "",
    "text": "These datasets from energy domains provide excellent analogs to cloud resources given their strong seasonality, resource-like behavior, and operational monitoring nature.\n\n\nSmart meter electricity consumption closely parallels cloud resource consumption—both represent resource usage over time, exhibit strong temporal patterns, and experience demand variations from user behavior and external conditions.\nDataset characteristics: This harmonized collection aggregates multiple smart meter datasets with hourly sampling across residential and commercial buildings. The data comes in three components: demand.parquet (consumption time series with unique_id, timestamp, y in kWh), metadata.parquet (building_class, cluster_size, location), and weather.parquet (25+ weather variables including temperature, humidity, precipitation, solar radiation, wind speed). The multi-building structure provides numerous parallel time series for comparative analysis.\nSeasonality patterns: Electricity demand shows pronounced patterns directly analogous to cloud resources. Daily cycles reflect business hours for commercial buildings or home activity for residential—matching cloud application usage. Weekly cycles distinguish weekdays from weekends—mirroring reduced weekend traffic for business applications. Seasonal variations from heating/cooling loads parallel seasonal e-commerce patterns (holiday shopping) or tax season spikes. The strong correlation with weather (temperature especially) demonstrates how external factors drive consumption—just as world events or viral content drive cloud traffic.\nAnomaly opportunities: While unlabeled, natural anomalies abound: equipment malfunctions (sudden drops or spikes), unusual consumption (vacant building with high usage suggesting waste), meter reading errors (negative values or impossibly high readings), or weather-adjusted anomalies (high usage on mild day). The weather covariates enable sophisticated contextual anomaly detection—flagging consumption unusual for the conditions, analogous to detecting high cloud resource usage during low user activity periods.\nWhy suitable: The parallels are strong: both are resource consumption metrics with strong time-of-day/day-of-week patterns, both have a “correct” expected baseline with deviations indicating issues, both are influenced by external factors (weather vs. user behavior), and both seek to identify inefficient usage. The building metadata enables clustering similar usage profiles—like grouping similar microservices or customer workloads.\nPreprocessing for anomaly detection: The rich weather covariates suggest multivariate anomaly detection incorporating context. Normalize consumption by building capacity or size for fair comparison. Use the multiple buildings for peer-comparison approaches—flagging buildings as anomalous if they deviate from similar buildings. The location data enables spatial analysis. Test algorithms for weather-adjusted anomaly detection—a valuable capability for cloud resources when expected load varies by time or external factors. Licensed under BSD 3-clause for flexible use.\n\n\n\nThis dataset provides 2 years of electricity transformer operational data from critical infrastructure—equipment failure here has severe consequences, making anomaly detection crucial.\nDataset characteristics: Four variants (ETT-h1, ETT-h2, ETT-m1, ETT-m2) from two transformers at two stations provide both hourly (17,520 points) and 15-minute (70,080 points) resolution data spanning 2016-2018. The target variable is Oil Temperature (OT), a critical safety indicator—overheating damages transformers. Six load features provide context: High/Middle/Low UseFul Load and High/Middle/Low UseLess Load, capturing the transformer’s operating conditions.\nSeasonality patterns: The dataset explicitly exhibits short-term periodical patterns (daily load cycles from grid usage), long-term periodical patterns (seasonal variations from weather-dependent demand), long-term trends (equipment aging/degradation), and complex irregular patterns. This combination of pattern types makes it excellent for testing robust algorithms that must handle multiple timescales—exactly the challenge in cloud resources with daily patterns, weekly patterns, and long-term growth trends.\nAnomaly detection value: Oil temperature anomalies indicate transformer malfunction risk—overheating from excess load, cooling system failure, or internal electrical issues. These parallel cloud resource anomalies: excess load, insufficient capacity, or component failures. False predictions can damage equipment—the same high-stakes environment as cloud anomaly detection where false alarms cause alert fatigue and missed detections cause outages.\nWhy suitable: The load features + temperature structure mirrors cloud metrics (CPU/memory/network load + response time/error rate). Both domains involve resource allocation, capacity planning, and failure prevention. The multi-transformer setup provides multiple parallel series for comparative analysis. The predictive maintenance application—detecting issues before failure—directly parallels cloud workload management.\nPreprocessing considerations: Pre-split into train/val/test (12/4/4 months) for consistent evaluation. The multivariate structure (6 load features + temperature) enables testing correlation-based anomaly detection—flagging temperature unusual given load conditions. The 15-minute variants provide higher resolution for detecting rapid-onset anomalies. Use the long-term trends to test algorithms robust to non-stationarity. Well-documented and widely used in time series research (from the Informer paper), ensuring reproducibility. CC-BY-4.0 license.\n\n\n\nWith over 30,000 solar PV systems tracked from 2010-2025, this dataset provides exceptional scale for anomaly detection research, plus partial ground truth via labeled bad data periods.\nDataset characteristics: The dataset covers 15 years of domestic solar installations across Great Britain with two resolution levels: 30-minute intervals (30,000+ systems, high quality cumulative energy) and 5-minute intervals (1,309 systems, instantaneous but noisy). Systems range from 0.47 kW to 250 kW capacity. Metadata includes latitude/longitude, panel orientation, tilt angle, and capacity. Critically, bad_data.csv identifies known periods of data quality issues—providing partial ground truth for anomaly detection evaluation.\nSeasonality patterns: Solar generation exhibits the strongest possible seasonal patterns: zero nighttime generation, predictable daily curves (sunrise ramp, midday peak, sunset decline), seasonal variation (long summer days, short winter days), and weather dependency (cloud cover causes rapid generation drops). Geographic spread across Britain provides diverse weather patterns. These characteristics parallel cloud resources with predictable baseline patterns disrupted by external events—like solar generation disrupted by clouds, cloud resources are disrupted by traffic events.\nAnomaly characteristics: The bad_data.csv provides labeled anomalies including: equipment malfunctions, negative generation values (sensor errors), excessive values (&gt;750× capacity, clearly impossible), nighttime generation (indicates clock errors), and zero generation during sunny periods (equipment failure). Beyond these labeled cases, natural unlabeled anomalies exist from inverter failures, panel degradation, shading issues, and soiling (dirt reducing output).\nWhy suitable: The massive scale (30,000+ systems) enables testing at cloud-scale where thousands of services or instances require monitoring. The partial ground truth balances labeled data for validation with unlabeled realistic data for unsupervised approaches. The geospatial dimension enables peer-comparison anomaly detection—flagging systems underperforming compared to nearby systems with similar conditions. The real-world messiness (missing data, quality issues) mirrors production cloud data.\nPreprocessing considerations: Extensive data cleaning guidance is provided. Use bad_data.csv as ground truth test set. Normalize generation by kWp capacity for fair comparison across system sizes. The location data enables clustering by geographic region or weather patterns. Handle missing midnight readings and gaps documented in the dataset. The 5-minute data is noisy—test algorithm robustness to noise. Gated dataset requiring access approval, but readily granted for research. DOI: 10.57967/hf/0878.\n\n\n\nThis dataset provides 4 years of Portuguese electricity grid substation data with 370 substations sampled every 15 minutes—a scale and operational focus directly relevant to infrastructure monitoring.\nDataset characteristics: Originally 15-minute sampling resampled to hourly, spanning 2011-2014. The LSTNet benchmark configuration uses 320 substations active during 2012-2014, providing consistent time series for algorithm comparison. The data captures actual grid load in kW across a national electricity system—critical infrastructure where anomalies indicate equipment issues, unexpected demand, or grid instability.\nSeasonality patterns: Electricity grid load exhibits textbook periodicity: 24-hour daily cycles (morning ramp-up, evening peak, overnight minimum), weekly patterns (weekday business/industrial load vs. weekend residential-dominated load), and seasonal variations (summer air conditioning, winter heating). These patterns mirror cloud infrastructure serving business applications with clear usage rhythms.\nAnomaly opportunities: Though unlabeled, natural anomalies include equipment failures (sudden drops), unexpected demand (unusual spikes), grid instability (rapid fluctuations), or seasonal anomalies (unusual load for the weather). The dataset documents known quirks: daylight saving time transitions create 23-hour days (March) and 25-hour days (October)—realistic complications that anomaly detectors must handle. No missing values simplifies initial development.\nWhy suitable: Multiple substations enable comparative anomaly detection—identifying substations behaving differently from peers. The hourly resolution matches common cloud metric collection intervals. The 4-year span covers sufficient seasonal cycles for learning robust patterns. The infrastructure monitoring domain parallels cloud monitoring—both involve critical systems requiring high availability where anomalies indicate serious issues.\nPreprocessing considerations: Account for daylight saving transitions where March has zeros 1-2am (23 hours) and October aggregates 1-2am (25 hours). The resampling to hourly from 15-minute data smooths some noise but may miss rapid anomalies—consider accessing original 15-minute data if available. Use the LSTNet configuration (320 substations, 2012-2014) for benchmark comparison. The dataset is clean and well-documented, used in multiple research papers for validation. Source: UCI Machine Learning Repository."
  },
  {
    "objectID": "concepts/research/timeseries-anomaly-datasets-review.html#specialized-and-qa-format-datasets",
    "href": "concepts/research/timeseries-anomaly-datasets-review.html#specialized-and-qa-format-datasets",
    "title": "Hugging Face Datasets for Cloud Resource Anomaly Detection",
    "section": "",
    "text": "This unique dataset reformulates time series tasks as question-answering pairs, enabling LLM-based approaches to anomaly detection—a frontier research direction.\nDataset characteristics: Approximately 200,000 QA pairs across 12 real-world domains with 37,000 instances specifically for anomaly detection. The dataset includes healthcare (EEG, ECG), finance, energy, IoT, environment, transport, web traffic, and critically AIOps (cloud monitoring) domain. Multiple formats are provided: 6,919 true/false questions, 11,281 multiple-choice questions, and 12,510 open-ended questions. Pre-trained models are available (Mistral 7B, Qwen-2.5 7B, Llama-3 8B).\nWhy relevant: The AIOps domain explicitly includes cloud monitoring data formatted for question answering. This enables approaches where models reason about time series rather than just pattern-match—answering questions like “Is this usage pattern anomalous given it’s Monday morning?” or “What caused this traffic spike?” The multi-task format (forecasting, imputation, anomaly detection, classification, open-ended reasoning) enables transfer learning where representations learned from one task improve others.\nUse case: For researchers exploring LLM-based anomaly detection or multi-task time series models, this provides ready-to-use training data. The context enhancement (auxiliary textual descriptions) helps models understand domain semantics—e.g., explaining that Monday mornings typically have high traffic helps contextual anomaly detection. Downloaded 262 times despite recent publication (ACL 2025), indicating strong interest.\nPreprocessing considerations: The QA format requires different architectures than traditional time series models—use sequence-to-sequence models or LLMs fine-tuned on time series. The dataset is designed for continued pre-training of foundation models. Consider how to convert raw time series into this format if generating additional training data. The multiple-choice and true/false formats enable classification-based evaluation."
  },
  {
    "objectID": "concepts/research/timeseries-anomaly-datasets-review.html#dataset-selection-guidance",
    "href": "concepts/research/timeseries-anomaly-datasets-review.html#dataset-selection-guidance",
    "title": "Hugging Face Datasets for Cloud Resource Anomaly Detection",
    "section": "",
    "text": "Primary choice: Lemma-RCA-NEC/Cloud_Computing_Original provides actual cloud platform metrics with real fault types, though limited documentation requires hands-on exploration. Supplement with AutonLab/Timeseries-PILE’s web server data for additional cloud-analogous patterns.\n\n\n\nPrimary choice: pryshlyak/seasonal_time_series_for_anomaly_detection offers clean, explicitly seasonal data ideal for developing and validating periodicity-based algorithms. Once proven on this controlled dataset, test on messier real-world data like EDS-lab/electricity-demand or electricity_load_diagrams.\n\n\n\nPrimary choice: AutonLab/Timeseries-PILE provides the most extensive benchmark with 1,980 labeled anomaly time series from 18 datasets. The TSB-UAD subset specifically addresses dataset quality issues plaguing older benchmarks. Use this for broad evaluation across diverse patterns and anomaly types.\n\n\n\nPrimary choice: patrickfleith/controlled-anomalies-time-series-dataset (CATS) offers 17 variables with precise root cause labels—ideal for developing and testing multivariate algorithms. Follow with ETDataset/ett for real-world multivariate data at scale.\n\n\n\nPrimary choice: openclimatefix/uk_pv with 30,000+ systems tests algorithmic scalability and peer-comparison approaches. Alternatively, electricity_load_diagrams with 320 substations provides a smaller but still substantial multi-entity dataset."
  },
  {
    "objectID": "concepts/research/timeseries-anomaly-datasets-review.html#key-preprocessing-considerations-across-datasets",
    "href": "concepts/research/timeseries-anomaly-datasets-review.html#key-preprocessing-considerations-across-datasets",
    "title": "Hugging Face Datasets for Cloud Resource Anomaly Detection",
    "section": "",
    "text": "Most datasets benefit from per-time-series normalization (z-score standardization) to account for different scales—one building’s 10 kW load differs from another’s 1000 kW load, but both may show similar relative patterns. For datasets with metadata (capacity, size), consider normalizing by these physical properties. For multivariate datasets like CATS or ETT, normalize each variable independently to prevent high-magnitude variables dominating.\n\n\n\nAlgorithms exploiting seasonality require sufficient cycles for learning—at least 2-3 complete periods. For daily patterns, 2-3 days suffices; for weekly patterns, 2-3 weeks; for seasonal patterns, 1-2 years. Use techniques like seasonal decomposition (STL) to explicitly model and remove seasonality, with residuals analyzed for anomalies. Alternatively, use day-of-week and hour-of-day encodings as features. The pryshlyak dataset’s pre-split by weekday demonstrates one approach.\n\n\n\nAlways use temporal splits, not random splits—time series have temporal dependencies making random splits leak information. The typical split is chronological: first 60-70% training, remaining testing. Ensure training data precedes test data temporally. For semi-supervised approaches, training should contain only normal data (or &lt;5% contamination). For unsupervised approaches, training may include realistic anomaly rates.\n\n\n\nReal-world datasets like openclimatefix/uk_pv have missing readings common in production. Strategies include: forward-fill for short gaps (carry last valid value), interpolation for medium gaps (linear or spline), masking and imputation modeling for long gaps, or excluding time series with excessive missingness. Test algorithm robustness to missing data patterns. CATS’s completeness provides a baseline; add synthetic gaps to test handling.\n\n\n\nMost energy and resource datasets lack anomaly labels—use these for unsupervised approaches (isolation forest, LOF, autoencoders) or semi-supervised approaches (one-class SVM, VAE). Alternatively, use domain knowledge to label obvious anomalies (e.g., negative electricity generation, usage 10× typical) or leverage labeled datasets for training with transfer learning to unlabeled domains.\n\n\n\nSliding window approaches are common—window size should capture complete patterns. For detecting daily anomalies, use 24-hour windows (24 points for hourly data). For point anomalies, smaller windows (10-50 points) suffice. For collective anomalies spanning hours or days, larger windows (100-500 points) are needed. The ETT dataset’s dual 15-minute and hourly resolution enables testing different granularities."
  },
  {
    "objectID": "concepts/research/timeseries-anomaly-datasets-review.html#important-gaps-and-limitations",
    "href": "concepts/research/timeseries-anomaly-datasets-review.html#important-gaps-and-limitations",
    "title": "Hugging Face Datasets for Cloud Resource Anomaly Detection",
    "section": "",
    "text": "Despite the strong need, comprehensive multi-metric cloud resource datasets (CPU + memory + network + disk I/O combined) are scarce on Hugging Face. Researchers must rely on analogous domains (energy, industrial) or the single Lemma-RCA-NEC cloud dataset with limited documentation. Classic benchmarks like SMD (Server Machine Dataset) and PSM (Pooled Server Metrics) are not on Hugging Face, requiring GitHub or Kaggle sources.\n\n\n\nFew datasets explicitly document seasonal patterns beyond pryshlyak’s dataset. Researchers must analyze other datasets to confirm periodicity presence and characteristics—EDS-lab and openclimatefix note seasonality in descriptions, but specific period strengths aren’t quantified. Consider exploratory analysis (autocorrelation, FFT) before selecting datasets for seasonal algorithm development.\n\n\n\nWidely-cited benchmarks like NAB, Yahoo S5, SMAP/MSL, SWaT, WADI are not directly available on Hugging Face—they exist on GitHub, via direct access requests, or through aggregations like Timeseries-PILE. Researchers wanting these specific datasets must obtain them separately, though Timeseries-PILE includes many in processed form.\n\n\n\nDatasets with cleanest labels and strongest seasonality (pryshlyak, CATS) are synthetic or simulated—they lack real-world messiness. Real-world datasets (electricity-demand, uk_pv, cloud_computing) have authentic complexity but limited or no labels and sparse documentation. Choose based on research stage: controlled synthetic for algorithm development, messy real-world for validation.\n\n\n\nRecent research (Wu & Keogh 2020, Liu & Paparrizos 2024) identified serious quality issues in traditional anomaly detection benchmarks: anomalies too obvious, unrealistic anomaly densities, mislabeled ground truth, and run-to-failure bias. The TSB-UAD and CATS datasets specifically address these concerns with careful curation and precise injection, respectively—prioritize these for rigorous evaluation over less-validated datasets."
  },
  {
    "objectID": "concepts/research/timeseries-anomaly-datasets-review.html#conclusion",
    "href": "concepts/research/timeseries-anomaly-datasets-review.html#conclusion",
    "title": "Hugging Face Datasets for Cloud Resource Anomaly Detection",
    "section": "",
    "text": "Hugging Face hosts several strong options for time series anomaly detection research applicable to cloud resource monitoring, though direct cloud datasets remain limited. AutonLab/Timeseries-PILE provides the most comprehensive starting point with 1,980 labeled anomaly time series including web server data, while Lemma-RCA-NEC/Cloud_Computing_Original offers the most direct cloud infrastructure data despite documentation gaps. For algorithm development, pryshlyak/seasonal_time_series_for_anomaly_detection delivers explicit seasonality in a clean format, and patrickfleith/controlled-anomalies-time-series-dataset enables rigorous multivariate evaluation with precise ground truth.\nEnergy domain datasets (EDS-lab/electricity-demand, ETDataset/ett, openclimatefix/uk_pv, electricity_load_diagrams) provide excellent analogs given their resource-like behavior, strong seasonality, and operational monitoring nature—the temporal patterns, external influences, and efficiency concerns directly parallel cloud resources. Combined with the benchmark collections, researchers have sufficient variety to develop, validate, and test anomaly detection algorithms for cloud-analogous time series before deploying to production cloud environments. The datasets span scales from controlled 67k-row experiments to massive 30,000-system deployments, enabling research at multiple stages from proof-of-concept to production readiness."
  },
  {
    "objectID": "how-to/generate-synthetic-data.html",
    "href": "how-to/generate-synthetic-data.html",
    "title": "Generate Synthetic Cloud Data",
    "section": "",
    "text": "Step-by-step instructions for creating production-ready synthetic cloud resource datasets.\n\n\n# Install with data generation dependencies\nuv pip install cloud-resource-simulator\n\n\n\n\n\n\n\nfrom cloud_sim.data_generation import WorkloadPatternGenerator, WorkloadType\nfrom datetime import datetime, timedelta\n\n# Initialize generator\ngenerator = WorkloadPatternGenerator(seed=42)\n\n# Generate 30 days of web application data\ndf = generator.generate_time_series(\n    workload_type=WorkloadType.WEB_APP,\n    start_time=datetime.now() - timedelta(days=30),\n    end_time=datetime.now(),\n    interval_minutes=60\n)\n\nprint(f\"Generated {len(df):,} samples\")\n\nOutput columns: - timestamp, cpu_utilization, memory_utilization - network_in_mbps, network_out_mbps, disk_iops - efficiency_score, waste_percentage - is_idle, is_overprovisioned\n\n\n\n\n\n\n\n\nimport polars as pl\n\nworkloads = {\n    WorkloadType.WEB_APP: \"web_app_data.parquet\",\n    WorkloadType.DATABASE_OLTP: \"database_data.parquet\",\n    WorkloadType.ML_TRAINING: \"ml_training_data.parquet\"\n}\n\nfor workload_type, filename in workloads.items():\n    df = generator.generate_time_series(\n        workload_type=workload_type,\n        start_time=datetime.now() - timedelta(days=90),\n        end_time=datetime.now(),\n        interval_minutes=5  # 5-minute granularity\n    )\n\n    df.write_parquet(filename)\n    print(f\"✓ Saved {filename}: {len(df):,} samples\")\n\n\n\n\n\n\n# Generate data with 2% anomaly rate\ndf = generator.generate_time_series(\n    workload_type=WorkloadType.WEB_APP,\n    start_time=datetime.now() - timedelta(days=30),\n    end_time=datetime.now(),\n    interval_minutes=60,\n    include_anomalies=True,\n    anomaly_rate=0.02  # 2% anomalies\n)\n\n# Check anomaly distribution\nanomaly_count = df.filter(pl.col('is_anomaly')).height\nprint(f\"Anomalies: {anomaly_count} ({anomaly_count/len(df)*100:.2f}%)\")\n\nAnomaly types: - CPU/memory spikes - Network saturation - Disk I/O bottlenecks - Complete failures (zero utilization)\n\n\n\n\n\n# Specific date range\nfrom datetime import datetime\n\ndf = generator.generate_time_series(\n    workload_type=WorkloadType.DATABASE_OLTP,\n    start_time=datetime(2024, 1, 1, 0, 0),\n    end_time=datetime(2024, 12, 31, 23, 59),\n    interval_minutes=15  # 15-minute intervals\n)\n\n# Full year of data\nprint(f\"Dataset spans: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n\n\n\n\n\n\nfrom datasets import Dataset\n\n# Convert Polars to HuggingFace format\nhf_dataset = Dataset.from_pandas(df.to_pandas())\n\n# Add metadata\nhf_dataset = hf_dataset.with_format(\"polars\")\n\n# Save locally\nhf_dataset.save_to_disk(\"cloud_sim_data\")\n\n# Or push to HuggingFace Hub\n# hf_dataset.push_to_hub(\"username/cloud-resource-sim\")\n\n\n\n\n\n\n# Generate fleet of resources\nresource_data = []\n\nfor resource_id in range(100):\n    df = generator.generate_time_series(\n        workload_type=WorkloadType.MICROSERVICES,\n        start_time=datetime.now() - timedelta(days=7),\n        end_time=datetime.now(),\n        interval_minutes=5\n    )\n\n    # Add resource identifier\n    df = df.with_columns(pl.lit(f\"resource-{resource_id:03d}\").alias(\"resource_id\"))\n    resource_data.append(df)\n\n# Combine all resources\nfleet_df = pl.concat(resource_data)\nprint(f\"Fleet data: {len(fleet_df):,} total samples across 100 resources\")\n\n\n\n\n\n\n\n\nFor custom scenarios, use CloudMetricsSimulator:\n\nfrom cloud_sim.data_generation import CloudMetricsSimulator\n\nsimulator = CloudMetricsSimulator(\n    num_resources=50,\n    start_date=datetime(2024, 1, 1),\n    end_date=datetime(2024, 12, 31),\n    sampling_interval_minutes=5,\n    cloud_providers=[\"AWS\", \"Azure\", \"GCP\"],\n    resource_types=[\"compute\", \"storage\", \"network\"]\n)\n\ndf = simulator.generate_dataset(\n    include_anomalies=True,\n    anomaly_rate=0.03,\n    include_unit_economics=True  # Add cost data\n)\n\nprint(df.head())\n\n\n\n\n\n\n\n\n\ndf.write_parquet(\"data.parquet\", compression=\"snappy\")\n\nBenefits: - Columnar format (fast analytics) - Excellent compression - Preserves data types\n\n\n\n\ndf.write_csv(\"data.csv\")\n\nWhen to use: - Need Excel compatibility - External tool integration - Human inspection\n\n\n\n\n# Keep in memory for immediate analysis\ndf.select(['timestamp', 'cpu_utilization', 'memory_utilization'])\n\n\n\n\n\n\nBefore using generated data:\n\nTemporal range: Check start/end timestamps\nSample count: Verify expected number of rows\nAnomaly rate: Confirm anomaly percentage\nStatistics: Compare to research benchmarks (13% CPU, 20% memory)\nNo nulls: df.null_count().sum() should be 0\nTemporal order: Timestamps should be sorted\n\n\n# Quick validation\nprint(f\"Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\nprint(f\"Sample count: {len(df):,}\")\nprint(f\"Anomalies: {df['is_anomaly'].sum()} ({df['is_anomaly'].mean()*100:.2f}%)\")\nprint(f\"Mean CPU: {df['cpu_utilization'].mean():.1f}%\")\nprint(f\"Mean Memory: {df['memory_utilization'].mean():.1f}%\")\nprint(f\"Nulls: {df.null_count().sum()}\")\n\n\n\n\n\n\n\nSolution: Check workload type selection\n\n# Different types have different characteristics\n# Web apps: 15-25% CPU\n# ML training: 70-90% CPU\n# Dev environments: 5-10% CPU\n\n\n\n\nSolution: Adjust anomaly rate\n\n# Operational systems: 0.5-2%\ndf = generator.generate_time_series(..., anomaly_rate=0.01)\n\n# Research/testing: 5-10%\ndf = generator.generate_time_series(..., anomaly_rate=0.08)\n\n\n\n\nSolution: Use streaming generation\n\n# Generate in chunks\nfor month in range(1, 13):\n    start = datetime(2024, month, 1)\n    end = datetime(2024, month + 1, 1) if month &lt; 12 else datetime(2025, 1, 1)\n\n    df_chunk = generator.generate_time_series(\n        workload_type=WorkloadType.WEB_APP,\n        start_time=start,\n        end_time=end,\n        interval_minutes=5\n    )\n\n    df_chunk.write_parquet(f\"data_2024_{month:02d}.parquet\")\n\n\n\n\n\n\n\nTutorial: Data Exploration - Learn the patterns\nTutorial: Workload Signatures - Understand archetypes\nAPI Reference - Complete API docs",
    "crumbs": [
      "Home",
      "How-To Guides",
      "Generate Synthetic Cloud Data"
    ]
  },
  {
    "objectID": "how-to/generate-synthetic-data.html#prerequisites",
    "href": "how-to/generate-synthetic-data.html#prerequisites",
    "title": "Generate Synthetic Cloud Data",
    "section": "",
    "text": "# Install with data generation dependencies\nuv pip install cloud-resource-simulator",
    "crumbs": [
      "Home",
      "How-To Guides",
      "Generate Synthetic Cloud Data"
    ]
  },
  {
    "objectID": "how-to/generate-synthetic-data.html#quick-start",
    "href": "how-to/generate-synthetic-data.html#quick-start",
    "title": "Generate Synthetic Cloud Data",
    "section": "",
    "text": "from cloud_sim.data_generation import WorkloadPatternGenerator, WorkloadType\nfrom datetime import datetime, timedelta\n\n# Initialize generator\ngenerator = WorkloadPatternGenerator(seed=42)\n\n# Generate 30 days of web application data\ndf = generator.generate_time_series(\n    workload_type=WorkloadType.WEB_APP,\n    start_time=datetime.now() - timedelta(days=30),\n    end_time=datetime.now(),\n    interval_minutes=60\n)\n\nprint(f\"Generated {len(df):,} samples\")\n\nOutput columns: - timestamp, cpu_utilization, memory_utilization - network_in_mbps, network_out_mbps, disk_iops - efficiency_score, waste_percentage - is_idle, is_overprovisioned",
    "crumbs": [
      "Home",
      "How-To Guides",
      "Generate Synthetic Cloud Data"
    ]
  },
  {
    "objectID": "how-to/generate-synthetic-data.html#common-tasks",
    "href": "how-to/generate-synthetic-data.html#common-tasks",
    "title": "Generate Synthetic Cloud Data",
    "section": "",
    "text": "import polars as pl\n\nworkloads = {\n    WorkloadType.WEB_APP: \"web_app_data.parquet\",\n    WorkloadType.DATABASE_OLTP: \"database_data.parquet\",\n    WorkloadType.ML_TRAINING: \"ml_training_data.parquet\"\n}\n\nfor workload_type, filename in workloads.items():\n    df = generator.generate_time_series(\n        workload_type=workload_type,\n        start_time=datetime.now() - timedelta(days=90),\n        end_time=datetime.now(),\n        interval_minutes=5  # 5-minute granularity\n    )\n\n    df.write_parquet(filename)\n    print(f\"✓ Saved {filename}: {len(df):,} samples\")\n\n\n\n\n\n\n# Generate data with 2% anomaly rate\ndf = generator.generate_time_series(\n    workload_type=WorkloadType.WEB_APP,\n    start_time=datetime.now() - timedelta(days=30),\n    end_time=datetime.now(),\n    interval_minutes=60,\n    include_anomalies=True,\n    anomaly_rate=0.02  # 2% anomalies\n)\n\n# Check anomaly distribution\nanomaly_count = df.filter(pl.col('is_anomaly')).height\nprint(f\"Anomalies: {anomaly_count} ({anomaly_count/len(df)*100:.2f}%)\")\n\nAnomaly types: - CPU/memory spikes - Network saturation - Disk I/O bottlenecks - Complete failures (zero utilization)\n\n\n\n\n\n# Specific date range\nfrom datetime import datetime\n\ndf = generator.generate_time_series(\n    workload_type=WorkloadType.DATABASE_OLTP,\n    start_time=datetime(2024, 1, 1, 0, 0),\n    end_time=datetime(2024, 12, 31, 23, 59),\n    interval_minutes=15  # 15-minute intervals\n)\n\n# Full year of data\nprint(f\"Dataset spans: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n\n\n\n\n\n\nfrom datasets import Dataset\n\n# Convert Polars to HuggingFace format\nhf_dataset = Dataset.from_pandas(df.to_pandas())\n\n# Add metadata\nhf_dataset = hf_dataset.with_format(\"polars\")\n\n# Save locally\nhf_dataset.save_to_disk(\"cloud_sim_data\")\n\n# Or push to HuggingFace Hub\n# hf_dataset.push_to_hub(\"username/cloud-resource-sim\")\n\n\n\n\n\n\n# Generate fleet of resources\nresource_data = []\n\nfor resource_id in range(100):\n    df = generator.generate_time_series(\n        workload_type=WorkloadType.MICROSERVICES,\n        start_time=datetime.now() - timedelta(days=7),\n        end_time=datetime.now(),\n        interval_minutes=5\n    )\n\n    # Add resource identifier\n    df = df.with_columns(pl.lit(f\"resource-{resource_id:03d}\").alias(\"resource_id\"))\n    resource_data.append(df)\n\n# Combine all resources\nfleet_df = pl.concat(resource_data)\nprint(f\"Fleet data: {len(fleet_df):,} total samples across 100 resources\")",
    "crumbs": [
      "Home",
      "How-To Guides",
      "Generate Synthetic Cloud Data"
    ]
  },
  {
    "objectID": "how-to/generate-synthetic-data.html#advanced-configuration",
    "href": "how-to/generate-synthetic-data.html#advanced-configuration",
    "title": "Generate Synthetic Cloud Data",
    "section": "",
    "text": "For custom scenarios, use CloudMetricsSimulator:\n\nfrom cloud_sim.data_generation import CloudMetricsSimulator\n\nsimulator = CloudMetricsSimulator(\n    num_resources=50,\n    start_date=datetime(2024, 1, 1),\n    end_date=datetime(2024, 12, 31),\n    sampling_interval_minutes=5,\n    cloud_providers=[\"AWS\", \"Azure\", \"GCP\"],\n    resource_types=[\"compute\", \"storage\", \"network\"]\n)\n\ndf = simulator.generate_dataset(\n    include_anomalies=True,\n    anomaly_rate=0.03,\n    include_unit_economics=True  # Add cost data\n)\n\nprint(df.head())",
    "crumbs": [
      "Home",
      "How-To Guides",
      "Generate Synthetic Cloud Data"
    ]
  },
  {
    "objectID": "how-to/generate-synthetic-data.html#output-formats",
    "href": "how-to/generate-synthetic-data.html#output-formats",
    "title": "Generate Synthetic Cloud Data",
    "section": "",
    "text": "df.write_parquet(\"data.parquet\", compression=\"snappy\")\n\nBenefits: - Columnar format (fast analytics) - Excellent compression - Preserves data types\n\n\n\n\ndf.write_csv(\"data.csv\")\n\nWhen to use: - Need Excel compatibility - External tool integration - Human inspection\n\n\n\n\n# Keep in memory for immediate analysis\ndf.select(['timestamp', 'cpu_utilization', 'memory_utilization'])",
    "crumbs": [
      "Home",
      "How-To Guides",
      "Generate Synthetic Cloud Data"
    ]
  },
  {
    "objectID": "how-to/generate-synthetic-data.html#validation-checklist",
    "href": "how-to/generate-synthetic-data.html#validation-checklist",
    "title": "Generate Synthetic Cloud Data",
    "section": "",
    "text": "Before using generated data:\n\nTemporal range: Check start/end timestamps\nSample count: Verify expected number of rows\nAnomaly rate: Confirm anomaly percentage\nStatistics: Compare to research benchmarks (13% CPU, 20% memory)\nNo nulls: df.null_count().sum() should be 0\nTemporal order: Timestamps should be sorted\n\n\n# Quick validation\nprint(f\"Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\nprint(f\"Sample count: {len(df):,}\")\nprint(f\"Anomalies: {df['is_anomaly'].sum()} ({df['is_anomaly'].mean()*100:.2f}%)\")\nprint(f\"Mean CPU: {df['cpu_utilization'].mean():.1f}%\")\nprint(f\"Mean Memory: {df['memory_utilization'].mean():.1f}%\")\nprint(f\"Nulls: {df.null_count().sum()}\")",
    "crumbs": [
      "Home",
      "How-To Guides",
      "Generate Synthetic Cloud Data"
    ]
  },
  {
    "objectID": "how-to/generate-synthetic-data.html#troubleshooting",
    "href": "how-to/generate-synthetic-data.html#troubleshooting",
    "title": "Generate Synthetic Cloud Data",
    "section": "",
    "text": "Solution: Check workload type selection\n\n# Different types have different characteristics\n# Web apps: 15-25% CPU\n# ML training: 70-90% CPU\n# Dev environments: 5-10% CPU\n\n\n\n\nSolution: Adjust anomaly rate\n\n# Operational systems: 0.5-2%\ndf = generator.generate_time_series(..., anomaly_rate=0.01)\n\n# Research/testing: 5-10%\ndf = generator.generate_time_series(..., anomaly_rate=0.08)\n\n\n\n\nSolution: Use streaming generation\n\n# Generate in chunks\nfor month in range(1, 13):\n    start = datetime(2024, month, 1)\n    end = datetime(2024, month + 1, 1) if month &lt; 12 else datetime(2025, 1, 1)\n\n    df_chunk = generator.generate_time_series(\n        workload_type=WorkloadType.WEB_APP,\n        start_time=start,\n        end_time=end,\n        interval_minutes=5\n    )\n\n    df_chunk.write_parquet(f\"data_2024_{month:02d}.parquet\")",
    "crumbs": [
      "Home",
      "How-To Guides",
      "Generate Synthetic Cloud Data"
    ]
  },
  {
    "objectID": "how-to/generate-synthetic-data.html#see-also",
    "href": "how-to/generate-synthetic-data.html#see-also",
    "title": "Generate Synthetic Cloud Data",
    "section": "",
    "text": "Tutorial: Data Exploration - Learn the patterns\nTutorial: Workload Signatures - Understand archetypes\nAPI Reference - Complete API docs",
    "crumbs": [
      "Home",
      "How-To Guides",
      "Generate Synthetic Cloud Data"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cloud Resource Simulator",
    "section": "",
    "text": "A research-grounded framework for modeling cloud resource utilization patterns using Gaussian Processes, Bayesian hierarchical models, and foundation model integrations.\n\n\n\n\n\n\nNoteBuilt on Real Research\n\n\n\nOur models are based on extensive empirical analysis showing:\n\n13% average CPU utilization across cloud infrastructure\n30-32% waste in cloud spending\nStrong temporal patterns (0.7-0.8 autocorrelation)\n\nSee Research Foundation →\n\n\n\n\n\n\nGenerate cloud resource time series data with empirically-grounded patterns, not assumptions.\n\n\n\n\nGaussian Processes (GPyTorch): Time series forecasting with uncertainty\nBayesian Hierarchical Models (PyMC): Industry→Application→Resource hierarchy\nFoundation Models: Zero-shot forecasting (Chronos, TimesFM)\n\n\n\n\nPre-configured patterns for web apps, databases, ML training, microservices, and more.\n\n\n\n92% test coverage on GP library. Built for both research and production use.\n\n\n\n\n\n\n\n# Install with all features\nuv pip install cloud-resource-simulator[all]\n\n# Or install specific components\nuv pip install cloud-resource-simulator[research]  # Notebooks + analysis tools\nuv pip install cloud-resource-simulator[gpu]       # GP modeling with GPyTorch\n\n\n\n\nfrom cloud_sim.data_generation import WorkloadPatternGenerator, WorkloadType\nfrom datetime import datetime, timedelta\n\n# Initialize generator\ngenerator = WorkloadPatternGenerator(seed=42)\n\n# Generate 30 days of web application data\ndf = generator.generate_time_series(\n    workload_type=WorkloadType.WEB_APP,\n    start_time=datetime.now() - timedelta(days=30),\n    end_time=datetime.now(),\n    interval_minutes=60,\n    include_anomalies=True\n)\n\nprint(df.head())\n\nGenerated metrics include: - CPU & memory utilization - Network I/O (ingress/egress) - Disk IOPS - Efficiency scores & waste estimates - Anomaly flags\n\n\n\n\nfrom cloud_sim.ml_models.gaussian_process import (\n    SparseGPModel,\n    train_gp_model,\n    compute_metrics\n)\n\n# Initialize model with composite periodic kernel\nmodel = SparseGPModel(\n    num_inducing=100,\n    likelihood=\"student_t\"  # Robust to outliers\n)\n\n# Train on your data\ntrained_model = train_gp_model(\n    model=model,\n    train_x=train_data,\n    train_y=train_targets,\n    num_epochs=100\n)\n\n# Make predictions with uncertainty\npredictions, uncertainty = trained_model.predict(test_x)\n\n# Evaluate (accuracy + calibration)\nmetrics = compute_metrics(predictions, actual_values)\nprint(f\"RMSE: {metrics['rmse']:.3f}\")\nprint(f\"Calibration Error: {metrics['calibration_error']:.3f}\")\n\n\n\n\n\n\ngraph TB\n    A[Application Layer] --&gt; B[ML/Forecasting Layer]\n    B --&gt; C[Data Generation Layer]\n\n    A --&gt; A1[Streamlit Dashboard]\n    A --&gt; A2[Jupyter Notebooks]\n\n    B --&gt; B1[Gaussian Processes]\n    B --&gt; B2[Bayesian Hierarchical]\n    B --&gt; B3[Foundation Models]\n\n    C --&gt; C1[Workload Patterns]\n    C --&gt; C2[Cloud Metrics Simulator]\nThree-tier design:\n\nData Generation Layer: Realistic synthetic data with configurable workload patterns\nML/Forecasting Layer: Multiple modeling approaches for different use cases\nApplication Layer: Interactive dashboards and analysis notebooks\n\n\n\n\n\n\n\nEvery workload archetype is configured with real-world statistics:\n\n\n\nWorkload Type\nAvg CPU\nAvg Memory\nTemporal Pattern\n\n\n\n\nWeb App\n15-25%\n30-40%\nDaily + Weekly\n\n\nML Training\n70-90%\n60-80%\nBurst + Steady\n\n\nDatabase OLTP\n40-60%\n50-70%\nBusiness Hours\n\n\nDev Environment\n5-10%\n10-20%\nSporadic\n\n\n\nSee All Archetypes →\n\n\n\nMultivariate metrics with empirical correlations:\n\nCPU ↔︎ Memory: 0.2-0.95 (workload-dependent)\nNetwork ↔︎ CPU: 0.7-0.8 (web applications)\nTemporal autocorrelation: 0.7-0.8 (first 10 lags)\n\nRead Correlation Research →\n\n\n\n\nPolars DataFrames (primary format)\nHuggingFace Datasets (for ML training)\nParquet/CSV (for external tools)\n\n\n\n\n\n\n\n\n\n\nTutorials\nStep-by-step guides to learn core concepts:\n\nData Exploration\nWorkload Signatures\nGaussian Process Modeling\n\n\n\n\n\nHow-To Guides\nPractical recipes for specific tasks:\n\nGenerate Synthetic Data\nTrain GP Models\nExport to HuggingFace\n\n\n\n\n\n\n\n\nConcepts\nDeep dives into theory and design:\n\nResearch Foundation (35+ citations)\nCorrelation Analysis\nGP Architecture\n\n\n\n\n\nAPI Reference\nComplete API documentation:\n\nData Generation\nML Models\nETL & Loaders\n\n\n\n\n\n\n\n\nThis project is built on extensive empirical analysis of cloud resource patterns:\n\n\n\n\n\n\nTip📊 Key Research Findings\n\n\n\n\nCPU Utilization: 13% average (industry-wide) vs. 38% (datacenter-wide)\nResource Waste: 30-32% of cloud spending\nDevelopment Environments: 70% waste (often idle/forgotten)\nTemporal Autocorrelation: 0.7-0.8 for operational workloads\n\n35+ academic citations from Google, Microsoft Research, Alibaba, and more.\nExplore Research →\n\n\n\n\n\n\n\n\n\nTrain time series forecasting models\nTest anomaly detection algorithms\nBenchmark optimization strategies\n\n\n\n\n\nSimulate cost-saving scenarios\nModel right-sizing impacts\nForecast capacity needs\n\n\n\n\n\nGenerate realistic demo datasets\nCreate training materials\nBuild FinOps dashboards\n\n\n\n\n\nTeach cloud resource management\nDemonstrate waste patterns\nIllustrate optimization techniques\n\n\n\n\n\n\n\nGitHub: nehalecky/cloud-resource-simulator\nIssues: Report bugs or request features\nDiscussions: Ask questions, share ideas\n\n\n\n\n\nMIT License - see LICENSE for details.\nBuilt with ❤️ using Python, Polars, GPyTorch, PyMC, and Quarto.",
    "crumbs": [
      "Home",
      "Cloud Resource Simulator"
    ]
  },
  {
    "objectID": "index.html#why-cloud-resource-simulator",
    "href": "index.html#why-cloud-resource-simulator",
    "title": "Cloud Resource Simulator",
    "section": "",
    "text": "Generate cloud resource time series data with empirically-grounded patterns, not assumptions.\n\n\n\n\nGaussian Processes (GPyTorch): Time series forecasting with uncertainty\nBayesian Hierarchical Models (PyMC): Industry→Application→Resource hierarchy\nFoundation Models: Zero-shot forecasting (Chronos, TimesFM)\n\n\n\n\nPre-configured patterns for web apps, databases, ML training, microservices, and more.\n\n\n\n92% test coverage on GP library. Built for both research and production use.",
    "crumbs": [
      "Home",
      "Cloud Resource Simulator"
    ]
  },
  {
    "objectID": "index.html#quick-start",
    "href": "index.html#quick-start",
    "title": "Cloud Resource Simulator",
    "section": "",
    "text": "# Install with all features\nuv pip install cloud-resource-simulator[all]\n\n# Or install specific components\nuv pip install cloud-resource-simulator[research]  # Notebooks + analysis tools\nuv pip install cloud-resource-simulator[gpu]       # GP modeling with GPyTorch\n\n\n\n\nfrom cloud_sim.data_generation import WorkloadPatternGenerator, WorkloadType\nfrom datetime import datetime, timedelta\n\n# Initialize generator\ngenerator = WorkloadPatternGenerator(seed=42)\n\n# Generate 30 days of web application data\ndf = generator.generate_time_series(\n    workload_type=WorkloadType.WEB_APP,\n    start_time=datetime.now() - timedelta(days=30),\n    end_time=datetime.now(),\n    interval_minutes=60,\n    include_anomalies=True\n)\n\nprint(df.head())\n\nGenerated metrics include: - CPU & memory utilization - Network I/O (ingress/egress) - Disk IOPS - Efficiency scores & waste estimates - Anomaly flags\n\n\n\n\nfrom cloud_sim.ml_models.gaussian_process import (\n    SparseGPModel,\n    train_gp_model,\n    compute_metrics\n)\n\n# Initialize model with composite periodic kernel\nmodel = SparseGPModel(\n    num_inducing=100,\n    likelihood=\"student_t\"  # Robust to outliers\n)\n\n# Train on your data\ntrained_model = train_gp_model(\n    model=model,\n    train_x=train_data,\n    train_y=train_targets,\n    num_epochs=100\n)\n\n# Make predictions with uncertainty\npredictions, uncertainty = trained_model.predict(test_x)\n\n# Evaluate (accuracy + calibration)\nmetrics = compute_metrics(predictions, actual_values)\nprint(f\"RMSE: {metrics['rmse']:.3f}\")\nprint(f\"Calibration Error: {metrics['calibration_error']:.3f}\")",
    "crumbs": [
      "Home",
      "Cloud Resource Simulator"
    ]
  },
  {
    "objectID": "index.html#architecture-overview",
    "href": "index.html#architecture-overview",
    "title": "Cloud Resource Simulator",
    "section": "",
    "text": "graph TB\n    A[Application Layer] --&gt; B[ML/Forecasting Layer]\n    B --&gt; C[Data Generation Layer]\n\n    A --&gt; A1[Streamlit Dashboard]\n    A --&gt; A2[Jupyter Notebooks]\n\n    B --&gt; B1[Gaussian Processes]\n    B --&gt; B2[Bayesian Hierarchical]\n    B --&gt; B3[Foundation Models]\n\n    C --&gt; C1[Workload Patterns]\n    C --&gt; C2[Cloud Metrics Simulator]\nThree-tier design:\n\nData Generation Layer: Realistic synthetic data with configurable workload patterns\nML/Forecasting Layer: Multiple modeling approaches for different use cases\nApplication Layer: Interactive dashboards and analysis notebooks",
    "crumbs": [
      "Home",
      "Cloud Resource Simulator"
    ]
  },
  {
    "objectID": "index.html#key-features",
    "href": "index.html#key-features",
    "title": "Cloud Resource Simulator",
    "section": "",
    "text": "Every workload archetype is configured with real-world statistics:\n\n\n\nWorkload Type\nAvg CPU\nAvg Memory\nTemporal Pattern\n\n\n\n\nWeb App\n15-25%\n30-40%\nDaily + Weekly\n\n\nML Training\n70-90%\n60-80%\nBurst + Steady\n\n\nDatabase OLTP\n40-60%\n50-70%\nBusiness Hours\n\n\nDev Environment\n5-10%\n10-20%\nSporadic\n\n\n\nSee All Archetypes →\n\n\n\nMultivariate metrics with empirical correlations:\n\nCPU ↔︎ Memory: 0.2-0.95 (workload-dependent)\nNetwork ↔︎ CPU: 0.7-0.8 (web applications)\nTemporal autocorrelation: 0.7-0.8 (first 10 lags)\n\nRead Correlation Research →\n\n\n\n\nPolars DataFrames (primary format)\nHuggingFace Datasets (for ML training)\nParquet/CSV (for external tools)",
    "crumbs": [
      "Home",
      "Cloud Resource Simulator"
    ]
  },
  {
    "objectID": "index.html#documentation",
    "href": "index.html#documentation",
    "title": "Cloud Resource Simulator",
    "section": "",
    "text": "Tutorials\nStep-by-step guides to learn core concepts:\n\nData Exploration\nWorkload Signatures\nGaussian Process Modeling\n\n\n\n\n\nHow-To Guides\nPractical recipes for specific tasks:\n\nGenerate Synthetic Data\nTrain GP Models\nExport to HuggingFace\n\n\n\n\n\n\n\n\nConcepts\nDeep dives into theory and design:\n\nResearch Foundation (35+ citations)\nCorrelation Analysis\nGP Architecture\n\n\n\n\n\nAPI Reference\nComplete API documentation:\n\nData Generation\nML Models\nETL & Loaders",
    "crumbs": [
      "Home",
      "Cloud Resource Simulator"
    ]
  },
  {
    "objectID": "index.html#research-foundation",
    "href": "index.html#research-foundation",
    "title": "Cloud Resource Simulator",
    "section": "",
    "text": "This project is built on extensive empirical analysis of cloud resource patterns:\n\n\n\n\n\n\nTip📊 Key Research Findings\n\n\n\n\nCPU Utilization: 13% average (industry-wide) vs. 38% (datacenter-wide)\nResource Waste: 30-32% of cloud spending\nDevelopment Environments: 70% waste (often idle/forgotten)\nTemporal Autocorrelation: 0.7-0.8 for operational workloads\n\n35+ academic citations from Google, Microsoft Research, Alibaba, and more.\nExplore Research →",
    "crumbs": [
      "Home",
      "Cloud Resource Simulator"
    ]
  },
  {
    "objectID": "index.html#use-cases",
    "href": "index.html#use-cases",
    "title": "Cloud Resource Simulator",
    "section": "",
    "text": "Train time series forecasting models\nTest anomaly detection algorithms\nBenchmark optimization strategies\n\n\n\n\n\nSimulate cost-saving scenarios\nModel right-sizing impacts\nForecast capacity needs\n\n\n\n\n\nGenerate realistic demo datasets\nCreate training materials\nBuild FinOps dashboards\n\n\n\n\n\nTeach cloud resource management\nDemonstrate waste patterns\nIllustrate optimization techniques",
    "crumbs": [
      "Home",
      "Cloud Resource Simulator"
    ]
  },
  {
    "objectID": "index.html#community-support",
    "href": "index.html#community-support",
    "title": "Cloud Resource Simulator",
    "section": "",
    "text": "GitHub: nehalecky/cloud-resource-simulator\nIssues: Report bugs or request features\nDiscussions: Ask questions, share ideas",
    "crumbs": [
      "Home",
      "Cloud Resource Simulator"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Cloud Resource Simulator",
    "section": "",
    "text": "MIT License - see LICENSE for details.\nBuilt with ❤️ using Python, Polars, GPyTorch, PyMC, and Quarto.",
    "crumbs": [
      "Home",
      "Cloud Resource Simulator"
    ]
  },
  {
    "objectID": "how-to/index.html",
    "href": "how-to/index.html",
    "title": "How-To Guides",
    "section": "",
    "text": "Task-oriented guides for accomplishing specific goals with Cloud Resource Simulator.\n\n\n\n\nStep-by-step guide for creating realistic synthetic cloud resource datasets.\nUse cases: - Testing optimization algorithms - Training machine learning models - Benchmarking analysis pipelines - Creating demonstration datasets\n\n\n\n\nPractical guide for training and evaluating GP models on cloud resource data.\nUse cases: - Forecasting resource utilization - Anomaly detection - Capacity planning - Cost optimization\n\n\n\n\n\n\nExport to HuggingFace Datasets: Share your synthetic data\nConfigure Bayesian Hierarchical Models: Industry→Application→Resource modeling\nOptimize Resource Allocation: Cost reduction strategies\nDetect Anomalies: Statistical anomaly detection patterns\n\n\n\n\n\nCheck the Tutorials for learning-oriented guides\nRead the Concepts for deeper understanding\nBrowse the API Reference for detailed documentation\n\n\n\n\nHave a how-to guide to share? Contributions welcome! See our contributing guidelines.",
    "crumbs": [
      "Home",
      "How-To Guides",
      "How-To Guides"
    ]
  },
  {
    "objectID": "how-to/index.html#data-generation",
    "href": "how-to/index.html#data-generation",
    "title": "How-To Guides",
    "section": "",
    "text": "Step-by-step guide for creating realistic synthetic cloud resource datasets.\nUse cases: - Testing optimization algorithms - Training machine learning models - Benchmarking analysis pipelines - Creating demonstration datasets\n\n\n\n\nPractical guide for training and evaluating GP models on cloud resource data.\nUse cases: - Forecasting resource utilization - Anomaly detection - Capacity planning - Cost optimization",
    "crumbs": [
      "Home",
      "How-To Guides",
      "How-To Guides"
    ]
  },
  {
    "objectID": "how-to/index.html#coming-soon",
    "href": "how-to/index.html#coming-soon",
    "title": "How-To Guides",
    "section": "",
    "text": "Export to HuggingFace Datasets: Share your synthetic data\nConfigure Bayesian Hierarchical Models: Industry→Application→Resource modeling\nOptimize Resource Allocation: Cost reduction strategies\nDetect Anomalies: Statistical anomaly detection patterns",
    "crumbs": [
      "Home",
      "How-To Guides",
      "How-To Guides"
    ]
  },
  {
    "objectID": "how-to/index.html#need-help",
    "href": "how-to/index.html#need-help",
    "title": "How-To Guides",
    "section": "",
    "text": "Check the Tutorials for learning-oriented guides\nRead the Concepts for deeper understanding\nBrowse the API Reference for detailed documentation",
    "crumbs": [
      "Home",
      "How-To Guides",
      "How-To Guides"
    ]
  },
  {
    "objectID": "how-to/index.html#contributing",
    "href": "how-to/index.html#contributing",
    "title": "How-To Guides",
    "section": "",
    "text": "Have a how-to guide to share? Contributions welcome! See our contributing guidelines.",
    "crumbs": [
      "Home",
      "How-To Guides",
      "How-To Guides"
    ]
  },
  {
    "objectID": "how-to/train-gp-models.html",
    "href": "how-to/train-gp-models.html",
    "title": "Train Gaussian Process Models",
    "section": "",
    "text": "Practical recipes for training production-ready GP models for cloud resource forecasting.\n\n\n# Install with GPU support (recommended)\nuv pip install cloud-resource-simulator[gpu]\n\n# Or CPU-only\nuv pip install cloud-resource-simulator\n\n\n\n\n\n\n\nimport torch\nimport polars as pl\nfrom cloud_sim.ml_models.gaussian_process import (\n    SparseGPModel,\n    train_gp_model,\n    compute_metrics\n)\n\n# Load your data\ndf = pl.read_parquet(\"cloud_data.parquet\")\n\n# Prepare tensors\nX = torch.arange(len(df), dtype=torch.float32).reshape(-1, 1)\ny = torch.tensor(df['cpu_utilization'].to_numpy(), dtype=torch.float32)\n\n# Initialize and train\nmodel = SparseGPModel(num_inducing=100, likelihood_type='student_t')\ntrained_model, losses = train_gp_model(\n    model=model,\n    train_x=X,\n    train_y=y,\n    num_epochs=100,\n    learning_rate=0.01\n)\n\nprint(\"✓ Training complete\")\n\n\n\n\n\n\n\n\nRule of thumb: num_inducing = sqrt(n) / 10\n\nimport numpy as np\n\nn_samples = len(df)\nnum_inducing = int(np.sqrt(n_samples) / 10)\nnum_inducing = max(50, min(500, num_inducing))  # Clamp to [50, 500]\n\nprint(f\"Dataset size: {n_samples:,}\")\nprint(f\"Recommended inducing points: {num_inducing}\")\n\nGuidelines: - Small datasets (&lt;10k samples): 50-100 points - Medium datasets (10k-100k): 100-300 points - Large datasets (&gt;100k): 300-500 points\n\n\n\n\nFor time series with known periodicity:\n\nfrom cloud_sim.ml_models.gaussian_process import CompositePeriodicKernel\n\n# Daily + weekly patterns (minute-level data)\nkernel = CompositePeriodicKernel(\n    periods=[1440.0, 10080.0],  # 1 day, 1 week in minutes\n    lengthscales=[360.0, 2520.0],  # Quarter-day, half-week\n    add_noise=True\n)\n\nmodel = SparseGPModel(\n    num_inducing=200,\n    likelihood_type='student_t',\n    kernel=kernel\n)\n\nCommon periods (minute-level data): - Hourly: 60 - Daily: 1440 - Weekly: 10080 - Monthly: 43200\n\n\n\n\nUse mini-batch training:\n\n# Train on batches for memory efficiency\nbatch_size = 10000\n\nfor epoch in range(100):\n    # Shuffle data\n    indices = torch.randperm(len(X))\n\n    for i in range(0, len(X), batch_size):\n        batch_indices = indices[i:i+batch_size]\n        X_batch = X[batch_indices]\n        y_batch = y[batch_indices]\n\n        # Training step\n        model.train()\n        optimizer.zero_grad()\n        output = model(X_batch)\n        loss = -model.likelihood(output, y_batch).log_prob(y_batch).mean()\n        loss.backward()\n        optimizer.step()\n\n\n\n\n\n\nimport altair as alt\n\n# Plot training loss\nloss_df = pl.DataFrame({'epoch': range(len(losses)), 'loss': losses})\n\nchart = alt.Chart(loss_df.to_pandas()).mark_line().encode(\n    x='epoch:Q',\n    y=alt.Y('loss:Q', scale=alt.Scale(type='log'))\n).properties(\n    width=600,\n    height=300,\n    title='Training Loss (Log Scale)'\n)\n\nchart\n\nExpected behavior: - Rapid decrease in first 10-20 epochs - Convergence around epoch 50-80 - Plateaus or slight oscillation at convergence\nWarning signs: - Loss increasing → learning rate too high - Flat from start → learning rate too low or bad initialization - Oscillating wildly → unstable training\n\n\n\n\n\n# Grid search over key hyperparameters\nconfigs = [\n    {'num_inducing': 100, 'lr': 0.01, 'likelihood': 'student_t'},\n    {'num_inducing': 200, 'lr': 0.01, 'likelihood': 'student_t'},\n    {'num_inducing': 100, 'lr': 0.005, 'likelihood': 'gaussian'},\n    {'num_inducing': 200, 'lr': 0.005, 'likelihood': 'gaussian'},\n]\n\nbest_model = None\nbest_rmse = float('inf')\n\nfor config in configs:\n    model = SparseGPModel(\n        num_inducing=config['num_inducing'],\n        likelihood_type=config['likelihood']\n    )\n\n    trained, _ = train_gp_model(\n        model, X_train, y_train,\n        num_epochs=50,\n        learning_rate=config['lr']\n    )\n\n    # Evaluate on validation set\n    with torch.no_grad():\n        pred = trained(X_val).mean\n        rmse = torch.sqrt(((pred - y_val)**2).mean()).item()\n\n    if rmse &lt; best_rmse:\n        best_rmse = rmse\n        best_model = trained\n        best_config = config\n\n    print(f\"Config: {config} → RMSE: {rmse:.3f}\")\n\nprint(f\"\\nBest config: {best_config} with RMSE: {best_rmse:.3f}\")\n\n\n\n\n\n\n\n\n\nfrom cloud_sim.ml_models.gaussian_process import save_model, load_model\n\n# Save trained model\nsave_model(trained_model, \"models/gp_cpu_forecaster_v1.pt\")\n\n# Load for inference\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nloaded_model = load_model(\"models/gp_cpu_forecaster_v1.pt\", device=device)\n\n\n\n\n\nimport gpytorch\n\n# Efficient batch prediction\nbatch_size = 5000\npredictions = []\nuncertainties = []\n\nmodel.eval()\nfor i in range(0, len(X_test), batch_size):\n    X_batch = X_test[i:i+batch_size]\n\n    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n        pred_dist = model(X_batch)\n        predictions.append(pred_dist.mean.numpy())\n        uncertainties.append(pred_dist.stddev.numpy())\n\npred_mean = np.concatenate(predictions)\npred_std = np.concatenate(uncertainties)\n\n\n\n\n\n# Retrain on sliding window\ndef update_model(model, new_data, window_size=10000):\n    \"\"\"Incremental update with recent data\"\"\"\n    recent = new_data[-window_size:]\n\n    X = torch.arange(len(recent), dtype=torch.float32).reshape(-1, 1)\n    y = torch.tensor(recent['value'].to_numpy(), dtype=torch.float32)\n\n    updated, _ = train_gp_model(\n        model, X, y,\n        num_epochs=20,  # Fewer epochs for updates\n        learning_rate=0.005  # Lower LR for fine-tuning\n    )\n\n    return updated\n\n# Update hourly\n# model = update_model(model, latest_data)\n\n\n\n\n\n\n\n\n\n# Evaluate accuracy AND calibration\nmetrics = compute_metrics(\n    y_true=y_test.numpy(),\n    y_pred=pred_mean,\n    y_std=pred_std\n)\n\nprint(\"Accuracy:\")\nprint(f\"  RMSE: {metrics['rmse']:.3f}\")\nprint(f\"  MAE: {metrics['mae']:.3f}\")\nprint(f\"  R²: {metrics['r2']:.3f}\")\n\nprint(\"\\nCalibration:\")\nprint(f\"  Coverage@95: {metrics['coverage_95']:.1f}%\")  # Should be ~95%\nprint(f\"  Calibration Error: {metrics['calibration_error']:.3f}\")  # Should be ~0\n\n\n\n\n\n# Time series cross-validation\nfrom sklearn.model_selection import TimeSeriesSplit\n\ntscv = TimeSeriesSplit(n_splits=5)\nscores = []\n\nfor train_idx, test_idx in tscv.split(X):\n    X_train_cv, X_test_cv = X[train_idx], X[test_idx]\n    y_train_cv, y_test_cv = y[train_idx], y[test_idx]\n\n    model_cv = SparseGPModel(num_inducing=100)\n    trained_cv, _ = train_gp_model(model_cv, X_train_cv, y_train_cv, num_epochs=50)\n\n    with torch.no_grad():\n        pred = trained_cv(X_test_cv).mean\n        rmse = torch.sqrt(((pred - y_test_cv)**2).mean()).item()\n\n    scores.append(rmse)\n\nprint(f\"CV RMSE: {np.mean(scores):.3f} ± {np.std(scores):.3f}\")\n\n\n\n\n\n\n\n\nSolutions: 1. Reduce num_inducing (100 vs. 500) 2. Use GPU if available 3. Reduce num_epochs (50 vs. 100) 4. Use mini-batch training\n\n\n\nSolutions: 1. Check data quality (nulls, outliers) 2. Try Student-t likelihood (robust to outliers) 3. Increase num_inducing points 4. Tune kernel hyperparameters 5. Add more training epochs\n\n\n\nSolutions: 1. Switch from Gaussian to Student-t likelihood 2. Add noise kernel component 3. Check for data leakage (train/test split)\n\n\n\nSolutions: 1. Reduce noise in data 2. Train longer (more epochs) 3. Check kernel configuration\n\n\n\n\n\n\nTutorial: Gaussian Processes - Learn the fundamentals\nConcepts: GP Design - Architecture details\nAPI Reference - Complete API documentation",
    "crumbs": [
      "Home",
      "How-To Guides",
      "Train Gaussian Process Models"
    ]
  },
  {
    "objectID": "how-to/train-gp-models.html#prerequisites",
    "href": "how-to/train-gp-models.html#prerequisites",
    "title": "Train Gaussian Process Models",
    "section": "",
    "text": "# Install with GPU support (recommended)\nuv pip install cloud-resource-simulator[gpu]\n\n# Or CPU-only\nuv pip install cloud-resource-simulator",
    "crumbs": [
      "Home",
      "How-To Guides",
      "Train Gaussian Process Models"
    ]
  },
  {
    "objectID": "how-to/train-gp-models.html#quick-start",
    "href": "how-to/train-gp-models.html#quick-start",
    "title": "Train Gaussian Process Models",
    "section": "",
    "text": "import torch\nimport polars as pl\nfrom cloud_sim.ml_models.gaussian_process import (\n    SparseGPModel,\n    train_gp_model,\n    compute_metrics\n)\n\n# Load your data\ndf = pl.read_parquet(\"cloud_data.parquet\")\n\n# Prepare tensors\nX = torch.arange(len(df), dtype=torch.float32).reshape(-1, 1)\ny = torch.tensor(df['cpu_utilization'].to_numpy(), dtype=torch.float32)\n\n# Initialize and train\nmodel = SparseGPModel(num_inducing=100, likelihood_type='student_t')\ntrained_model, losses = train_gp_model(\n    model=model,\n    train_x=X,\n    train_y=y,\n    num_epochs=100,\n    learning_rate=0.01\n)\n\nprint(\"✓ Training complete\")",
    "crumbs": [
      "Home",
      "How-To Guides",
      "Train Gaussian Process Models"
    ]
  },
  {
    "objectID": "how-to/train-gp-models.html#common-tasks",
    "href": "how-to/train-gp-models.html#common-tasks",
    "title": "Train Gaussian Process Models",
    "section": "",
    "text": "Rule of thumb: num_inducing = sqrt(n) / 10\n\nimport numpy as np\n\nn_samples = len(df)\nnum_inducing = int(np.sqrt(n_samples) / 10)\nnum_inducing = max(50, min(500, num_inducing))  # Clamp to [50, 500]\n\nprint(f\"Dataset size: {n_samples:,}\")\nprint(f\"Recommended inducing points: {num_inducing}\")\n\nGuidelines: - Small datasets (&lt;10k samples): 50-100 points - Medium datasets (10k-100k): 100-300 points - Large datasets (&gt;100k): 300-500 points\n\n\n\n\nFor time series with known periodicity:\n\nfrom cloud_sim.ml_models.gaussian_process import CompositePeriodicKernel\n\n# Daily + weekly patterns (minute-level data)\nkernel = CompositePeriodicKernel(\n    periods=[1440.0, 10080.0],  # 1 day, 1 week in minutes\n    lengthscales=[360.0, 2520.0],  # Quarter-day, half-week\n    add_noise=True\n)\n\nmodel = SparseGPModel(\n    num_inducing=200,\n    likelihood_type='student_t',\n    kernel=kernel\n)\n\nCommon periods (minute-level data): - Hourly: 60 - Daily: 1440 - Weekly: 10080 - Monthly: 43200\n\n\n\n\nUse mini-batch training:\n\n# Train on batches for memory efficiency\nbatch_size = 10000\n\nfor epoch in range(100):\n    # Shuffle data\n    indices = torch.randperm(len(X))\n\n    for i in range(0, len(X), batch_size):\n        batch_indices = indices[i:i+batch_size]\n        X_batch = X[batch_indices]\n        y_batch = y[batch_indices]\n\n        # Training step\n        model.train()\n        optimizer.zero_grad()\n        output = model(X_batch)\n        loss = -model.likelihood(output, y_batch).log_prob(y_batch).mean()\n        loss.backward()\n        optimizer.step()\n\n\n\n\n\n\nimport altair as alt\n\n# Plot training loss\nloss_df = pl.DataFrame({'epoch': range(len(losses)), 'loss': losses})\n\nchart = alt.Chart(loss_df.to_pandas()).mark_line().encode(\n    x='epoch:Q',\n    y=alt.Y('loss:Q', scale=alt.Scale(type='log'))\n).properties(\n    width=600,\n    height=300,\n    title='Training Loss (Log Scale)'\n)\n\nchart\n\nExpected behavior: - Rapid decrease in first 10-20 epochs - Convergence around epoch 50-80 - Plateaus or slight oscillation at convergence\nWarning signs: - Loss increasing → learning rate too high - Flat from start → learning rate too low or bad initialization - Oscillating wildly → unstable training\n\n\n\n\n\n# Grid search over key hyperparameters\nconfigs = [\n    {'num_inducing': 100, 'lr': 0.01, 'likelihood': 'student_t'},\n    {'num_inducing': 200, 'lr': 0.01, 'likelihood': 'student_t'},\n    {'num_inducing': 100, 'lr': 0.005, 'likelihood': 'gaussian'},\n    {'num_inducing': 200, 'lr': 0.005, 'likelihood': 'gaussian'},\n]\n\nbest_model = None\nbest_rmse = float('inf')\n\nfor config in configs:\n    model = SparseGPModel(\n        num_inducing=config['num_inducing'],\n        likelihood_type=config['likelihood']\n    )\n\n    trained, _ = train_gp_model(\n        model, X_train, y_train,\n        num_epochs=50,\n        learning_rate=config['lr']\n    )\n\n    # Evaluate on validation set\n    with torch.no_grad():\n        pred = trained(X_val).mean\n        rmse = torch.sqrt(((pred - y_val)**2).mean()).item()\n\n    if rmse &lt; best_rmse:\n        best_rmse = rmse\n        best_model = trained\n        best_config = config\n\n    print(f\"Config: {config} → RMSE: {rmse:.3f}\")\n\nprint(f\"\\nBest config: {best_config} with RMSE: {best_rmse:.3f}\")",
    "crumbs": [
      "Home",
      "How-To Guides",
      "Train Gaussian Process Models"
    ]
  },
  {
    "objectID": "how-to/train-gp-models.html#production-patterns",
    "href": "how-to/train-gp-models.html#production-patterns",
    "title": "Train Gaussian Process Models",
    "section": "",
    "text": "from cloud_sim.ml_models.gaussian_process import save_model, load_model\n\n# Save trained model\nsave_model(trained_model, \"models/gp_cpu_forecaster_v1.pt\")\n\n# Load for inference\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nloaded_model = load_model(\"models/gp_cpu_forecaster_v1.pt\", device=device)\n\n\n\n\n\nimport gpytorch\n\n# Efficient batch prediction\nbatch_size = 5000\npredictions = []\nuncertainties = []\n\nmodel.eval()\nfor i in range(0, len(X_test), batch_size):\n    X_batch = X_test[i:i+batch_size]\n\n    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n        pred_dist = model(X_batch)\n        predictions.append(pred_dist.mean.numpy())\n        uncertainties.append(pred_dist.stddev.numpy())\n\npred_mean = np.concatenate(predictions)\npred_std = np.concatenate(uncertainties)\n\n\n\n\n\n# Retrain on sliding window\ndef update_model(model, new_data, window_size=10000):\n    \"\"\"Incremental update with recent data\"\"\"\n    recent = new_data[-window_size:]\n\n    X = torch.arange(len(recent), dtype=torch.float32).reshape(-1, 1)\n    y = torch.tensor(recent['value'].to_numpy(), dtype=torch.float32)\n\n    updated, _ = train_gp_model(\n        model, X, y,\n        num_epochs=20,  # Fewer epochs for updates\n        learning_rate=0.005  # Lower LR for fine-tuning\n    )\n\n    return updated\n\n# Update hourly\n# model = update_model(model, latest_data)",
    "crumbs": [
      "Home",
      "How-To Guides",
      "Train Gaussian Process Models"
    ]
  },
  {
    "objectID": "how-to/train-gp-models.html#evaluation-best-practices",
    "href": "how-to/train-gp-models.html#evaluation-best-practices",
    "title": "Train Gaussian Process Models",
    "section": "",
    "text": "# Evaluate accuracy AND calibration\nmetrics = compute_metrics(\n    y_true=y_test.numpy(),\n    y_pred=pred_mean,\n    y_std=pred_std\n)\n\nprint(\"Accuracy:\")\nprint(f\"  RMSE: {metrics['rmse']:.3f}\")\nprint(f\"  MAE: {metrics['mae']:.3f}\")\nprint(f\"  R²: {metrics['r2']:.3f}\")\n\nprint(\"\\nCalibration:\")\nprint(f\"  Coverage@95: {metrics['coverage_95']:.1f}%\")  # Should be ~95%\nprint(f\"  Calibration Error: {metrics['calibration_error']:.3f}\")  # Should be ~0\n\n\n\n\n\n# Time series cross-validation\nfrom sklearn.model_selection import TimeSeriesSplit\n\ntscv = TimeSeriesSplit(n_splits=5)\nscores = []\n\nfor train_idx, test_idx in tscv.split(X):\n    X_train_cv, X_test_cv = X[train_idx], X[test_idx]\n    y_train_cv, y_test_cv = y[train_idx], y[test_idx]\n\n    model_cv = SparseGPModel(num_inducing=100)\n    trained_cv, _ = train_gp_model(model_cv, X_train_cv, y_train_cv, num_epochs=50)\n\n    with torch.no_grad():\n        pred = trained_cv(X_test_cv).mean\n        rmse = torch.sqrt(((pred - y_test_cv)**2).mean()).item()\n\n    scores.append(rmse)\n\nprint(f\"CV RMSE: {np.mean(scores):.3f} ± {np.std(scores):.3f}\")",
    "crumbs": [
      "Home",
      "How-To Guides",
      "Train Gaussian Process Models"
    ]
  },
  {
    "objectID": "how-to/train-gp-models.html#troubleshooting",
    "href": "how-to/train-gp-models.html#troubleshooting",
    "title": "Train Gaussian Process Models",
    "section": "",
    "text": "Solutions: 1. Reduce num_inducing (100 vs. 500) 2. Use GPU if available 3. Reduce num_epochs (50 vs. 100) 4. Use mini-batch training\n\n\n\nSolutions: 1. Check data quality (nulls, outliers) 2. Try Student-t likelihood (robust to outliers) 3. Increase num_inducing points 4. Tune kernel hyperparameters 5. Add more training epochs\n\n\n\nSolutions: 1. Switch from Gaussian to Student-t likelihood 2. Add noise kernel component 3. Check for data leakage (train/test split)\n\n\n\nSolutions: 1. Reduce noise in data 2. Train longer (more epochs) 3. Check kernel configuration",
    "crumbs": [
      "Home",
      "How-To Guides",
      "Train Gaussian Process Models"
    ]
  },
  {
    "objectID": "how-to/train-gp-models.html#see-also",
    "href": "how-to/train-gp-models.html#see-also",
    "title": "Train Gaussian Process Models",
    "section": "",
    "text": "Tutorial: Gaussian Processes - Learn the fundamentals\nConcepts: GP Design - Architecture details\nAPI Reference - Complete API documentation",
    "crumbs": [
      "Home",
      "How-To Guides",
      "Train Gaussian Process Models"
    ]
  },
  {
    "objectID": "concepts/research/cloud-resource-correlations-report.html",
    "href": "concepts/research/cloud-resource-correlations-report.html",
    "title": "Cloud Resource Metrics Correlation Patterns: Empirical Research Report",
    "section": "",
    "text": "This report synthesizes empirical research on correlation patterns between cloud resource metrics (CPU, memory, network, disk I/O) across different application types. Research shows strong temporal correlations and self-similarity in resource usage patterns [1], with memory emerging as a critical bottleneck in co-located clusters, reducing throughput by up to 46% [2]. Machine learning workloads demonstrate unique GPU-CPU-memory interdependencies with 6.5-10x performance differences [3], while microservices exhibit cross-VM correlations with up to 79% performance overhead compared to monolithic architectures [4].\n\n\n\n\n\nResearch on cloud workload patterns reveals strong temporal correlations in resource usage patterns [1]. Studies of memory access patterns in SPEC CPU2017 benchmarks show that ~80% of workloads exhibit correlation in their access intervals, with all correlated workloads demonstrating Hurst parameters &gt; 0.5, confirming self-similarity and long-range dependence [1]. This indicates that resource usage is predictable in the short-term (up to a few hours).\n\n\n\nIn SPEC CPU2017 workloads: - ~80% of applications show correlation in memory access patterns (vs. &lt;30% in SPEC CPU2006) [5] - All correlated workloads demonstrate Hurst parameters &gt; 0.5, confirming self-similarity [5] - Memory access intervals at small time scales (milliseconds) follow exponential distribution - Aggregated processes at large scales (minutes) show self-similarity - Some benchmarks use up to 16GB main memory and 2.3GB/s memory bandwidth [5]\n\n\n\nMicrosoft’s Resource Central study on Azure workloads reveals strong positive correlations between utilization metrics [6]: - CPU utilization correlates with memory usage - Disk I/O operations correlate with CPU cycles - Network latency impacts CPU wait times - Higher utilization VMs tend to be smaller and live longer - Negative correlation exists between VM size and utilization\nIncluding these correlated features improves predictive performance significantly compared to CPU-only models.\n\n\n\n\n\n\nWeb applications demonstrate three distinct daily and three weekly workload patterns based on K-Means clustering analysis of 3,191 daily and 466 weekly data points [7]: - Time-series analysis captures temporal dependencies effectively - Recurring patterns link to service quality metrics - Service Workload Patterns (SWPs) remain relatively stable during normal operations [8] - Fixed mapping exists between infrastructure input and QoS during stable periods [8]\n\n\n\nDatabase systems show specific correlation patterns: - Peak operations significantly exceed baseline loads (specific ratios vary by workload type) [9] - Strong correlation between unsuccessful jobs and requested resources (CPU, memory, disk) [9] - Terminated tasks utilize significant cloud resources before being killed, wasting compute cycles [9] - Enhanced monitoring available at 1, 5, 10, 15, 30, or 60-second intervals for Aurora/RDS [9]\n\n\n\nML workloads demonstrate unique resource patterns [3]:\nTraining Phase: - GPU performance shows 6.5x speedup (2 hours GPU vs 13 hours CPU for 20 epochs) in comparative studies [10] - GPU compute improved 32x in 9 years vs 13x for memory bandwidth, creating bottleneck [11] - ResNet-50 requires 14 days on single M40 GPU for 100 epochs [12] - NeuSight framework reduces prediction error from 121.4% to 2.3% for GPT-3 latency [12]\nInference Phase: - Memory-efficient deep learning inference techniques enable incremental weight loading [13] - KV caches statically over-provisioned for max sequence length (e.g., 2048) [13] - Lower resource requirements but latency-sensitive - CPUs viable for lightweight model inference with optimization\n\n\n\nMicroservices exhibit cross-VM workload correlations with significant performance implications [14]: - CrossTrace achieves &gt;90% accuracy correlating thousands of spans within seconds using eBPF [14] - Microservice performance can be 79.1% slower than monolithic on same hardware [14] - 4.22x more time in runtime libraries (Node.js), 2.69x (Java EE) [14] - Container-based microservices can reduce infrastructure costs by 70% despite overhead [15]\nKey metrics for microservice benchmarking [15]: - Latency (primary concern) - Throughput - Scalability patterns - CPU usage per service - Memory usage patterns - Network usage between services\n\n\n\n\n\n\nResearch identifies important time-lagged relationships [16]: - CPU allocation spikes → Memory pressure (delayed response) - CPU bottlenecks cause queuing, leading to subsequent memory issues - Network congestion correlates with later CPU spikes - Performance interference from memory thrashing can reduce throughput by 46% even without over-commitment [16]\n\n\n\nGoogle Cloud documentation confirms monitoring delays [17]: - Metric collection latency: 2-4 minutes for Pub/Sub metrics - Metrics sampled every 60 seconds may take up to 240 seconds to become visible - This affects autoscaling responsiveness and anomaly detection - High-frequency monitoring (1-minute windows) recommended for 99th percentile tracking\n\n\n\nLSTM and RNN models effectively capture temporal dependencies [18]: - Long Short Term Memory RNN achieved MSE of 3.17×10⁻³ on web server log datasets [18] - Attention-based LSTM encoder-decoder networks map historical sequences to predictions [18] - esDNN addresses LSTM gradient issues using GRU-based algorithms for multivariate series [18] - Models retain contextual information across time steps for evolving workload trends\n\n\n\n\n\n\nDuring normal operations: - Service Workload Patterns (SWPs) remain relatively stable [8] - Fixed mapping exists between infrastructure input and Quality of Service metrics - Predictable resource consumption patterns enable proactive management - Small variations in consecutive time steps allow simple prediction methods\n\n\n\nUnder peak load: - Memory becomes primary bottleneck in co-located clusters, causing up to 46% throughput reduction [2] - Unmovable allocations scattered across address space cause fragmentation (Meta datacenters) [2] - CPU and disk I/O show daily cyclical correlation patterns - Memory usage remains approximately constant while other resources spike\n\n\n\nDuring failures [9]: - Significant correlation between unsuccessful tasks and requested resources (CPU, memory, disk) - Failed jobs consumed many resources before being killed, heavily wasting CPU and RAM - All tasks with scheduling class 3 failed in Google cluster traces - Direct relationship exists between scheduling class, priority, and failure rates\n\n\n\n\n\n\nBased on Alibaba cluster traces (4,000 machines, 8 days, 71K online services) [19]: - CPU and disk I/O show daily cyclical correlation patterns - Memory usage exhibits weak correlation with CPU cycles in co-located workloads - Network throughput correlates with CPU during batch processing phases - Sigma scheduler manages online services, Fuxi manages batch workloads\n\n\n\nEstablished correlations from production systems [8]: - Optimal CPU utilization varies by workload (20-50% for latency-sensitive) - Memory utilization &gt; 80% → Significant performance degradation begins - Network latency increases → CPU wait time increases proportionally - Strong positive correlation between all utilization metrics (Microsoft Azure study)\n\n\n\n\n\n\nMultiple versions available on GitHub [19]: - cluster-trace-v2017: 1,300 machines, 12 hours, online+batch workloads - cluster-trace-v2018: 4,000 machines, 8 days, 71K online services, 4M batch jobs - AMTrace: Fine-granularity microarchitectural metrics - Size: 270+ GB uncompressed (50 GB compressed) - Contains: DAG dependency information for offline tasks - URL: https://github.com/alibaba/clusterdata\n\n\n\n2019 dataset contains [20]: - 2.4 TiB compressed workload traces from 8 Borg cells - Available via BigQuery for analysis - CPU usage histograms per 5-minute period - Alloc sets information and job-parent relationships for MapReduce - Detailed resource usage and job failure patterns - URL: https://github.com/google/cluster-data\n\n\n\n\n\n\n\nStrong temporal correlations with self-similarity confirmed by Hurst parameters &gt; 0.5 [1]\n~80% of SPEC CPU2017 workloads show memory access correlation\nResource usage predictable up to several hours using LSTM/RNN models\nCritical for proactive resource management and autoscaling\n\n\n\n\n\nMemory thrashing can reduce throughput by 46% even without over-commitment [2]\nFragmentation from unmovable allocations is primary cause in production datacenters\nUnlike CPU/disk, memory usage remains constant during load spikes\nMemory-aware scheduling and contiguity management crucial for performance\n\n\n\n\n\nWeb applications show 3 daily and 3 weekly distinct patterns from clustering analysis [7]\nML workloads show 6.5-10x GPU performance advantage, require GPU-CPU-memory balance [3]\nMicroservices exhibit 79% performance overhead but 70% infrastructure cost reduction [14]\nDatabase workloads need monitoring at sub-minute intervals for accurate correlation\n\n\n\n\n\nSub-minute monitoring (1-60 second intervals) required to capture spikes [17]\nGoogle Cloud metrics have 2-4 minute collection latency affecting real-time decisions\nMulti-metric correlation essential for root cause analysis and anomaly detection [16]\nTime-lagged effects and cascade failures must be considered in autoscaling policies [18]\n\n\n\n\n\n[1] Zou, Y., et al. (2022). “Temporal Characterization of Memory Access Behaviors in SPEC CPU2017.” Future Generation Computer Systems, Volume 129, pp. 206-217. https://www.sciencedirect.com/science/article/abs/pii/S0167739X21004908 ~80% of SPEC CPU2017 workloads show correlation in memory access intervals with Hurst parameters &gt;0.5.\n[2] “Performance Interference of Memory Thrashing in Virtualized Cloud Environments.” (2016). IEEE International Conference on Cloud Computing. https://ieeexplore.ieee.org/document/7820282/ Memory thrashing can reduce system throughput by 46% even without memory over-commitment.\n[3] “Comparative Analysis of CPU and GPU Profiling for Deep Learning Models.” (2023). ArXiv Preprint. https://arxiv.org/pdf/2309.02521 Training time comparison: CPU ~13 hours vs GPU ~2 hours for 20 epochs (6.5x speedup).\n[4] IBM Research. (2016). “Workload Characterization for Microservices.” IEEE International Symposium on Workload Characterization. https://ieeexplore.ieee.org/document/7581269/ Microservice performance 79.1% slower than monolithic on same hardware, 4.22x overhead in runtime.\n[5] Singh, S., and Awasthi, M. (2019). “Memory Centric Characterization and Analysis of SPEC CPU2017 Suite.” ICPE 2019. https://arxiv.org/abs/1910.00651 ~50% of dynamic instructions are memory intensive; benchmarks use up to 16GB RAM and 2.3GB/s bandwidth.\n[6] Microsoft Research. (2017). “Resource Central: Understanding and Predicting Workloads for Improved Resource Management.” SOSP 2017. https://www.microsoft.com/en-us/research/wp-content/uploads/2017/10/Resource-Central-SOSP17.pdf Strong positive correlation between utilization metrics in Azure workloads.\n[7] “Understanding Web Application Workloads: Systematic Literature Review.” (2024). ArXiv & IEEE. https://arxiv.org/abs/2409.12299 Identifies 3 daily and 3 weekly patterns using K-Means clustering on 3,191 daily and 466 weekly data points.\n[8] “Service Workload Patterns for QoS-Driven Cloud Resource Management.” (2018). Journal of Cloud Computing: Advances, Systems and Applications. https://journalofcloudcomputing.springeropen.com/articles/10.1186/s13677-018-0106-7 Service Workload Patterns remain stable during normal operations with fixed infrastructure-QoS mapping.\n[9] “Analysis of Job Failure and Prediction Model for Cloud Computing Using Machine Learning.” (2022). Sensors, 22(5), 2035. https://www.mdpi.com/1424-8220/22/5/2035 Significant correlation between unsuccessful tasks and requested resources; failed jobs waste CPU and RAM.\n[10] “Comparative Analysis of CPU and GPU Profiling for Deep Learning Models.” (2023). ArXiv Preprint. https://arxiv.org/pdf/2309.02521 Documented 6.5x speedup for GPU training vs CPU across multiple deep learning models.\n[11] Lee, S., et al. (2024). “Forecasting GPU Performance for Deep Learning Training and Inference.” ASPLOS 2025. https://dl.acm.org/doi/10.1145/3669940.3707265 NeuSight framework; GPU compute increased 32x in 9 years vs 13x for memory bandwidth.\n[12] Lee, S., et al. (2024). “Forecasting GPU Performance for Deep Learning Training and Inference.” ArXiv. https://arxiv.org/abs/2407.13853 NeuSight reduces GPT-3 latency prediction error from 121.4% to 2.3%.\n[13] “Memory-efficient Deep Learning Inference in Trusted Execution Environments.” (2021). Journal of Systems Architecture. https://www.sciencedirect.com/science/article/abs/pii/S1383762121001314 MDI approach with incremental weight loading and data layout reorganization for inference.\n[14] “CrossTrace: Efficient Cross-Thread and Cross-Service Span Correlation.” (2025). ArXiv. https://arxiv.org/html/2508.11342 eBPF-based tracing achieves &gt;90% accuracy correlating spans; includes IBM microservices overhead study.\n[15] “Microservice Performance Degradation Correlation.” (2020). ResearchGate. https://www.researchgate.net/publication/346782444_Microservice_Performance_Degradation_Correlation Container-based microservices can reduce infrastructure costs by 70% despite performance overhead.\n[16] “Contiguitas: The Pursuit of Physical Memory Contiguity in Datacenters.” (2023). 50th Annual International Symposium on Computer Architecture. https://dl.acm.org/doi/10.1145/3579371.3589079 Memory fragmentation from unmovable allocations causes performance degradation in production.\n[17] Google Cloud. (2024). “Retention and Latency of Metric Data.” Cloud Monitoring Documentation. https://cloud.google.com/monitoring/api/v3/latency-n-retention Pub/Sub metrics have 2-4 minute latencies; sampled every 60 seconds, visible after 240 seconds.\n[18] Kumar, J., et al. (2018). “Long Short Term Memory RNN Based Workload Forecasting for Cloud Datacenters.” Procedia Computer Science, Volume 125, pp. 676-682. https://www.sciencedirect.com/science/article/pii/S1877050917328557 LSTM-RNN achieves MSE of 3.17×10⁻³ on web server log datasets.\n[19] Alibaba Cloud. (2018). “Alibaba Cluster Trace v2018.” GitHub Repository. https://github.com/alibaba/clusterdata 4,000 machines, 8 days, 71K online services, 4M batch jobs, 270+ GB uncompressed data.\n[20] Google Research. (2019). “Google Cluster Workload Traces 2019.” Google Research Datasets. https://github.com/google/cluster-data 2.4 TiB compressed traces from 8 Borg cells, available via BigQuery."
  },
  {
    "objectID": "concepts/research/cloud-resource-correlations-report.html#executive-summary",
    "href": "concepts/research/cloud-resource-correlations-report.html#executive-summary",
    "title": "Cloud Resource Metrics Correlation Patterns: Empirical Research Report",
    "section": "",
    "text": "This report synthesizes empirical research on correlation patterns between cloud resource metrics (CPU, memory, network, disk I/O) across different application types. Research shows strong temporal correlations and self-similarity in resource usage patterns [1], with memory emerging as a critical bottleneck in co-located clusters, reducing throughput by up to 46% [2]. Machine learning workloads demonstrate unique GPU-CPU-memory interdependencies with 6.5-10x performance differences [3], while microservices exhibit cross-VM correlations with up to 79% performance overhead compared to monolithic architectures [4]."
  },
  {
    "objectID": "concepts/research/cloud-resource-correlations-report.html#empirical-correlation-coefficients",
    "href": "concepts/research/cloud-resource-correlations-report.html#empirical-correlation-coefficients",
    "title": "Cloud Resource Metrics Correlation Patterns: Empirical Research Report",
    "section": "",
    "text": "Research on cloud workload patterns reveals strong temporal correlations in resource usage patterns [1]. Studies of memory access patterns in SPEC CPU2017 benchmarks show that ~80% of workloads exhibit correlation in their access intervals, with all correlated workloads demonstrating Hurst parameters &gt; 0.5, confirming self-similarity and long-range dependence [1]. This indicates that resource usage is predictable in the short-term (up to a few hours).\n\n\n\nIn SPEC CPU2017 workloads: - ~80% of applications show correlation in memory access patterns (vs. &lt;30% in SPEC CPU2006) [5] - All correlated workloads demonstrate Hurst parameters &gt; 0.5, confirming self-similarity [5] - Memory access intervals at small time scales (milliseconds) follow exponential distribution - Aggregated processes at large scales (minutes) show self-similarity - Some benchmarks use up to 16GB main memory and 2.3GB/s memory bandwidth [5]\n\n\n\nMicrosoft’s Resource Central study on Azure workloads reveals strong positive correlations between utilization metrics [6]: - CPU utilization correlates with memory usage - Disk I/O operations correlate with CPU cycles - Network latency impacts CPU wait times - Higher utilization VMs tend to be smaller and live longer - Negative correlation exists between VM size and utilization\nIncluding these correlated features improves predictive performance significantly compared to CPU-only models."
  },
  {
    "objectID": "concepts/research/cloud-resource-correlations-report.html#application-specific-correlation-patterns",
    "href": "concepts/research/cloud-resource-correlations-report.html#application-specific-correlation-patterns",
    "title": "Cloud Resource Metrics Correlation Patterns: Empirical Research Report",
    "section": "",
    "text": "Web applications demonstrate three distinct daily and three weekly workload patterns based on K-Means clustering analysis of 3,191 daily and 466 weekly data points [7]: - Time-series analysis captures temporal dependencies effectively - Recurring patterns link to service quality metrics - Service Workload Patterns (SWPs) remain relatively stable during normal operations [8] - Fixed mapping exists between infrastructure input and QoS during stable periods [8]\n\n\n\nDatabase systems show specific correlation patterns: - Peak operations significantly exceed baseline loads (specific ratios vary by workload type) [9] - Strong correlation between unsuccessful jobs and requested resources (CPU, memory, disk) [9] - Terminated tasks utilize significant cloud resources before being killed, wasting compute cycles [9] - Enhanced monitoring available at 1, 5, 10, 15, 30, or 60-second intervals for Aurora/RDS [9]\n\n\n\nML workloads demonstrate unique resource patterns [3]:\nTraining Phase: - GPU performance shows 6.5x speedup (2 hours GPU vs 13 hours CPU for 20 epochs) in comparative studies [10] - GPU compute improved 32x in 9 years vs 13x for memory bandwidth, creating bottleneck [11] - ResNet-50 requires 14 days on single M40 GPU for 100 epochs [12] - NeuSight framework reduces prediction error from 121.4% to 2.3% for GPT-3 latency [12]\nInference Phase: - Memory-efficient deep learning inference techniques enable incremental weight loading [13] - KV caches statically over-provisioned for max sequence length (e.g., 2048) [13] - Lower resource requirements but latency-sensitive - CPUs viable for lightweight model inference with optimization\n\n\n\nMicroservices exhibit cross-VM workload correlations with significant performance implications [14]: - CrossTrace achieves &gt;90% accuracy correlating thousands of spans within seconds using eBPF [14] - Microservice performance can be 79.1% slower than monolithic on same hardware [14] - 4.22x more time in runtime libraries (Node.js), 2.69x (Java EE) [14] - Container-based microservices can reduce infrastructure costs by 70% despite overhead [15]\nKey metrics for microservice benchmarking [15]: - Latency (primary concern) - Throughput - Scalability patterns - CPU usage per service - Memory usage patterns - Network usage between services"
  },
  {
    "objectID": "concepts/research/cloud-resource-correlations-report.html#time-lagged-correlations",
    "href": "concepts/research/cloud-resource-correlations-report.html#time-lagged-correlations",
    "title": "Cloud Resource Metrics Correlation Patterns: Empirical Research Report",
    "section": "",
    "text": "Research identifies important time-lagged relationships [16]: - CPU allocation spikes → Memory pressure (delayed response) - CPU bottlenecks cause queuing, leading to subsequent memory issues - Network congestion correlates with later CPU spikes - Performance interference from memory thrashing can reduce throughput by 46% even without over-commitment [16]\n\n\n\nGoogle Cloud documentation confirms monitoring delays [17]: - Metric collection latency: 2-4 minutes for Pub/Sub metrics - Metrics sampled every 60 seconds may take up to 240 seconds to become visible - This affects autoscaling responsiveness and anomaly detection - High-frequency monitoring (1-minute windows) recommended for 99th percentile tracking\n\n\n\nLSTM and RNN models effectively capture temporal dependencies [18]: - Long Short Term Memory RNN achieved MSE of 3.17×10⁻³ on web server log datasets [18] - Attention-based LSTM encoder-decoder networks map historical sequences to predictions [18] - esDNN addresses LSTM gradient issues using GRU-based algorithms for multivariate series [18] - Models retain contextual information across time steps for evolving workload trends"
  },
  {
    "objectID": "concepts/research/cloud-resource-correlations-report.html#correlation-patterns-by-operating-state",
    "href": "concepts/research/cloud-resource-correlations-report.html#correlation-patterns-by-operating-state",
    "title": "Cloud Resource Metrics Correlation Patterns: Empirical Research Report",
    "section": "",
    "text": "During normal operations: - Service Workload Patterns (SWPs) remain relatively stable [8] - Fixed mapping exists between infrastructure input and Quality of Service metrics - Predictable resource consumption patterns enable proactive management - Small variations in consecutive time steps allow simple prediction methods\n\n\n\nUnder peak load: - Memory becomes primary bottleneck in co-located clusters, causing up to 46% throughput reduction [2] - Unmovable allocations scattered across address space cause fragmentation (Meta datacenters) [2] - CPU and disk I/O show daily cyclical correlation patterns - Memory usage remains approximately constant while other resources spike\n\n\n\nDuring failures [9]: - Significant correlation between unsuccessful tasks and requested resources (CPU, memory, disk) - Failed jobs consumed many resources before being killed, heavily wasting CPU and RAM - All tasks with scheduling class 3 failed in Google cluster traces - Direct relationship exists between scheduling class, priority, and failure rates"
  },
  {
    "objectID": "concepts/research/cloud-resource-correlations-report.html#quantitative-correlation-matrices",
    "href": "concepts/research/cloud-resource-correlations-report.html#quantitative-correlation-matrices",
    "title": "Cloud Resource Metrics Correlation Patterns: Empirical Research Report",
    "section": "",
    "text": "Based on Alibaba cluster traces (4,000 machines, 8 days, 71K online services) [19]: - CPU and disk I/O show daily cyclical correlation patterns - Memory usage exhibits weak correlation with CPU cycles in co-located workloads - Network throughput correlates with CPU during batch processing phases - Sigma scheduler manages online services, Fuxi manages batch workloads\n\n\n\nEstablished correlations from production systems [8]: - Optimal CPU utilization varies by workload (20-50% for latency-sensitive) - Memory utilization &gt; 80% → Significant performance degradation begins - Network latency increases → CPU wait time increases proportionally - Strong positive correlation between all utilization metrics (Microsoft Azure study)"
  },
  {
    "objectID": "concepts/research/cloud-resource-correlations-report.html#published-datasets-for-validation",
    "href": "concepts/research/cloud-resource-correlations-report.html#published-datasets-for-validation",
    "title": "Cloud Resource Metrics Correlation Patterns: Empirical Research Report",
    "section": "",
    "text": "Multiple versions available on GitHub [19]: - cluster-trace-v2017: 1,300 machines, 12 hours, online+batch workloads - cluster-trace-v2018: 4,000 machines, 8 days, 71K online services, 4M batch jobs - AMTrace: Fine-granularity microarchitectural metrics - Size: 270+ GB uncompressed (50 GB compressed) - Contains: DAG dependency information for offline tasks - URL: https://github.com/alibaba/clusterdata\n\n\n\n2019 dataset contains [20]: - 2.4 TiB compressed workload traces from 8 Borg cells - Available via BigQuery for analysis - CPU usage histograms per 5-minute period - Alloc sets information and job-parent relationships for MapReduce - Detailed resource usage and job failure patterns - URL: https://github.com/google/cluster-data"
  },
  {
    "objectID": "concepts/research/cloud-resource-correlations-report.html#key-findings-and-implications",
    "href": "concepts/research/cloud-resource-correlations-report.html#key-findings-and-implications",
    "title": "Cloud Resource Metrics Correlation Patterns: Empirical Research Report",
    "section": "",
    "text": "Strong temporal correlations with self-similarity confirmed by Hurst parameters &gt; 0.5 [1]\n~80% of SPEC CPU2017 workloads show memory access correlation\nResource usage predictable up to several hours using LSTM/RNN models\nCritical for proactive resource management and autoscaling\n\n\n\n\n\nMemory thrashing can reduce throughput by 46% even without over-commitment [2]\nFragmentation from unmovable allocations is primary cause in production datacenters\nUnlike CPU/disk, memory usage remains constant during load spikes\nMemory-aware scheduling and contiguity management crucial for performance\n\n\n\n\n\nWeb applications show 3 daily and 3 weekly distinct patterns from clustering analysis [7]\nML workloads show 6.5-10x GPU performance advantage, require GPU-CPU-memory balance [3]\nMicroservices exhibit 79% performance overhead but 70% infrastructure cost reduction [14]\nDatabase workloads need monitoring at sub-minute intervals for accurate correlation\n\n\n\n\n\nSub-minute monitoring (1-60 second intervals) required to capture spikes [17]\nGoogle Cloud metrics have 2-4 minute collection latency affecting real-time decisions\nMulti-metric correlation essential for root cause analysis and anomaly detection [16]\nTime-lagged effects and cascade failures must be considered in autoscaling policies [18]"
  },
  {
    "objectID": "concepts/research/cloud-resource-correlations-report.html#references",
    "href": "concepts/research/cloud-resource-correlations-report.html#references",
    "title": "Cloud Resource Metrics Correlation Patterns: Empirical Research Report",
    "section": "",
    "text": "[1] Zou, Y., et al. (2022). “Temporal Characterization of Memory Access Behaviors in SPEC CPU2017.” Future Generation Computer Systems, Volume 129, pp. 206-217. https://www.sciencedirect.com/science/article/abs/pii/S0167739X21004908 ~80% of SPEC CPU2017 workloads show correlation in memory access intervals with Hurst parameters &gt;0.5.\n[2] “Performance Interference of Memory Thrashing in Virtualized Cloud Environments.” (2016). IEEE International Conference on Cloud Computing. https://ieeexplore.ieee.org/document/7820282/ Memory thrashing can reduce system throughput by 46% even without memory over-commitment.\n[3] “Comparative Analysis of CPU and GPU Profiling for Deep Learning Models.” (2023). ArXiv Preprint. https://arxiv.org/pdf/2309.02521 Training time comparison: CPU ~13 hours vs GPU ~2 hours for 20 epochs (6.5x speedup).\n[4] IBM Research. (2016). “Workload Characterization for Microservices.” IEEE International Symposium on Workload Characterization. https://ieeexplore.ieee.org/document/7581269/ Microservice performance 79.1% slower than monolithic on same hardware, 4.22x overhead in runtime.\n[5] Singh, S., and Awasthi, M. (2019). “Memory Centric Characterization and Analysis of SPEC CPU2017 Suite.” ICPE 2019. https://arxiv.org/abs/1910.00651 ~50% of dynamic instructions are memory intensive; benchmarks use up to 16GB RAM and 2.3GB/s bandwidth.\n[6] Microsoft Research. (2017). “Resource Central: Understanding and Predicting Workloads for Improved Resource Management.” SOSP 2017. https://www.microsoft.com/en-us/research/wp-content/uploads/2017/10/Resource-Central-SOSP17.pdf Strong positive correlation between utilization metrics in Azure workloads.\n[7] “Understanding Web Application Workloads: Systematic Literature Review.” (2024). ArXiv & IEEE. https://arxiv.org/abs/2409.12299 Identifies 3 daily and 3 weekly patterns using K-Means clustering on 3,191 daily and 466 weekly data points.\n[8] “Service Workload Patterns for QoS-Driven Cloud Resource Management.” (2018). Journal of Cloud Computing: Advances, Systems and Applications. https://journalofcloudcomputing.springeropen.com/articles/10.1186/s13677-018-0106-7 Service Workload Patterns remain stable during normal operations with fixed infrastructure-QoS mapping.\n[9] “Analysis of Job Failure and Prediction Model for Cloud Computing Using Machine Learning.” (2022). Sensors, 22(5), 2035. https://www.mdpi.com/1424-8220/22/5/2035 Significant correlation between unsuccessful tasks and requested resources; failed jobs waste CPU and RAM.\n[10] “Comparative Analysis of CPU and GPU Profiling for Deep Learning Models.” (2023). ArXiv Preprint. https://arxiv.org/pdf/2309.02521 Documented 6.5x speedup for GPU training vs CPU across multiple deep learning models.\n[11] Lee, S., et al. (2024). “Forecasting GPU Performance for Deep Learning Training and Inference.” ASPLOS 2025. https://dl.acm.org/doi/10.1145/3669940.3707265 NeuSight framework; GPU compute increased 32x in 9 years vs 13x for memory bandwidth.\n[12] Lee, S., et al. (2024). “Forecasting GPU Performance for Deep Learning Training and Inference.” ArXiv. https://arxiv.org/abs/2407.13853 NeuSight reduces GPT-3 latency prediction error from 121.4% to 2.3%.\n[13] “Memory-efficient Deep Learning Inference in Trusted Execution Environments.” (2021). Journal of Systems Architecture. https://www.sciencedirect.com/science/article/abs/pii/S1383762121001314 MDI approach with incremental weight loading and data layout reorganization for inference.\n[14] “CrossTrace: Efficient Cross-Thread and Cross-Service Span Correlation.” (2025). ArXiv. https://arxiv.org/html/2508.11342 eBPF-based tracing achieves &gt;90% accuracy correlating spans; includes IBM microservices overhead study.\n[15] “Microservice Performance Degradation Correlation.” (2020). ResearchGate. https://www.researchgate.net/publication/346782444_Microservice_Performance_Degradation_Correlation Container-based microservices can reduce infrastructure costs by 70% despite performance overhead.\n[16] “Contiguitas: The Pursuit of Physical Memory Contiguity in Datacenters.” (2023). 50th Annual International Symposium on Computer Architecture. https://dl.acm.org/doi/10.1145/3579371.3589079 Memory fragmentation from unmovable allocations causes performance degradation in production.\n[17] Google Cloud. (2024). “Retention and Latency of Metric Data.” Cloud Monitoring Documentation. https://cloud.google.com/monitoring/api/v3/latency-n-retention Pub/Sub metrics have 2-4 minute latencies; sampled every 60 seconds, visible after 240 seconds.\n[18] Kumar, J., et al. (2018). “Long Short Term Memory RNN Based Workload Forecasting for Cloud Datacenters.” Procedia Computer Science, Volume 125, pp. 676-682. https://www.sciencedirect.com/science/article/pii/S1877050917328557 LSTM-RNN achieves MSE of 3.17×10⁻³ on web server log datasets.\n[19] Alibaba Cloud. (2018). “Alibaba Cluster Trace v2018.” GitHub Repository. https://github.com/alibaba/clusterdata 4,000 machines, 8 days, 71K online services, 4M batch jobs, 270+ GB uncompressed data.\n[20] Google Research. (2019). “Google Cluster Workload Traces 2019.” Google Research Datasets. https://github.com/google/cluster-data 2.4 TiB compressed traces from 8 Borg cells, available via BigQuery."
  },
  {
    "objectID": "concepts/research/opentslm-foundation-model-evaluation.html",
    "href": "concepts/research/opentslm-foundation-model-evaluation.html",
    "title": "OpenTSLM Foundation Model Evaluation",
    "section": "",
    "text": "Evaluation Date: October 2, 2025 Evaluator: Research Team Status: ❌ Not Recommended for Cloud Anomaly Detection\n\n\n\nOpenTSLM is a Stanford-developed timeseries foundation model that processes multivariate time series through language model reasoning. While architecturally innovative, it is not suitable for cloud resource anomaly detection due to:\n\n❌ No Pre-trained Weights - Requires training from scratch (5-stage curriculum, days/weeks with GPU)\n❌ Medical Domain Focus - Optimized for ECG, EEG, and human activity recognition, not cloud metrics\n❌ High Training Overhead - ~6GB dataset downloads, CUDA GPU required, HuggingFace authentication needed\n❌ Not Designed for Anomaly Detection - Focused on Q&A, captioning, and chain-of-thought reasoning\n\nRecommendation: Explore purpose-built anomaly detection models or cloud-metric-trained foundation models instead.\n\n\n\n\n\n\nWhile reviewing HackerNews on October 2, 2025, we discovered OpenTSLM - a newly published timeseries foundation model from Stanford. Given our cloud-resource-simulator project’s need for anomaly detection capabilities, we investigated whether OpenTSLM could serve as a foundation model for:\n\nTimeseries Anomaly Detection in cloud resource utilization\nPattern Recognition across multivariate cloud metrics (CPU, memory, network, disk)\nNatural Language Explanations for detected anomalies\n\n\n\n\nCan OpenTSLM be adapted or fine-tuned for cloud resource anomaly detection in the cloud-resource-simulator project?\n\n\n\n\n\n\n\n\nForked Repository to personal GitHub account: nehalecky/OpenTSLM\nCloned Locally to /Users/nehalecky/Projects/cloudzero/OpenTSLM\nInitialized Submodules (open_flamingo dependency)\nExamined Documentation - README, code structure, training pipeline\n\n\n\n\n\nSearched for pre-trained checkpoints in repository\nChecked HuggingFace for published models\nReviewed training scripts for weight download mechanisms\nAnalyzed curriculum_learning.py for checkpoint handling\n\n\n\n\n\nReviewed model implementations (OpenTSLMFlamingo, OpenTSLMSP)\nExamined encoder architecture (TransformerCNN)\nAnalyzed training datasets and their domains\nAssessed infrastructure requirements\n\n\n\n\n\n\n\n\nOpenTSLM does NOT provide pre-trained model weights. Users must train models from scratch using the full 5-stage curriculum.\nWhat’s Available: - Base LLM models from HuggingFace (Llama 3.2-1B, Gemma-3-270m) - These are untrained base models, not OpenTSLM-trained weights - No shortcuts or intermediate checkpoints provided\nWhat’s Required:\n# 1. Obtain base LLM (requires HuggingFace authentication)\nhuggingface-cli login\n\n# 2. Run full 5-stage curriculum training\npython curriculum_learning.py --model OpenTSLMFlamingo\n\n# Stages:\n# - Stage 1: Multiple Choice Q&A (TSQA dataset)\n# - Stage 2: Time Series Captioning (M4 dataset)\n# - Stage 3: HAR Chain-of-Thought (~download required)\n# - Stage 4: Sleep Staging CoT (EEG data)\n# - Stage 5: ECG Q&A CoT (~6GB download)\n\n# Training time: Days to weeks depending on GPU\nCheckpoints Storage:\nresults/\n└── Llama3_2_1B/\n    └── OpenTSLMFlamingo/\n        ├── stage1_mcq/checkpoints/best_model.pt\n        ├── stage2_captioning/checkpoints/best_model.pt\n        ├── stage3_cot/checkpoints/best_model.pt\n        ├── stage4_sleep_cot/checkpoints/best_model.pt\n        └── stage5_ecg_cot/checkpoints/best_model.pt\n\n\n\n\nPrimary Use Cases: - ECG Analysis - 12-lead electrocardiogram interpretation - Sleep Staging - EEG-based sleep classification - Human Activity Recognition - Accelerometer/gyroscope data - Medical Time Series Q&A - Clinical reasoning tasks\nTraining Datasets: | Stage | Dataset | Domain | Size | |——-|———|——–|——| | 1 | TSQA | Time Series Q&A | Auto-download | | 2 | M4 | General forecasting | Auto-download | | 3 | HAR | Human activity | ~Download | | 4 | SleepEDF | EEG sleep staging | Auto-download | | 5 | ECG-QA + PTB-XL | 12-lead ECG | ~6GB |\nDomain Characteristics: - High sampling rates (100-500 Hz for medical signals) - Strong physiological constraints (QRS complexes, sleep stages) - Clinical terminology and reasoning patterns - Diagnostic question-answering focus\nCloud Metrics Characteristics: - Low sampling rates (1-5 minute intervals typical) - Different correlation patterns (resource contention, not physiology) - Infrastructure terminology (pods, nodes, services) - Anomaly detection focus (not diagnostic Q&A)\nConclusion: Significant domain gap between medical time series and cloud infrastructure metrics.\n\n\n\n\n\n\n1. OpenTSLMFlamingo Architecture\nTime Series Input → TransformerCNN Encoder → MLP Projector → Frozen LLM\n                                                              ↓\n                                                    Natural Language Output\nComponents: - Encoder: TransformerCNN - Processes multivariate time series of any length - Projector: MLP layers align time series embeddings with LLM embedding space - LLM: Pre-trained language model (Llama 3.2-1B or Gemma variants) - Training: LoRA fine-tuning with parameter-efficient adaptation\n2. Alternative: OpenTSLMSP - Uses special tokens instead of Flamingo architecture - Same encoder/projector concept - Different integration with base LLM\nKey Innovation: - Combines time series understanding with natural language reasoning - Enables chain-of-thought explanations for predictions - Processes multivariate time series with variable lengths\n\n\n\n\n\n\n\n\nPreferred: CUDA-enabled NVIDIA GPU\nAlternative: Apple Silicon MPS (with compatibility warnings)\nWarning: Models trained on CUDA may not transfer to MPS\n\n\n\n\n# Core ML/DL (from requirements.txt)\ntorch\ntransformers\npeft  # LoRA fine-tuning\nhuggingface-hub\n\n# Time Series\nchronos-forecasting\nwfdb  # ECG signal processing\n\n# Vision/Multimodal\nopen-clip-torch\neinops\n\n# Data Processing\nnumpy, pandas\nscikit-learn\nmatplotlib\n\n\n\nStage 1: Multiple Choice Questions (~hours)\npython curriculum_learning.py --model OpenTSLMFlamingo --stages stage1_mcq\n\nDataset: TSQA (Time Series Question Answering)\nTask: Answer multiple choice questions about time series patterns\nAuto-downloads from HuggingFace\n\nStage 2: Captioning (~hours)\npython curriculum_learning.py --model OpenTSLMFlamingo --stages stage2_captioning\n\nDataset: M4 competition data\nTask: Generate natural language descriptions of time series\nFocuses on pattern recognition and verbalization\n\nStage 3: HAR Chain-of-Thought (~hours-days)\npython curriculum_learning.py --model OpenTSLMFlamingo --stages stage3_cot\n\nDataset: Human Activity Recognition (HAR)\nDownload: https://polybox.ethz.ch/index.php/s/kD74GnMYxn3HBEM/download\nTask: Classify activities with reasoning steps\n\nStage 4: Sleep Staging CoT (~hours-days)\npython curriculum_learning.py --model OpenTSLMFlamingo --stages stage4_sleep_cot\n\nDataset: SleepEDF (EEG data)\nTask: Sleep stage classification with chain-of-thought\nMedical domain specialization begins\n\nStage 5: ECG Q&A CoT (~days)\npython curriculum_learning.py --model OpenTSLMFlamingo --stages stage5_ecg_cot\n\nDatasets: ECG-QA + PTB-XL (~6GB combined)\nDownload: https://polybox.ethz.ch/index.php/s/D5QaJSEw4dXkzXm/download\nTask: 12-lead ECG clinical reasoning\nMost medically specialized stage\n\nFull Curriculum:\npython curriculum_learning.py --model OpenTSLMFlamingo\n# Estimated time: Days to weeks depending on GPU\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRequirement\nOpenTSLM Support\nAssessment\n\n\n\n\nAnomaly Detection\n❌ Not primary focus\nQ&A/captioning oriented, not outlier detection\n\n\nCloud Metrics\n❌ Medical training data\nDomain mismatch (ECG/EEG vs CPU/memory)\n\n\nPre-trained Model\n❌ Must train from scratch\nProhibitive for exploration phase\n\n\nFast Inference\n⚠️ Depends on LLM size\nLlama 3.2-1B moderate, Gemma-270m faster\n\n\nMultivariate Support\n✅ Native support\nHandles multiple metrics simultaneously\n\n\nVariable Length\n✅ Any length\nGood for different time windows\n\n\nExplainability\n✅ Chain-of-thought\nNatural language reasoning available\n\n\n\n\n\n\n✅ Multivariate Time Series - Can process CPU, memory, network, disk together ✅ Variable Length Sequences - Handles different monitoring windows ✅ Natural Language Output - Could explain anomalies in plain English ✅ Modular Architecture - Encoder/projector/LLM separation allows adaptation\n\n\n\n❌ No Pre-trained Weights - Cannot evaluate without weeks of training ❌ Medical Domain Bias - Training data fundamentally different from cloud metrics ❌ Not Anomaly-Focused - Designed for Q&A, not outlier/anomaly detection ❌ Training Overhead - Requires substantial GPU resources and time ❌ Dataset Mismatch - No cloud infrastructure training data included\n\n\n\nFor Anomaly Detection: 1. Traditional ML Models - Isolation Forest (scikit-learn) - LSTM Autoencoders (reconstruction error) - Prophet (Facebook) for seasonal decomposition\n\nPurpose-Built Time Series Models\n\nAmazon Chronos - Already integrated in our project\nGoogle TimesFM - Zero-shot forecasting\nBoth have pre-trained weights and better domain fit\n\nCloud-Specific Models\n\nAWS DeepAR (if using AWS data)\nAzure Anomaly Detector (if using Azure data)\nGCP Time Series Insights (if using GCP data)\n\n\nFor Explainability: - SHAP values on anomaly detection models - Attention weights from transformer-based detectors - Rule-based explanations from traditional methods\n\n\n\n\n\n\n\n\nGitHub: https://github.com/nehalecky/OpenTSLM\nUpstream: https://github.com/StanfordBDHG/OpenTSLM\nLocal Path: /Users/nehalecky/Projects/cloudzero/OpenTSLM\nStars: 73 (as of Oct 2, 2025)\nCreated: May 2025\nLast Updated: October 1, 2025\n\n\n\n\nOpenTSLM/\n├── curriculum_learning.py          # Main training script\n├── requirements.txt                # 21 dependencies\n├── src/\n│   ├── model/\n│   │   ├── encoder/               # TransformerCNN\n│   │   ├── llm/                   # OpenTSLMFlamingo, OpenTSLMSP\n│   │   └── projector/             # MLP alignment\n│   ├── time_series_datasets/      # Dataset loaders (TSQA, M4, HAR, Sleep, ECG)\n│   ├── prompt/                    # Prompt templates\n│   └── open_flamingo/            # Submodule\n├── evaluation/                    # Evaluation scripts\n├── test/                         # Unit tests\n└── data/                         # Auto-downloaded datasets\n\n\n\n\nPaper: https://doi.org/10.13140/RG.2.2.14827.60963\nWebsite: https://www.opentslm.com\nRelated Papers:\n\nECG-QA\nPTB-XL Dataset\n\n\n\n\n\n\n\n\n\nPrimary Reasons: 1. Training Barrier - No pre-trained weights; requires weeks of GPU time 2. Domain Mismatch - Medical focus doesn’t transfer well to cloud infrastructure 3. Wrong Task Focus - Designed for Q&A/captioning, not anomaly detection 4. Better Alternatives Exist - Purpose-built models with cloud data experience\n\n\n\nWhile OpenTSLM demonstrates impressive multimodal capabilities for medical time series, the combination of lacking pre-trained weights and medical domain specialization makes it impractical for our cloud anomaly detection needs. The opportunity cost of training from scratch (GPU time, dataset engineering, validation) outweighs potential benefits when superior alternatives exist.\nKey Insight: Foundation models are only valuable if: - Pre-trained weights are available (transfer learning), OR - Training data closely matches your domain\nOpenTSLM fails both criteria for cloud metrics.\n\n\n\nImmediate Actions: 1. ✅ Archive Fork - Keep for reference, but don’t actively develop 2. ✅ Document Evaluation - This report serves as institutional knowledge\nAlternative Exploration Priority:\nHigh Priority (Immediate): - [ ] Enhance Chronos Integration - Already in our codebase, has pre-trained weights - [ ] Explore TimesFM - Google’s zero-shot forecasting model - [ ] Traditional Anomaly Detection - Isolation Forest baseline\nMedium Priority (Next Quarter): - [ ] Investigate Cloud-Specific Models - AWS DeepAR, Azure Anomaly Detector - [ ] Custom LSTM Autoencoder - Train on our synthetic cloud data - [ ] Hybrid Approach - Chronos forecasting + statistical anomaly detection\nLow Priority (Future Research): - [ ] Foundation Model Fine-tuning - If cloud-trained foundation model emerges - [ ] LLM-Based Explainability - Use GPT-4/Claude for anomaly explanations\n\n\n\n\n\n\n\nPre-Evaluation Checklist: 1. ✅ Check for Pre-trained Weights - First question, not last 2. ✅ Verify Domain Match - Medical ≠ Cloud Infrastructure 3. ✅ Assess Task Alignment - Q&A ≠ Anomaly Detection 4. ✅ Estimate Training Cost - GPU hours, dataset size, time to validation\nRed Flags Identified: - 🚩 “Train from scratch” without pre-trained option - 🚩 All training examples from unrelated domain - 🚩 No mentions of your use case in documentation - 🚩 Base models require special access (Llama 3.2 gating)\nGreen Flags for Future Models: - ✅ Pre-trained weights on HuggingFace - ✅ Training data includes infrastructure/system metrics - ✅ Explicit anomaly detection capabilities - ✅ Active community with cloud use cases\n\n\n\nWhat Worked Well: - Using repository-manager agent for systematic analysis - Forking before deep evaluation (preserves exploration) - Checking for weights availability early - Documenting findings immediately\nProcess Improvements: - Consider creating “Model Evaluation Template” for future assessments - Build checklist of domain-fit questions - Maintain “Models Under Consideration” tracking document\n\n\n\n\n\n\n\n\nGitHub Repository: https://github.com/StanfordBDHG/OpenTSLM\nProject Website: https://www.opentslm.com\nResearch Paper: https://doi.org/10.13140/RG.2.2.14827.60963\nOur Fork: https://github.com/nehalecky/OpenTSLM\n\n\n\n\n\nAmazon Chronos: https://github.com/amazon-science/chronos-forecasting\nGoogle TimesFM: https://github.com/google-research/timesfm\nHugging Face Time Series: https://huggingface.co/models?pipeline_tag=time-series-forecasting\n\n\n\n\n\nECG-QA Paper: https://arxiv.org/abs/2306.15681\nPTB-XL Dataset: https://www.nature.com/articles/s41597-020-0495-6\nSleepEDF: https://physionet.org/content/sleep-edfx/1.0.0/\n\n\n\n\n\nFinOps Foundation: https://www.finops.org/\nAWS CloudWatch Anomaly Detection: https://aws.amazon.com/cloudwatch/\nAzure Monitor Anomaly Detector: https://azure.microsoft.com/en-us/products/ai-services/ai-anomaly-detector\n\n\nDocument Status: Final Last Updated: October 2, 2025 Next Review: When new cloud-focused foundation models emerge"
  },
  {
    "objectID": "concepts/research/opentslm-foundation-model-evaluation.html#executive-summary",
    "href": "concepts/research/opentslm-foundation-model-evaluation.html#executive-summary",
    "title": "OpenTSLM Foundation Model Evaluation",
    "section": "",
    "text": "OpenTSLM is a Stanford-developed timeseries foundation model that processes multivariate time series through language model reasoning. While architecturally innovative, it is not suitable for cloud resource anomaly detection due to:\n\n❌ No Pre-trained Weights - Requires training from scratch (5-stage curriculum, days/weeks with GPU)\n❌ Medical Domain Focus - Optimized for ECG, EEG, and human activity recognition, not cloud metrics\n❌ High Training Overhead - ~6GB dataset downloads, CUDA GPU required, HuggingFace authentication needed\n❌ Not Designed for Anomaly Detection - Focused on Q&A, captioning, and chain-of-thought reasoning\n\nRecommendation: Explore purpose-built anomaly detection models or cloud-metric-trained foundation models instead."
  },
  {
    "objectID": "concepts/research/opentslm-foundation-model-evaluation.html#research-context",
    "href": "concepts/research/opentslm-foundation-model-evaluation.html#research-context",
    "title": "OpenTSLM Foundation Model Evaluation",
    "section": "",
    "text": "While reviewing HackerNews on October 2, 2025, we discovered OpenTSLM - a newly published timeseries foundation model from Stanford. Given our cloud-resource-simulator project’s need for anomaly detection capabilities, we investigated whether OpenTSLM could serve as a foundation model for:\n\nTimeseries Anomaly Detection in cloud resource utilization\nPattern Recognition across multivariate cloud metrics (CPU, memory, network, disk)\nNatural Language Explanations for detected anomalies\n\n\n\n\nCan OpenTSLM be adapted or fine-tuned for cloud resource anomaly detection in the cloud-resource-simulator project?"
  },
  {
    "objectID": "concepts/research/opentslm-foundation-model-evaluation.html#investigation-methodology",
    "href": "concepts/research/opentslm-foundation-model-evaluation.html#investigation-methodology",
    "title": "OpenTSLM Foundation Model Evaluation",
    "section": "",
    "text": "Forked Repository to personal GitHub account: nehalecky/OpenTSLM\nCloned Locally to /Users/nehalecky/Projects/cloudzero/OpenTSLM\nInitialized Submodules (open_flamingo dependency)\nExamined Documentation - README, code structure, training pipeline\n\n\n\n\n\nSearched for pre-trained checkpoints in repository\nChecked HuggingFace for published models\nReviewed training scripts for weight download mechanisms\nAnalyzed curriculum_learning.py for checkpoint handling\n\n\n\n\n\nReviewed model implementations (OpenTSLMFlamingo, OpenTSLMSP)\nExamined encoder architecture (TransformerCNN)\nAnalyzed training datasets and their domains\nAssessed infrastructure requirements"
  },
  {
    "objectID": "concepts/research/opentslm-foundation-model-evaluation.html#key-findings",
    "href": "concepts/research/opentslm-foundation-model-evaluation.html#key-findings",
    "title": "OpenTSLM Foundation Model Evaluation",
    "section": "",
    "text": "OpenTSLM does NOT provide pre-trained model weights. Users must train models from scratch using the full 5-stage curriculum.\nWhat’s Available: - Base LLM models from HuggingFace (Llama 3.2-1B, Gemma-3-270m) - These are untrained base models, not OpenTSLM-trained weights - No shortcuts or intermediate checkpoints provided\nWhat’s Required:\n# 1. Obtain base LLM (requires HuggingFace authentication)\nhuggingface-cli login\n\n# 2. Run full 5-stage curriculum training\npython curriculum_learning.py --model OpenTSLMFlamingo\n\n# Stages:\n# - Stage 1: Multiple Choice Q&A (TSQA dataset)\n# - Stage 2: Time Series Captioning (M4 dataset)\n# - Stage 3: HAR Chain-of-Thought (~download required)\n# - Stage 4: Sleep Staging CoT (EEG data)\n# - Stage 5: ECG Q&A CoT (~6GB download)\n\n# Training time: Days to weeks depending on GPU\nCheckpoints Storage:\nresults/\n└── Llama3_2_1B/\n    └── OpenTSLMFlamingo/\n        ├── stage1_mcq/checkpoints/best_model.pt\n        ├── stage2_captioning/checkpoints/best_model.pt\n        ├── stage3_cot/checkpoints/best_model.pt\n        ├── stage4_sleep_cot/checkpoints/best_model.pt\n        └── stage5_ecg_cot/checkpoints/best_model.pt\n\n\n\n\nPrimary Use Cases: - ECG Analysis - 12-lead electrocardiogram interpretation - Sleep Staging - EEG-based sleep classification - Human Activity Recognition - Accelerometer/gyroscope data - Medical Time Series Q&A - Clinical reasoning tasks\nTraining Datasets: | Stage | Dataset | Domain | Size | |——-|———|——–|——| | 1 | TSQA | Time Series Q&A | Auto-download | | 2 | M4 | General forecasting | Auto-download | | 3 | HAR | Human activity | ~Download | | 4 | SleepEDF | EEG sleep staging | Auto-download | | 5 | ECG-QA + PTB-XL | 12-lead ECG | ~6GB |\nDomain Characteristics: - High sampling rates (100-500 Hz for medical signals) - Strong physiological constraints (QRS complexes, sleep stages) - Clinical terminology and reasoning patterns - Diagnostic question-answering focus\nCloud Metrics Characteristics: - Low sampling rates (1-5 minute intervals typical) - Different correlation patterns (resource contention, not physiology) - Infrastructure terminology (pods, nodes, services) - Anomaly detection focus (not diagnostic Q&A)\nConclusion: Significant domain gap between medical time series and cloud infrastructure metrics.\n\n\n\n\n\n\n1. OpenTSLMFlamingo Architecture\nTime Series Input → TransformerCNN Encoder → MLP Projector → Frozen LLM\n                                                              ↓\n                                                    Natural Language Output\nComponents: - Encoder: TransformerCNN - Processes multivariate time series of any length - Projector: MLP layers align time series embeddings with LLM embedding space - LLM: Pre-trained language model (Llama 3.2-1B or Gemma variants) - Training: LoRA fine-tuning with parameter-efficient adaptation\n2. Alternative: OpenTSLMSP - Uses special tokens instead of Flamingo architecture - Same encoder/projector concept - Different integration with base LLM\nKey Innovation: - Combines time series understanding with natural language reasoning - Enables chain-of-thought explanations for predictions - Processes multivariate time series with variable lengths\n\n\n\n\n\n\n\n\nPreferred: CUDA-enabled NVIDIA GPU\nAlternative: Apple Silicon MPS (with compatibility warnings)\nWarning: Models trained on CUDA may not transfer to MPS\n\n\n\n\n# Core ML/DL (from requirements.txt)\ntorch\ntransformers\npeft  # LoRA fine-tuning\nhuggingface-hub\n\n# Time Series\nchronos-forecasting\nwfdb  # ECG signal processing\n\n# Vision/Multimodal\nopen-clip-torch\neinops\n\n# Data Processing\nnumpy, pandas\nscikit-learn\nmatplotlib\n\n\n\nStage 1: Multiple Choice Questions (~hours)\npython curriculum_learning.py --model OpenTSLMFlamingo --stages stage1_mcq\n\nDataset: TSQA (Time Series Question Answering)\nTask: Answer multiple choice questions about time series patterns\nAuto-downloads from HuggingFace\n\nStage 2: Captioning (~hours)\npython curriculum_learning.py --model OpenTSLMFlamingo --stages stage2_captioning\n\nDataset: M4 competition data\nTask: Generate natural language descriptions of time series\nFocuses on pattern recognition and verbalization\n\nStage 3: HAR Chain-of-Thought (~hours-days)\npython curriculum_learning.py --model OpenTSLMFlamingo --stages stage3_cot\n\nDataset: Human Activity Recognition (HAR)\nDownload: https://polybox.ethz.ch/index.php/s/kD74GnMYxn3HBEM/download\nTask: Classify activities with reasoning steps\n\nStage 4: Sleep Staging CoT (~hours-days)\npython curriculum_learning.py --model OpenTSLMFlamingo --stages stage4_sleep_cot\n\nDataset: SleepEDF (EEG data)\nTask: Sleep stage classification with chain-of-thought\nMedical domain specialization begins\n\nStage 5: ECG Q&A CoT (~days)\npython curriculum_learning.py --model OpenTSLMFlamingo --stages stage5_ecg_cot\n\nDatasets: ECG-QA + PTB-XL (~6GB combined)\nDownload: https://polybox.ethz.ch/index.php/s/D5QaJSEw4dXkzXm/download\nTask: 12-lead ECG clinical reasoning\nMost medically specialized stage\n\nFull Curriculum:\npython curriculum_learning.py --model OpenTSLMFlamingo\n# Estimated time: Days to weeks depending on GPU"
  },
  {
    "objectID": "concepts/research/opentslm-foundation-model-evaluation.html#applicability-assessment-for-cloud-resource-simulator",
    "href": "concepts/research/opentslm-foundation-model-evaluation.html#applicability-assessment-for-cloud-resource-simulator",
    "title": "OpenTSLM Foundation Model Evaluation",
    "section": "",
    "text": "Requirement\nOpenTSLM Support\nAssessment\n\n\n\n\nAnomaly Detection\n❌ Not primary focus\nQ&A/captioning oriented, not outlier detection\n\n\nCloud Metrics\n❌ Medical training data\nDomain mismatch (ECG/EEG vs CPU/memory)\n\n\nPre-trained Model\n❌ Must train from scratch\nProhibitive for exploration phase\n\n\nFast Inference\n⚠️ Depends on LLM size\nLlama 3.2-1B moderate, Gemma-270m faster\n\n\nMultivariate Support\n✅ Native support\nHandles multiple metrics simultaneously\n\n\nVariable Length\n✅ Any length\nGood for different time windows\n\n\nExplainability\n✅ Chain-of-thought\nNatural language reasoning available\n\n\n\n\n\n\n✅ Multivariate Time Series - Can process CPU, memory, network, disk together ✅ Variable Length Sequences - Handles different monitoring windows ✅ Natural Language Output - Could explain anomalies in plain English ✅ Modular Architecture - Encoder/projector/LLM separation allows adaptation\n\n\n\n❌ No Pre-trained Weights - Cannot evaluate without weeks of training ❌ Medical Domain Bias - Training data fundamentally different from cloud metrics ❌ Not Anomaly-Focused - Designed for Q&A, not outlier/anomaly detection ❌ Training Overhead - Requires substantial GPU resources and time ❌ Dataset Mismatch - No cloud infrastructure training data included\n\n\n\nFor Anomaly Detection: 1. Traditional ML Models - Isolation Forest (scikit-learn) - LSTM Autoencoders (reconstruction error) - Prophet (Facebook) for seasonal decomposition\n\nPurpose-Built Time Series Models\n\nAmazon Chronos - Already integrated in our project\nGoogle TimesFM - Zero-shot forecasting\nBoth have pre-trained weights and better domain fit\n\nCloud-Specific Models\n\nAWS DeepAR (if using AWS data)\nAzure Anomaly Detector (if using Azure data)\nGCP Time Series Insights (if using GCP data)\n\n\nFor Explainability: - SHAP values on anomaly detection models - Attention weights from transformer-based detectors - Rule-based explanations from traditional methods"
  },
  {
    "objectID": "concepts/research/opentslm-foundation-model-evaluation.html#repository-information",
    "href": "concepts/research/opentslm-foundation-model-evaluation.html#repository-information",
    "title": "OpenTSLM Foundation Model Evaluation",
    "section": "",
    "text": "GitHub: https://github.com/nehalecky/OpenTSLM\nUpstream: https://github.com/StanfordBDHG/OpenTSLM\nLocal Path: /Users/nehalecky/Projects/cloudzero/OpenTSLM\nStars: 73 (as of Oct 2, 2025)\nCreated: May 2025\nLast Updated: October 1, 2025\n\n\n\n\nOpenTSLM/\n├── curriculum_learning.py          # Main training script\n├── requirements.txt                # 21 dependencies\n├── src/\n│   ├── model/\n│   │   ├── encoder/               # TransformerCNN\n│   │   ├── llm/                   # OpenTSLMFlamingo, OpenTSLMSP\n│   │   └── projector/             # MLP alignment\n│   ├── time_series_datasets/      # Dataset loaders (TSQA, M4, HAR, Sleep, ECG)\n│   ├── prompt/                    # Prompt templates\n│   └── open_flamingo/            # Submodule\n├── evaluation/                    # Evaluation scripts\n├── test/                         # Unit tests\n└── data/                         # Auto-downloaded datasets\n\n\n\n\nPaper: https://doi.org/10.13140/RG.2.2.14827.60963\nWebsite: https://www.opentslm.com\nRelated Papers:\n\nECG-QA\nPTB-XL Dataset"
  },
  {
    "objectID": "concepts/research/opentslm-foundation-model-evaluation.html#decision-next-steps",
    "href": "concepts/research/opentslm-foundation-model-evaluation.html#decision-next-steps",
    "title": "OpenTSLM Foundation Model Evaluation",
    "section": "",
    "text": "Primary Reasons: 1. Training Barrier - No pre-trained weights; requires weeks of GPU time 2. Domain Mismatch - Medical focus doesn’t transfer well to cloud infrastructure 3. Wrong Task Focus - Designed for Q&A/captioning, not anomaly detection 4. Better Alternatives Exist - Purpose-built models with cloud data experience\n\n\n\nWhile OpenTSLM demonstrates impressive multimodal capabilities for medical time series, the combination of lacking pre-trained weights and medical domain specialization makes it impractical for our cloud anomaly detection needs. The opportunity cost of training from scratch (GPU time, dataset engineering, validation) outweighs potential benefits when superior alternatives exist.\nKey Insight: Foundation models are only valuable if: - Pre-trained weights are available (transfer learning), OR - Training data closely matches your domain\nOpenTSLM fails both criteria for cloud metrics.\n\n\n\nImmediate Actions: 1. ✅ Archive Fork - Keep for reference, but don’t actively develop 2. ✅ Document Evaluation - This report serves as institutional knowledge\nAlternative Exploration Priority:\nHigh Priority (Immediate): - [ ] Enhance Chronos Integration - Already in our codebase, has pre-trained weights - [ ] Explore TimesFM - Google’s zero-shot forecasting model - [ ] Traditional Anomaly Detection - Isolation Forest baseline\nMedium Priority (Next Quarter): - [ ] Investigate Cloud-Specific Models - AWS DeepAR, Azure Anomaly Detector - [ ] Custom LSTM Autoencoder - Train on our synthetic cloud data - [ ] Hybrid Approach - Chronos forecasting + statistical anomaly detection\nLow Priority (Future Research): - [ ] Foundation Model Fine-tuning - If cloud-trained foundation model emerges - [ ] LLM-Based Explainability - Use GPT-4/Claude for anomaly explanations"
  },
  {
    "objectID": "concepts/research/opentslm-foundation-model-evaluation.html#lessons-learned",
    "href": "concepts/research/opentslm-foundation-model-evaluation.html#lessons-learned",
    "title": "OpenTSLM Foundation Model Evaluation",
    "section": "",
    "text": "Pre-Evaluation Checklist: 1. ✅ Check for Pre-trained Weights - First question, not last 2. ✅ Verify Domain Match - Medical ≠ Cloud Infrastructure 3. ✅ Assess Task Alignment - Q&A ≠ Anomaly Detection 4. ✅ Estimate Training Cost - GPU hours, dataset size, time to validation\nRed Flags Identified: - 🚩 “Train from scratch” without pre-trained option - 🚩 All training examples from unrelated domain - 🚩 No mentions of your use case in documentation - 🚩 Base models require special access (Llama 3.2 gating)\nGreen Flags for Future Models: - ✅ Pre-trained weights on HuggingFace - ✅ Training data includes infrastructure/system metrics - ✅ Explicit anomaly detection capabilities - ✅ Active community with cloud use cases\n\n\n\nWhat Worked Well: - Using repository-manager agent for systematic analysis - Forking before deep evaluation (preserves exploration) - Checking for weights availability early - Documenting findings immediately\nProcess Improvements: - Consider creating “Model Evaluation Template” for future assessments - Build checklist of domain-fit questions - Maintain “Models Under Consideration” tracking document"
  },
  {
    "objectID": "concepts/research/opentslm-foundation-model-evaluation.html#references",
    "href": "concepts/research/opentslm-foundation-model-evaluation.html#references",
    "title": "OpenTSLM Foundation Model Evaluation",
    "section": "",
    "text": "GitHub Repository: https://github.com/StanfordBDHG/OpenTSLM\nProject Website: https://www.opentslm.com\nResearch Paper: https://doi.org/10.13140/RG.2.2.14827.60963\nOur Fork: https://github.com/nehalecky/OpenTSLM\n\n\n\n\n\nAmazon Chronos: https://github.com/amazon-science/chronos-forecasting\nGoogle TimesFM: https://github.com/google-research/timesfm\nHugging Face Time Series: https://huggingface.co/models?pipeline_tag=time-series-forecasting\n\n\n\n\n\nECG-QA Paper: https://arxiv.org/abs/2306.15681\nPTB-XL Dataset: https://www.nature.com/articles/s41597-020-0495-6\nSleepEDF: https://physionet.org/content/sleep-edfx/1.0.0/\n\n\n\n\n\nFinOps Foundation: https://www.finops.org/\nAWS CloudWatch Anomaly Detection: https://aws.amazon.com/cloudwatch/\nAzure Monitor Anomaly Detector: https://azure.microsoft.com/en-us/products/ai-services/ai-anomaly-detector\n\n\nDocument Status: Final Last Updated: October 2, 2025 Next Review: When new cloud-focused foundation models emerge"
  },
  {
    "objectID": "concepts/design/rebase-execution-plan.html",
    "href": "concepts/design/rebase-execution-plan.html",
    "title": "Rebase Execution Plan",
    "section": "",
    "text": "Why this approach: - 89 commits to squash - interactive rebase would be extremely tedious - Want full control over narrative and commit messages - Minimize conflict risk - Can reference original commits for guidance\n\n\n\n\n\n\nSHA\nDescription\nAction\n\n\n\n\nd0d1e1f\nProject vision\nKEEP INTACT\n\n\nfe20c8e\nResearch foundation\nKEEP INTACT\n\n\nf5709bb\nArchitecture\nKEEP INTACT\n\n\n91cf3e3\nCore engine\nKEEP INTACT\n\n\n3ff7b8f\nWorkload signatures guide\nReference for new commit\n\n\n97342c0\nEDA notebooks added\nReference for new commit\n\n\na6e6648\nIOPS EDA initial\nReference for new commit\n\n\n3ba2011\nGP modeling notebook\nReference for new commit\n\n\n15c1a67\nGP library extraction\nCHERRY-PICK\n\n\n3c30138\nGP runbook conversion\nCHERRY-PICK\n\n\n794d945\nCloudZero PiedPiper EDA\nReference for new commit\n\n\na4aa885\nCleanup (just committed)\nKEEP\n\n\n\n\n\n\n\n\nCommits d0d1e1f → 91cf3e3 are good. Keep as-is.\n\n\n\ngit reset --soft 91cf3e3\nThis keeps all changes since 91cf3e3 as staged.\n\n\n\ngit reset HEAD\nNow all changes are unstaged. We can selectively build history.\n\n\n\nCommit 5: ML Foundation\ngit add src/cloud_sim/ml_models/ pyproject.toml\ngit add src/cloud_sim/data_generation/\ngit add tests/\ngit commit -m \"feat: add ML models and workload taxonomy foundation\n\nEstablish machine learning capabilities:\n- PyMC hierarchical Bayesian model for cloud resources\n- Workload taxonomy with 12+ application archetypes\n- HuggingFace dataset builder integration\n- Pydantic-based data validation\n\nAdd development infrastructure:\n- GitHub Actions CI with uv\n- Comprehensive test suite (70%+ coverage)\n- Black + Ruff code quality tooling\n\nBased on empirical research showing 12-15% CPU utilization and\n25-35% waste in cloud infrastructure.\n\"\nCommit 6: Workload Signatures Guide\ngit add notebooks/02_workload_signatures_guide.md\ngit add docs/research/cloud-resource-correlations-report.md\ngit commit -m \"docs: add comprehensive workload signatures guide\n\nEducational notebook demonstrating 12 empirical workload patterns:\n- Web applications, batch processing, ML training\n- Temporal autocorrelation (0.7-0.8)\n- Multi-variate correlation structures\n- Altair visualizations for pattern analysis\n\nBased on peer-reviewed research on cloud waste patterns.\n\"\nCommit 7: Dataset Exploration\ngit add notebooks/01_data_exploration.md\ngit add src/cloud_sim/etl/\ngit commit -m \"feat: explore public time series datasets for cloud workload analysis\n\nEvaluate multiple public datasets for cloud pattern research:\n- Alibaba Cluster Trace 2018 (explored, not retained)\n- Google Cluster Traces (explored, not retained)\n- IOPS webserver dataset (TSB-UAD) ← Selected for analysis\n\nAdd ETL infrastructure for HuggingFace dataset integration.\nCloudZero production data loader (stub for future use).\n\"\nCommit 8: IOPS Webserver EDA\ngit add notebooks/03_iops_web_server_eda.md\ngit commit -m \"feat: add IOPS webserver anomaly detection EDA\n\nComprehensive exploratory data analysis of IOPS dataset from TSB-UAD:\n- Statistical characterization (distributions, stationarity)\n- Seasonality and periodicity analysis (ACF, FFT, STL)\n- Univariate distribution analysis (PDF, CDF)\n- GP kernel selection methodology\n\nKey findings:\n- Clear periodic patterns detected (useful for GP modeling)\n- Anomalies show distinct distributional characteristics\n- Dataset suitable for time series forecasting experiments\n\nSource: AutonLab/Timeseries-PILE on HuggingFace\n\"\nCommit 9: GP Modeling Notebook\ngit add notebooks/04_gaussian_process_modeling.md\ngit commit -m \"feat: add Gaussian Process modeling notebook for cloud time series\n\nDevelop GP-based approach for cloud workload forecasting:\n- Composite periodic kernel (multi-scale patterns)\n- Student-t likelihood for robustness to outliers\n- Sparse variational GP for scalability\n- Comprehensive evaluation (RMSE, calibration, coverage)\n\nComparison: Robust (Student-t) vs Traditional (Gaussian) likelihood\n- Robust model maintains calibration in presence of anomalies\n- Traditional model overconfident on outliers\n\nBased on IOPS dataset analysis - demonstrates methodology for\ncloud workload time series.\n\"\nCommit 10: Library Extraction (Cherry-pick)\ngit cherry-pick 15c1a67\n# If conflicts, resolve and continue\nCommit 11: Runbook Conversion (Cherry-pick)\ngit cherry-pick 3c30138\n# If conflicts, resolve and continue\nCommit 12: Foundation Model Stubs\ngit add src/cloud_sim/ml_models/foundation/\ngit add pyproject.toml  # Optional dependencies\ngit commit -m \"feat: add foundation model stubs for future integration\n\nAdd stub implementations for time series foundation models:\n- TimesFM (Google Research) - Decoder-only transformer\n- Chronos (Amazon) - Probabilistic forecasting\n\nNot yet implemented - placeholders for future work.\n\nAlso add optional dependency groups in pyproject.toml:\n- [gpu] for GPyTorch training\n- [foundation] for foundation model integration\n\"\nCommit 13: Repository Cleanup\ngit add .gitignore docs/\ngit commit -m \"chore: comprehensive repository cleanup and documentation\n\n- Enhance .gitignore for notebook artifacts\n- Reorganize documentation structure\n- Remove redundant test files\n- Update README with current architecture\n- Add notebook workflow documentation\n\"\nCommit 14: CloudZero PiedPiper EDA\ngit add notebooks/05_cloudzero_piedpiper_eda.md\ngit add src/cloud_sim/utils/eda_analysis.py\ngit add docs/eda-workflow-summary.md\ngit commit -m \"feat: add CloudZero PiedPiper EDA with rigorous analysis framework\n\nComprehensive EDA notebook for CloudZero production data:\n- Reusable EDA analysis utilities module\n- Statistical foundations for cloud cost patterns\n- Integration with workflow automation\n- Hot reload pattern for iterative development\n\nDemonstrates library-first architecture: notebooks import\ncloud_sim utilities rather than embedding implementation.\n\"\nCommit 15: Final Cleanup (Already exists) This is commit a4aa885 we just made.\n\n\n\ngit stash pop\n\n\n\n\nIf anything goes wrong:\ngit reset --hard backup/pre-cleanup-rebase-20251008-1905\ngit stash pop  # Restore WIP changes\n\n\n\n\n15 commits total (down from 95)\nAll commits build successfully\nTests pass at HEAD\nClean narrative visible in git log --oneline\nFoundation commits (first 4) unchanged\nWIP changes restored after rebase"
  },
  {
    "objectID": "concepts/design/rebase-execution-plan.html#approach-soft-reset-manual-recommit",
    "href": "concepts/design/rebase-execution-plan.html#approach-soft-reset-manual-recommit",
    "title": "Rebase Execution Plan",
    "section": "",
    "text": "Why this approach: - 89 commits to squash - interactive rebase would be extremely tedious - Want full control over narrative and commit messages - Minimize conflict risk - Can reference original commits for guidance"
  },
  {
    "objectID": "concepts/design/rebase-execution-plan.html#key-milestone-commits-to-preserve-as-reference",
    "href": "concepts/design/rebase-execution-plan.html#key-milestone-commits-to-preserve-as-reference",
    "title": "Rebase Execution Plan",
    "section": "",
    "text": "SHA\nDescription\nAction\n\n\n\n\nd0d1e1f\nProject vision\nKEEP INTACT\n\n\nfe20c8e\nResearch foundation\nKEEP INTACT\n\n\nf5709bb\nArchitecture\nKEEP INTACT\n\n\n91cf3e3\nCore engine\nKEEP INTACT\n\n\n3ff7b8f\nWorkload signatures guide\nReference for new commit\n\n\n97342c0\nEDA notebooks added\nReference for new commit\n\n\na6e6648\nIOPS EDA initial\nReference for new commit\n\n\n3ba2011\nGP modeling notebook\nReference for new commit\n\n\n15c1a67\nGP library extraction\nCHERRY-PICK\n\n\n3c30138\nGP runbook conversion\nCHERRY-PICK\n\n\n794d945\nCloudZero PiedPiper EDA\nReference for new commit\n\n\na4aa885\nCleanup (just committed)\nKEEP"
  },
  {
    "objectID": "concepts/design/rebase-execution-plan.html#execution-steps",
    "href": "concepts/design/rebase-execution-plan.html#execution-steps",
    "title": "Rebase Execution Plan",
    "section": "",
    "text": "Commits d0d1e1f → 91cf3e3 are good. Keep as-is.\n\n\n\ngit reset --soft 91cf3e3\nThis keeps all changes since 91cf3e3 as staged.\n\n\n\ngit reset HEAD\nNow all changes are unstaged. We can selectively build history.\n\n\n\nCommit 5: ML Foundation\ngit add src/cloud_sim/ml_models/ pyproject.toml\ngit add src/cloud_sim/data_generation/\ngit add tests/\ngit commit -m \"feat: add ML models and workload taxonomy foundation\n\nEstablish machine learning capabilities:\n- PyMC hierarchical Bayesian model for cloud resources\n- Workload taxonomy with 12+ application archetypes\n- HuggingFace dataset builder integration\n- Pydantic-based data validation\n\nAdd development infrastructure:\n- GitHub Actions CI with uv\n- Comprehensive test suite (70%+ coverage)\n- Black + Ruff code quality tooling\n\nBased on empirical research showing 12-15% CPU utilization and\n25-35% waste in cloud infrastructure.\n\"\nCommit 6: Workload Signatures Guide\ngit add notebooks/02_workload_signatures_guide.md\ngit add docs/research/cloud-resource-correlations-report.md\ngit commit -m \"docs: add comprehensive workload signatures guide\n\nEducational notebook demonstrating 12 empirical workload patterns:\n- Web applications, batch processing, ML training\n- Temporal autocorrelation (0.7-0.8)\n- Multi-variate correlation structures\n- Altair visualizations for pattern analysis\n\nBased on peer-reviewed research on cloud waste patterns.\n\"\nCommit 7: Dataset Exploration\ngit add notebooks/01_data_exploration.md\ngit add src/cloud_sim/etl/\ngit commit -m \"feat: explore public time series datasets for cloud workload analysis\n\nEvaluate multiple public datasets for cloud pattern research:\n- Alibaba Cluster Trace 2018 (explored, not retained)\n- Google Cluster Traces (explored, not retained)\n- IOPS webserver dataset (TSB-UAD) ← Selected for analysis\n\nAdd ETL infrastructure for HuggingFace dataset integration.\nCloudZero production data loader (stub for future use).\n\"\nCommit 8: IOPS Webserver EDA\ngit add notebooks/03_iops_web_server_eda.md\ngit commit -m \"feat: add IOPS webserver anomaly detection EDA\n\nComprehensive exploratory data analysis of IOPS dataset from TSB-UAD:\n- Statistical characterization (distributions, stationarity)\n- Seasonality and periodicity analysis (ACF, FFT, STL)\n- Univariate distribution analysis (PDF, CDF)\n- GP kernel selection methodology\n\nKey findings:\n- Clear periodic patterns detected (useful for GP modeling)\n- Anomalies show distinct distributional characteristics\n- Dataset suitable for time series forecasting experiments\n\nSource: AutonLab/Timeseries-PILE on HuggingFace\n\"\nCommit 9: GP Modeling Notebook\ngit add notebooks/04_gaussian_process_modeling.md\ngit commit -m \"feat: add Gaussian Process modeling notebook for cloud time series\n\nDevelop GP-based approach for cloud workload forecasting:\n- Composite periodic kernel (multi-scale patterns)\n- Student-t likelihood for robustness to outliers\n- Sparse variational GP for scalability\n- Comprehensive evaluation (RMSE, calibration, coverage)\n\nComparison: Robust (Student-t) vs Traditional (Gaussian) likelihood\n- Robust model maintains calibration in presence of anomalies\n- Traditional model overconfident on outliers\n\nBased on IOPS dataset analysis - demonstrates methodology for\ncloud workload time series.\n\"\nCommit 10: Library Extraction (Cherry-pick)\ngit cherry-pick 15c1a67\n# If conflicts, resolve and continue\nCommit 11: Runbook Conversion (Cherry-pick)\ngit cherry-pick 3c30138\n# If conflicts, resolve and continue\nCommit 12: Foundation Model Stubs\ngit add src/cloud_sim/ml_models/foundation/\ngit add pyproject.toml  # Optional dependencies\ngit commit -m \"feat: add foundation model stubs for future integration\n\nAdd stub implementations for time series foundation models:\n- TimesFM (Google Research) - Decoder-only transformer\n- Chronos (Amazon) - Probabilistic forecasting\n\nNot yet implemented - placeholders for future work.\n\nAlso add optional dependency groups in pyproject.toml:\n- [gpu] for GPyTorch training\n- [foundation] for foundation model integration\n\"\nCommit 13: Repository Cleanup\ngit add .gitignore docs/\ngit commit -m \"chore: comprehensive repository cleanup and documentation\n\n- Enhance .gitignore for notebook artifacts\n- Reorganize documentation structure\n- Remove redundant test files\n- Update README with current architecture\n- Add notebook workflow documentation\n\"\nCommit 14: CloudZero PiedPiper EDA\ngit add notebooks/05_cloudzero_piedpiper_eda.md\ngit add src/cloud_sim/utils/eda_analysis.py\ngit add docs/eda-workflow-summary.md\ngit commit -m \"feat: add CloudZero PiedPiper EDA with rigorous analysis framework\n\nComprehensive EDA notebook for CloudZero production data:\n- Reusable EDA analysis utilities module\n- Statistical foundations for cloud cost patterns\n- Integration with workflow automation\n- Hot reload pattern for iterative development\n\nDemonstrates library-first architecture: notebooks import\ncloud_sim utilities rather than embedding implementation.\n\"\nCommit 15: Final Cleanup (Already exists) This is commit a4aa885 we just made.\n\n\n\ngit stash pop"
  },
  {
    "objectID": "concepts/design/rebase-execution-plan.html#rollback-plan",
    "href": "concepts/design/rebase-execution-plan.html#rollback-plan",
    "title": "Rebase Execution Plan",
    "section": "",
    "text": "If anything goes wrong:\ngit reset --hard backup/pre-cleanup-rebase-20251008-1905\ngit stash pop  # Restore WIP changes"
  },
  {
    "objectID": "concepts/design/rebase-execution-plan.html#success-criteria",
    "href": "concepts/design/rebase-execution-plan.html#success-criteria",
    "title": "Rebase Execution Plan",
    "section": "",
    "text": "15 commits total (down from 95)\nAll commits build successfully\nTests pass at HEAD\nClean narrative visible in git log --oneline\nFoundation commits (first 4) unchanged\nWIP changes restored after rebase"
  },
  {
    "objectID": "concepts/design/history-cleanup-strategy.html",
    "href": "concepts/design/history-cleanup-strategy.html",
    "title": "Repository History Cleanup Strategy",
    "section": "",
    "text": "Current State: 94 commits with significant concept churn, abandoned dataset explorations, and temporary diagnostic artifacts.\nDesired State: Clean, linear history telling the story of a focused research project exploring cloud workload characterization, culminating in a library-first GP modeling implementation.\nKey Principle: Preserve genuine intellectual progression while eliminating false starts and implementation churn.\n\n\n\n\n\n\nRoot directory diagnostics:\nrm diagnose_gp_results.py\nrm gp_diagnostics.txt gp_diagnostics_plots.png\nrm subsampling_aliasing_analysis.txt subsampling_psd_analysis.png\nrm subsampling_validation.txt subsampling_visual_validation.png\nUntracked documentation:\n# Keep: docs/eda-workflow-summary.md (currently in active use)\nrm docs/logging-strategy.md\nrm docs/modeling/gp-inducing-points-analysis.md\nrm docs/modeling/gp-initialization-fix.md\n\n\n\nFiles to delete:\nrm docs/research/alibaba-trace-analysis.md\nFiles to edit: - tests/test_notebooks.py: Remove \"alibaba_trace_analysis.md\" from skip list (line ~35)\nFiles to KEEP (legitimate references): - src/cloud_sim/ml_models/foundation/timesfm.py - Google’s TimesFM model - src/cloud_sim/ml_models/foundation/__init__.py - TimesFM documentation - src/cloud_sim/ml_models/application_taxonomy.py - “Google” in example_companies list\n\n\n\ngit add -A\ngit commit -m \"chore: remove diagnostic artifacts and abandoned dataset documentation\n\nRemove temporary diagnostic files from GP experimentation:\n- diagnose_gp_results.py\n- gp_diagnostics.txt, gp_diagnostics_plots.png\n- subsampling analysis artifacts\n\nRemove untracked documentation from exploratory phases:\n- logging-strategy.md\n- gp modeling iteration docs (inducing points, initialization fixes)\n\nKeep active documentation:\n- eda-workflow-summary.md (currently in use)\n\nRemove Alibaba trace dataset documentation (dataset exploration abandoned):\n- docs/research/alibaba-trace-analysis.md\n- Test reference in test_notebooks.py\n\nRetain legitimate Google references (TimesFM foundation model).\n\"\n\n\n\n\n\n\n\nFoundation (Keep as-is): - d0d1e1f - feat: establish project vision (initial commit) - fe20c8e - docs: establish empirical research foundation - f5709bb - feat: define simulation architecture - 91cf3e3 - feat: implement core simulation engine\nMiddle Churn (Squash heavily): - Commits f59f45d through 97342c0 (30+ commits): - Initial ML models - Workload taxonomy iterations - Alibaba/Google dataset explorations (abandoned) - Multiple documentation rewrites - Test infrastructure setup\nRecent Mature Work (Preserve structure, squash fixes): - 97342c0 - docs: add comprehensive EDA notebooks - a6e6648 - feat: add web server anomaly detection EDA (IOPS) - 3ba2011 - feat: add Gaussian Process modeling notebook - 04c055f - refactor: remove Alibaba/Google datasets - 15c1a67 - refactor: extract GP implementation to library (92% coverage) - 3c30138 - refactor: convert GP notebook to runbook - Recent commits - CloudZero EDA work\n\n\n\nFOUNDATION PHASE (Keep original commits):\n1. d0d1e1f - feat: establish project vision for cloud cost optimization\n2. fe20c8e - docs: establish empirical research foundation\n3. f5709bb - feat: define simulation architecture and modeling framework\n4. 91cf3e3 - feat: implement core cloud resource simulation engine\n\nRESEARCH SPIKE PHASE (Squash into 3-4 commits):\n5. NEW - feat: add workload pattern taxonomy and ML modeling foundation\n   [Squash: f59f45d + rebranding + Pydantic migration + CI setup]\n\n6. NEW - docs: add workload signatures guide with empirical patterns\n   [Squash: 3ff7b8f + correlation reports + documentation improvements]\n\n7. NEW - feat: establish notebook infrastructure for research workflows\n   [Squash: notebook setup + MyST conversion + test infrastructure]\n\nDATASET EXPLORATION PHASE (Squash into 2 commits):\n8. NEW - feat: explore public time series datasets for cloud workload analysis\n   [Squash: Initial dataset explorations, HF integration, preliminary analysis]\n   [Narrative: \"Evaluated multiple datasets (Alibaba, Google traces, IOPS) for cloud workload patterns\"]\n\n9. NEW - feat: add IOPS web server EDA - anomaly detection analysis\n   [Squash: a6e6648 + refinements + print statement removal + citation fixes]\n   [Keep: Final IOPS notebook as canonical example]\n\nGAUSSIAN PROCESS PHASE (Squash into 3-4 commits):\n10. NEW - feat: add Gaussian Process modeling notebook for cloud time series\n    [Squash: 3ba2011 + GP fixes + array handling + defensive programming]\n\n11. NEW - refactor: extract GP implementation to library module (92% test coverage)\n    [Keep: 15c1a67 - this is a major milestone]\n\n12. NEW - refactor: convert GP notebook to runbook using library\n    [Keep: 3c30138 - demonstrates library-first architecture]\n\n13. NEW - feat: add foundation model stubs (Chronos, TimesFM) and CloudZero ETL\n    [Squash: b216f58 + related dependency updates]\n\nPOLISH PHASE (Squash into 2-3 commits):\n14. NEW - chore: comprehensive repository cleanup and documentation\n    [Squash: Multiple cleanup commits + .gitignore updates + redundancy removal]\n\n15. NEW - feat: add CloudZero PiedPiper EDA with rigorous analysis framework\n    [Squash: 794d945 + eda utilities + workflow updates]\n\n16. NEW - chore: remove diagnostic artifacts and abandoned dataset documentation\n    [This is the cleanup we're doing now]\n\n\n\nThe Story We Tell: &gt; “In September 2025, I started this project to tackle cloud waste optimization. After establishing the architecture and core simulation engine, I spiked into multiple public datasets (Alibaba traces, Google traces, IOPS anomaly detection data) to understand real-world cloud workload patterns. The IOPS webserver dataset proved most valuable for anomaly detection analysis. &gt; &gt; I then developed a Gaussian Process modeling approach for cloud time series, iterated on the implementation in a notebook, and ultimately extracted it into a production-ready library with 92% test coverage. This library-first approach enables both research (notebooks) and production use (importable modules). &gt; &gt; Most recently, I’ve been applying this framework to CloudZero’s PiedPiper production data, creating rigorous EDA workflows.”\n\n\n\n\n\n\n\n# Create backup branch\ngit branch backup/pre-cleanup-rebase-$(date +%Y%m%d)\n\n# Create rebase script\ncat &gt; .git/rebase-plan.txt &lt;&lt; 'EOF'\n# Keep foundation commits as-is\npick d0d1e1f feat: establish project vision\npick fe20c8e docs: establish empirical research foundation\npick f5709bb feat: define simulation architecture\npick 91cf3e3 feat: implement core simulation engine\n\n# Squash ML foundation commits\npick f59f45d feat: add ML models and forecasting capabilities\nsquash 6b881af refactor: rebrand as Cloud Resource Simulator\nsquash de8a404 docs: reorganize documentation structure\nsquash 5b92385 refactor: migrate to Pydantic BaseModel\nsquash 40af580 fix: correct TOML parsing errors\nsquash b662202 ci: add GitHub Actions workflow\n# ... (continue for all related commits)\n\n# New commit: Workload signatures guide\npick 3ff7b8f feat: add workload signatures guide\nsquash 8c40494 docs: update correlation report\nsquash 8e080f5 chore: add coverage.xml to .gitignore\n\n# Continue pattern for each phase...\nEOF\n\n\n\nOption A: Interactive Rebase (Recommended)\ngit rebase -i d0d1e1f~1\n# Edit rebase-todo list following the plan above\n# Resolve conflicts as needed\n# Reword commit messages to match target narrative\nOption B: Soft Reset + Recommit (Nuclear Option)\ngit checkout -b clean-history d0d1e1f\n# Manually cherry-pick foundation commits\ngit cherry-pick fe20c8e f5709bb 91cf3e3\n\n# Create new squashed commits manually\n# (More control, but more work)\nRecommendation: Start with Option A. If rebase becomes too complex, fall back to Option B for problematic sections.\n\n\n\n&lt;type&gt;(&lt;scope&gt;): &lt;concise description&gt;\n\nNarrative: &lt;Why this work was done&gt;\n\nKey Changes:\n- &lt;Bullet point 1&gt;\n- &lt;Bullet point 2&gt;\n\nResearch Context: &lt;Any empirical findings or decisions&gt;\n\nCoverage: &lt;If applicable, test coverage %&gt;\n\n🤖 Generated with Claude Code (https://claude.ai/code)\n\nCo-Authored-By: Claude &lt;noreply@anthropic.com&gt;\n\n\n\n\n\n\n\n# Verify all commits build\ngit rebase --exec \"uv sync\" &lt;target-branch&gt;\n\n# Verify tests pass at each commit\ngit rebase --exec \"uv run pytest tests/ -v\" &lt;target-branch&gt;\n\n# Verify notebooks are valid\ngit rebase --exec \"uv run pytest tests/test_notebooks.py -m smoke\" &lt;target-branch&gt;\n\n# Check commit count\ngit log --oneline | wc -l  # Should be ~15-20\n\n# Verify no broken references\nrg \"alibaba_trace_analysis\" --type py\nrg \"GoogleTraceLoader\" --type py\n\n\n\nAfter rebase, update: - .claude-project-memory.md - Remove references to abandoned explorations - docs/design/library-first-refactoring-plan.md - Mark Phase 3 complete - README.md - Ensure history aligns with cleaned narrative - CHANGELOG.md - Create clean changelog from new history\n\n\n\n\n\n\n\nIf rebase fails:\ngit rebase --abort\ngit reset --hard backup/pre-cleanup-rebase-YYYYMMDD\nIf rebase succeeds but breaks something:\ngit reflog  # Find pre-rebase HEAD\ngit reset --hard HEAD@{N}  # Where N is pre-rebase state\n\n\n\n# Run full test suite\nuv run pytest tests/ -v --cov=src/cloud_sim --cov-report=term-missing\n\n# Test notebook execution\nuv run pytest tests/test_notebooks.py -v\n\n# Verify build\nuv build\n\n# Check for missing files\ngit ls-files | wc -l\n\n\n\nDO NOT force push to main immediately!\n# Push to temporary branch first\ngit push origin HEAD:temp/clean-history-verification\n\n# Test in GitHub UI\n# - Browse files\n# - Check Actions run\n# - Review commit history\n\n# If satisfied, force push to main\ngit push --force-with-lease origin main\n\n\n\n\n\n\n\n\n✅ Review this strategy document\n✅ Get user approval on narrative and approach\n✅ Create backup branch\n✅ Run full test suite (baseline)\n✅ Execute Phase 1 cleanup (remove artifacts)\n\n\n\n\n\n⏳ Start interactive rebase from d0d1e1f\n⏳ Squash commits following target structure\n⏳ Reword commit messages with narrative\n⏳ Resolve any conflicts\n\n\n\n\n\n⏳ Run verification checklist\n⏳ Push to temporary branch\n⏳ Review in GitHub UI\n⏳ Get final approval\n\n\n\n\n\n⏳ Force push to main with lease\n⏳ Update documentation\n⏳ Delete backup branch (after confirmation)\n⏳ Create summary audio (work-completion-summary)\n\n\n\n\n\n\n\n\nA: No. The code was removed in commit 04c055f, and these were false starts. The narrative should mention “evaluated multiple datasets” in a single squashed commit, but not preserve the full iteration history.\n\n\n\nA: Keep 3 commits: 1. Initial GP notebook implementation 2. Library extraction (92% coverage) - MILESTONE 3. Notebook conversion to runbook\nSquash all the fixes, array handling, defensive programming iterations.\n\n\n\nA: Squash into “Establish notebook infrastructure” commit. The final state is what matters, not the iteration history.\n\n\n\nA: Keep as recent commits (794d945, 4f4d57e, etc). This is current work and shouldn’t be squashed yet. May refine later when sharing externally.\n\n\n\nA: No. filter-branch is for removing sensitive data or large files. Interactive rebase gives us fine-grained control over commit messages and narrative.\n\n\n\n\n\n✅ Commit Count: 15-20 commits (down from 94) ✅ All Tests Pass: Every commit buildable and testable ✅ Clean Narrative: History tells coherent research story ✅ No Broken References: Removed datasets don’t appear in code/tests ✅ Documentation Aligned: Docs reflect cleaned history ✅ Shareable: Repository ready for public/external sharing\n\n\n\n\n\nOriginal Plan: docs/design/library-first-refactoring-plan.md\nGit Rebase Documentation: https://git-scm.com/docs/git-rebase\nConventional Commits: https://www.conventionalcommits.org/\nPro Git Chapter 7.6: Rewriting History: https://git-scm.com/book/en/v2/Git-Tools-Rewriting-History\n\n\n\n\n\n\n\n\n\n\n\n\n\nOld Commits\nNew Commit\nRationale\n\n\n\n\nd0d1e1f\nKeep as-is\nFoundation commit\n\n\nfe20c8e\nKeep as-is\nResearch foundation\n\n\nf5709bb\nKeep as-is\nArchitecture definition\n\n\n91cf3e3\nKeep as-is\nCore engine\n\n\nf59f45d + 6b881af + de8a404 + 5b92385 + 40af580 + b662202\nSquash into “feat: add workload taxonomy and ML foundation”\nMultiple iterations of same work\n\n\n3ff7b8f + 8c40494 + b8965b2 + 76b5aa3\nSquash into “docs: add workload signatures guide”\nDocumentation and refinement\n\n\n8c203bb + 69a7c64 + d557b52 + (Google commits)\nSquash into “feat: explore datasets”\nDataset exploration spike\n\n\na6e6648 + d423b24 + fixes\nSquash into “feat: IOPS EDA analysis”\nIOPS notebook work\n\n\n3ba2011 + 6bf9581 + d20b046 + 00d56c3\nSquash into “feat: GP modeling notebook”\nGP notebook iterations\n\n\n15c1a67\nKeep as-is\nMILESTONE: Library extraction\n\n\n3c30138\nKeep as-is\nRunbook conversion\n\n\n04c055f + 78320f2 + 41a1f3b + 7374d24\nSquash into “chore: repository cleanup”\nCleanup iterations\n\n\n794d945 + 4f4d57e + 3d59a6a + 9e8c7b3\nKeep structure, maybe minor squash\nRecent CloudZero work\n\n\n\n\nEnd of Strategy Document"
  },
  {
    "objectID": "concepts/design/history-cleanup-strategy.html#executive-summary",
    "href": "concepts/design/history-cleanup-strategy.html#executive-summary",
    "title": "Repository History Cleanup Strategy",
    "section": "",
    "text": "Current State: 94 commits with significant concept churn, abandoned dataset explorations, and temporary diagnostic artifacts.\nDesired State: Clean, linear history telling the story of a focused research project exploring cloud workload characterization, culminating in a library-first GP modeling implementation.\nKey Principle: Preserve genuine intellectual progression while eliminating false starts and implementation churn."
  },
  {
    "objectID": "concepts/design/history-cleanup-strategy.html#phase-1-pre-rebase-cleanup-file-code-level",
    "href": "concepts/design/history-cleanup-strategy.html#phase-1-pre-rebase-cleanup-file-code-level",
    "title": "Repository History Cleanup Strategy",
    "section": "",
    "text": "Root directory diagnostics:\nrm diagnose_gp_results.py\nrm gp_diagnostics.txt gp_diagnostics_plots.png\nrm subsampling_aliasing_analysis.txt subsampling_psd_analysis.png\nrm subsampling_validation.txt subsampling_visual_validation.png\nUntracked documentation:\n# Keep: docs/eda-workflow-summary.md (currently in active use)\nrm docs/logging-strategy.md\nrm docs/modeling/gp-inducing-points-analysis.md\nrm docs/modeling/gp-initialization-fix.md\n\n\n\nFiles to delete:\nrm docs/research/alibaba-trace-analysis.md\nFiles to edit: - tests/test_notebooks.py: Remove \"alibaba_trace_analysis.md\" from skip list (line ~35)\nFiles to KEEP (legitimate references): - src/cloud_sim/ml_models/foundation/timesfm.py - Google’s TimesFM model - src/cloud_sim/ml_models/foundation/__init__.py - TimesFM documentation - src/cloud_sim/ml_models/application_taxonomy.py - “Google” in example_companies list\n\n\n\ngit add -A\ngit commit -m \"chore: remove diagnostic artifacts and abandoned dataset documentation\n\nRemove temporary diagnostic files from GP experimentation:\n- diagnose_gp_results.py\n- gp_diagnostics.txt, gp_diagnostics_plots.png\n- subsampling analysis artifacts\n\nRemove untracked documentation from exploratory phases:\n- logging-strategy.md\n- gp modeling iteration docs (inducing points, initialization fixes)\n\nKeep active documentation:\n- eda-workflow-summary.md (currently in use)\n\nRemove Alibaba trace dataset documentation (dataset exploration abandoned):\n- docs/research/alibaba-trace-analysis.md\n- Test reference in test_notebooks.py\n\nRetain legitimate Google references (TimesFM foundation model).\n\""
  },
  {
    "objectID": "concepts/design/history-cleanup-strategy.html#phase-2-design-clean-history-narrative",
    "href": "concepts/design/history-cleanup-strategy.html#phase-2-design-clean-history-narrative",
    "title": "Repository History Cleanup Strategy",
    "section": "",
    "text": "Foundation (Keep as-is): - d0d1e1f - feat: establish project vision (initial commit) - fe20c8e - docs: establish empirical research foundation - f5709bb - feat: define simulation architecture - 91cf3e3 - feat: implement core simulation engine\nMiddle Churn (Squash heavily): - Commits f59f45d through 97342c0 (30+ commits): - Initial ML models - Workload taxonomy iterations - Alibaba/Google dataset explorations (abandoned) - Multiple documentation rewrites - Test infrastructure setup\nRecent Mature Work (Preserve structure, squash fixes): - 97342c0 - docs: add comprehensive EDA notebooks - a6e6648 - feat: add web server anomaly detection EDA (IOPS) - 3ba2011 - feat: add Gaussian Process modeling notebook - 04c055f - refactor: remove Alibaba/Google datasets - 15c1a67 - refactor: extract GP implementation to library (92% coverage) - 3c30138 - refactor: convert GP notebook to runbook - Recent commits - CloudZero EDA work\n\n\n\nFOUNDATION PHASE (Keep original commits):\n1. d0d1e1f - feat: establish project vision for cloud cost optimization\n2. fe20c8e - docs: establish empirical research foundation\n3. f5709bb - feat: define simulation architecture and modeling framework\n4. 91cf3e3 - feat: implement core cloud resource simulation engine\n\nRESEARCH SPIKE PHASE (Squash into 3-4 commits):\n5. NEW - feat: add workload pattern taxonomy and ML modeling foundation\n   [Squash: f59f45d + rebranding + Pydantic migration + CI setup]\n\n6. NEW - docs: add workload signatures guide with empirical patterns\n   [Squash: 3ff7b8f + correlation reports + documentation improvements]\n\n7. NEW - feat: establish notebook infrastructure for research workflows\n   [Squash: notebook setup + MyST conversion + test infrastructure]\n\nDATASET EXPLORATION PHASE (Squash into 2 commits):\n8. NEW - feat: explore public time series datasets for cloud workload analysis\n   [Squash: Initial dataset explorations, HF integration, preliminary analysis]\n   [Narrative: \"Evaluated multiple datasets (Alibaba, Google traces, IOPS) for cloud workload patterns\"]\n\n9. NEW - feat: add IOPS web server EDA - anomaly detection analysis\n   [Squash: a6e6648 + refinements + print statement removal + citation fixes]\n   [Keep: Final IOPS notebook as canonical example]\n\nGAUSSIAN PROCESS PHASE (Squash into 3-4 commits):\n10. NEW - feat: add Gaussian Process modeling notebook for cloud time series\n    [Squash: 3ba2011 + GP fixes + array handling + defensive programming]\n\n11. NEW - refactor: extract GP implementation to library module (92% test coverage)\n    [Keep: 15c1a67 - this is a major milestone]\n\n12. NEW - refactor: convert GP notebook to runbook using library\n    [Keep: 3c30138 - demonstrates library-first architecture]\n\n13. NEW - feat: add foundation model stubs (Chronos, TimesFM) and CloudZero ETL\n    [Squash: b216f58 + related dependency updates]\n\nPOLISH PHASE (Squash into 2-3 commits):\n14. NEW - chore: comprehensive repository cleanup and documentation\n    [Squash: Multiple cleanup commits + .gitignore updates + redundancy removal]\n\n15. NEW - feat: add CloudZero PiedPiper EDA with rigorous analysis framework\n    [Squash: 794d945 + eda utilities + workflow updates]\n\n16. NEW - chore: remove diagnostic artifacts and abandoned dataset documentation\n    [This is the cleanup we're doing now]\n\n\n\nThe Story We Tell: &gt; “In September 2025, I started this project to tackle cloud waste optimization. After establishing the architecture and core simulation engine, I spiked into multiple public datasets (Alibaba traces, Google traces, IOPS anomaly detection data) to understand real-world cloud workload patterns. The IOPS webserver dataset proved most valuable for anomaly detection analysis. &gt; &gt; I then developed a Gaussian Process modeling approach for cloud time series, iterated on the implementation in a notebook, and ultimately extracted it into a production-ready library with 92% test coverage. This library-first approach enables both research (notebooks) and production use (importable modules). &gt; &gt; Most recently, I’ve been applying this framework to CloudZero’s PiedPiper production data, creating rigorous EDA workflows.”"
  },
  {
    "objectID": "concepts/design/history-cleanup-strategy.html#phase-3-interactive-rebase-execution-plan",
    "href": "concepts/design/history-cleanup-strategy.html#phase-3-interactive-rebase-execution-plan",
    "title": "Repository History Cleanup Strategy",
    "section": "",
    "text": "# Create backup branch\ngit branch backup/pre-cleanup-rebase-$(date +%Y%m%d)\n\n# Create rebase script\ncat &gt; .git/rebase-plan.txt &lt;&lt; 'EOF'\n# Keep foundation commits as-is\npick d0d1e1f feat: establish project vision\npick fe20c8e docs: establish empirical research foundation\npick f5709bb feat: define simulation architecture\npick 91cf3e3 feat: implement core simulation engine\n\n# Squash ML foundation commits\npick f59f45d feat: add ML models and forecasting capabilities\nsquash 6b881af refactor: rebrand as Cloud Resource Simulator\nsquash de8a404 docs: reorganize documentation structure\nsquash 5b92385 refactor: migrate to Pydantic BaseModel\nsquash 40af580 fix: correct TOML parsing errors\nsquash b662202 ci: add GitHub Actions workflow\n# ... (continue for all related commits)\n\n# New commit: Workload signatures guide\npick 3ff7b8f feat: add workload signatures guide\nsquash 8c40494 docs: update correlation report\nsquash 8e080f5 chore: add coverage.xml to .gitignore\n\n# Continue pattern for each phase...\nEOF\n\n\n\nOption A: Interactive Rebase (Recommended)\ngit rebase -i d0d1e1f~1\n# Edit rebase-todo list following the plan above\n# Resolve conflicts as needed\n# Reword commit messages to match target narrative\nOption B: Soft Reset + Recommit (Nuclear Option)\ngit checkout -b clean-history d0d1e1f\n# Manually cherry-pick foundation commits\ngit cherry-pick fe20c8e f5709bb 91cf3e3\n\n# Create new squashed commits manually\n# (More control, but more work)\nRecommendation: Start with Option A. If rebase becomes too complex, fall back to Option B for problematic sections.\n\n\n\n&lt;type&gt;(&lt;scope&gt;): &lt;concise description&gt;\n\nNarrative: &lt;Why this work was done&gt;\n\nKey Changes:\n- &lt;Bullet point 1&gt;\n- &lt;Bullet point 2&gt;\n\nResearch Context: &lt;Any empirical findings or decisions&gt;\n\nCoverage: &lt;If applicable, test coverage %&gt;\n\n🤖 Generated with Claude Code (https://claude.ai/code)\n\nCo-Authored-By: Claude &lt;noreply@anthropic.com&gt;"
  },
  {
    "objectID": "concepts/design/history-cleanup-strategy.html#phase-4-post-rebase-verification",
    "href": "concepts/design/history-cleanup-strategy.html#phase-4-post-rebase-verification",
    "title": "Repository History Cleanup Strategy",
    "section": "",
    "text": "# Verify all commits build\ngit rebase --exec \"uv sync\" &lt;target-branch&gt;\n\n# Verify tests pass at each commit\ngit rebase --exec \"uv run pytest tests/ -v\" &lt;target-branch&gt;\n\n# Verify notebooks are valid\ngit rebase --exec \"uv run pytest tests/test_notebooks.py -m smoke\" &lt;target-branch&gt;\n\n# Check commit count\ngit log --oneline | wc -l  # Should be ~15-20\n\n# Verify no broken references\nrg \"alibaba_trace_analysis\" --type py\nrg \"GoogleTraceLoader\" --type py\n\n\n\nAfter rebase, update: - .claude-project-memory.md - Remove references to abandoned explorations - docs/design/library-first-refactoring-plan.md - Mark Phase 3 complete - README.md - Ensure history aligns with cleaned narrative - CHANGELOG.md - Create clean changelog from new history"
  },
  {
    "objectID": "concepts/design/history-cleanup-strategy.html#phase-5-risk-mitigation",
    "href": "concepts/design/history-cleanup-strategy.html#phase-5-risk-mitigation",
    "title": "Repository History Cleanup Strategy",
    "section": "",
    "text": "If rebase fails:\ngit rebase --abort\ngit reset --hard backup/pre-cleanup-rebase-YYYYMMDD\nIf rebase succeeds but breaks something:\ngit reflog  # Find pre-rebase HEAD\ngit reset --hard HEAD@{N}  # Where N is pre-rebase state\n\n\n\n# Run full test suite\nuv run pytest tests/ -v --cov=src/cloud_sim --cov-report=term-missing\n\n# Test notebook execution\nuv run pytest tests/test_notebooks.py -v\n\n# Verify build\nuv build\n\n# Check for missing files\ngit ls-files | wc -l\n\n\n\nDO NOT force push to main immediately!\n# Push to temporary branch first\ngit push origin HEAD:temp/clean-history-verification\n\n# Test in GitHub UI\n# - Browse files\n# - Check Actions run\n# - Review commit history\n\n# If satisfied, force push to main\ngit push --force-with-lease origin main"
  },
  {
    "objectID": "concepts/design/history-cleanup-strategy.html#phase-6-execution-order-step-by-step",
    "href": "concepts/design/history-cleanup-strategy.html#phase-6-execution-order-step-by-step",
    "title": "Repository History Cleanup Strategy",
    "section": "",
    "text": "✅ Review this strategy document\n✅ Get user approval on narrative and approach\n✅ Create backup branch\n✅ Run full test suite (baseline)\n✅ Execute Phase 1 cleanup (remove artifacts)\n\n\n\n\n\n⏳ Start interactive rebase from d0d1e1f\n⏳ Squash commits following target structure\n⏳ Reword commit messages with narrative\n⏳ Resolve any conflicts\n\n\n\n\n\n⏳ Run verification checklist\n⏳ Push to temporary branch\n⏳ Review in GitHub UI\n⏳ Get final approval\n\n\n\n\n\n⏳ Force push to main with lease\n⏳ Update documentation\n⏳ Delete backup branch (after confirmation)\n⏳ Create summary audio (work-completion-summary)"
  },
  {
    "objectID": "concepts/design/history-cleanup-strategy.html#decision-points",
    "href": "concepts/design/history-cleanup-strategy.html#decision-points",
    "title": "Repository History Cleanup Strategy",
    "section": "",
    "text": "A: No. The code was removed in commit 04c055f, and these were false starts. The narrative should mention “evaluated multiple datasets” in a single squashed commit, but not preserve the full iteration history.\n\n\n\nA: Keep 3 commits: 1. Initial GP notebook implementation 2. Library extraction (92% coverage) - MILESTONE 3. Notebook conversion to runbook\nSquash all the fixes, array handling, defensive programming iterations.\n\n\n\nA: Squash into “Establish notebook infrastructure” commit. The final state is what matters, not the iteration history.\n\n\n\nA: Keep as recent commits (794d945, 4f4d57e, etc). This is current work and shouldn’t be squashed yet. May refine later when sharing externally.\n\n\n\nA: No. filter-branch is for removing sensitive data or large files. Interactive rebase gives us fine-grained control over commit messages and narrative."
  },
  {
    "objectID": "concepts/design/history-cleanup-strategy.html#success-metrics",
    "href": "concepts/design/history-cleanup-strategy.html#success-metrics",
    "title": "Repository History Cleanup Strategy",
    "section": "",
    "text": "✅ Commit Count: 15-20 commits (down from 94) ✅ All Tests Pass: Every commit buildable and testable ✅ Clean Narrative: History tells coherent research story ✅ No Broken References: Removed datasets don’t appear in code/tests ✅ Documentation Aligned: Docs reflect cleaned history ✅ Shareable: Repository ready for public/external sharing"
  },
  {
    "objectID": "concepts/design/history-cleanup-strategy.html#references",
    "href": "concepts/design/history-cleanup-strategy.html#references",
    "title": "Repository History Cleanup Strategy",
    "section": "",
    "text": "Original Plan: docs/design/library-first-refactoring-plan.md\nGit Rebase Documentation: https://git-scm.com/docs/git-rebase\nConventional Commits: https://www.conventionalcommits.org/\nPro Git Chapter 7.6: Rewriting History: https://git-scm.com/book/en/v2/Git-Tools-Rewriting-History"
  },
  {
    "objectID": "concepts/design/history-cleanup-strategy.html#appendix-commit-mapping-table",
    "href": "concepts/design/history-cleanup-strategy.html#appendix-commit-mapping-table",
    "title": "Repository History Cleanup Strategy",
    "section": "",
    "text": "Old Commits\nNew Commit\nRationale\n\n\n\n\nd0d1e1f\nKeep as-is\nFoundation commit\n\n\nfe20c8e\nKeep as-is\nResearch foundation\n\n\nf5709bb\nKeep as-is\nArchitecture definition\n\n\n91cf3e3\nKeep as-is\nCore engine\n\n\nf59f45d + 6b881af + de8a404 + 5b92385 + 40af580 + b662202\nSquash into “feat: add workload taxonomy and ML foundation”\nMultiple iterations of same work\n\n\n3ff7b8f + 8c40494 + b8965b2 + 76b5aa3\nSquash into “docs: add workload signatures guide”\nDocumentation and refinement\n\n\n8c203bb + 69a7c64 + d557b52 + (Google commits)\nSquash into “feat: explore datasets”\nDataset exploration spike\n\n\na6e6648 + d423b24 + fixes\nSquash into “feat: IOPS EDA analysis”\nIOPS notebook work\n\n\n3ba2011 + 6bf9581 + d20b046 + 00d56c3\nSquash into “feat: GP modeling notebook”\nGP notebook iterations\n\n\n15c1a67\nKeep as-is\nMILESTONE: Library extraction\n\n\n3c30138\nKeep as-is\nRunbook conversion\n\n\n04c055f + 78320f2 + 41a1f3b + 7374d24\nSquash into “chore: repository cleanup”\nCleanup iterations\n\n\n794d945 + 4f4d57e + 3d59a6a + 9e8c7b3\nKeep structure, maybe minor squash\nRecent CloudZero work\n\n\n\n\nEnd of Strategy Document"
  },
  {
    "objectID": "tutorials/gaussian-processes.html",
    "href": "tutorials/gaussian-processes.html",
    "title": "Gaussian Process Modeling",
    "section": "",
    "text": "Learn to build production-ready Gaussian Process models for cloud resource forecasting and anomaly detection using GPyTorch’s sparse variational approximations.\n\n\n\nUnderstand sparse variational GPs for large datasets\nUse composite periodic kernels for multi-scale patterns\nTrain robust models with Student-t likelihood\nEvaluate both accuracy and calibration\nMake predictions with uncertainty quantification\n\n\n\n\n\n\n\nTipEstimated Time\n\n\n\n30 minutes\n\n\n\n\n\n\n\n\nNotePrerequisites\n\n\n\n\nBasic understanding of Gaussian Processes (see Rasmussen & Williams)\nFamiliarity with PyTorch\nCompleted Data Exploration Tutorial\n\nGPU recommended but not required - CPU works well for this tutorial.\n\n\n\n\n\n\n\nimport numpy as np\nimport polars as pl\nimport torch\nimport gpytorch\n\nfrom cloud_sim.ml_models.gaussian_process import (\n    CompositePeriodicKernel,\n    SparseGPModel,\n    train_gp_model,\n    compute_metrics,\n    compute_prediction_intervals\n)\n\n# Visualization\nimport altair as alt\nfrom datetime import datetime, timedelta\n\n# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Device selection\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n\n\n\n\n\n\nWe need to forecast cloud resource metrics (CPU, IOPS, memory) that exhibit:\n\nMulti-scale periodicity: Daily cycles + weekly patterns\nAnomalies: Operational issues, spikes, outages\nUncertainty: Need confidence intervals, not just point predictions\nScale: Thousands to millions of data points\n\nGaussian Processes are ideal because they provide: - ✅ Flexible modeling via kernel design - ✅ Principled uncertainty quantification - ✅ Non-parametric learning from data\n\n\n\n\n\nLet’s create a realistic IOPS (I/O operations per second) workload:\n\nfrom cloud_sim.data_generation import WorkloadPatternGenerator, WorkloadType\n\n# Generate 30 days of data\ngenerator = WorkloadPatternGenerator(seed=42)\ndf = generator.generate_time_series(\n    workload_type=WorkloadType.DATABASE_OLTP,\n    start_time=datetime.now() - timedelta(days=30),\n    end_time=datetime.now(),\n    interval_minutes=1,  # Minute-level granularity\n    include_anomalies=True,\n    anomaly_rate=0.005  # 0.5% anomaly rate\n)\n\n# Use disk IOPS as our target metric\nprint(f\"Dataset size: {len(df):,} samples\")\nprint(f\"Anomalies: {df['is_anomaly'].sum()} ({df['is_anomaly'].mean()*100:.2f}%)\")\n\n\n\n\n# Split: 70% train, 30% test\ntrain_size = int(0.7 * len(df))\n\ntrain_df = df[:train_size]\ntest_df = df[train_size:]\n\nprint(f\"Train: {len(train_df):,} samples\")\nprint(f\"Test: {len(test_df):,} samples\")\n\n\n\n\n\n\n\n\n\n# Plot the data\ntrain_plot = train_df.to_pandas()\n\nchart = alt.Chart(train_plot.iloc[::60]).mark_line(size=1).encode(  # Subsample for visualization\n    x=alt.X('timestamp:T', title='Time'),\n    y=alt.Y('disk_iops:Q', title='IOPS'),\n    color=alt.condition(\n        alt.datum.is_anomaly == True,\n        alt.value('red'),\n        alt.value('steelblue')\n    )\n).properties(\n    width=800,\n    height=300,\n    title='Database OLTP: IOPS Over Time (Training Data)'\n).interactive()\n\nchart\n\n\n\n\n\n\n\nNotePattern Observations\n\n\n\n\nDaily periodicity: Clear 24-hour cycles\nWeekly seasonality: Weekday vs. weekend differences\nAnomalies: Occasional spikes and dips (red points)\nNon-stationary: Mean shifts over time\n\n\n\n\n\n\n\n\n\n\nWe use a composite kernel to capture multi-scale patterns:\n\\[\nk(x, x') = k_{daily}(x, x') \\times k_{weekly}(x, x') + k_{noise}(x, x')\n\\]\nWhere: - Daily component: period_length=1440 (minutes per day) - Weekly component: period_length=10080 (minutes per week) - Noise component: Handles residual variance\n\n# Initialize kernel\nkernel = CompositePeriodicKernel(\n    periods=[1440.0, 10080.0],  # Daily + weekly\n    lengthscales=[360.0, 2520.0],  # Quarter-day, half-week\n    add_noise=True\n)\n\n\n\n\nFor large datasets, we use inducing points to approximate the full GP:\n\n# Prepare tensors\nX_train = torch.arange(len(train_df), dtype=torch.float32).reshape(-1, 1)\ny_train = torch.tensor(train_df['disk_iops'].to_numpy(), dtype=torch.float32)\n\n# Initialize model with 100 inducing points\nmodel = SparseGPModel(\n    num_inducing=100,\n    likelihood_type='student_t',  # Robust to outliers\n    kernel=kernel\n)\n\nprint(f\"Model: {model.__class__.__name__}\")\nprint(f\"Inducing points: 100\")\nprint(f\"Likelihood: Student-t (robust to anomalies)\")\n\n\n\n\n\n\n\nTipWhy Student-t Likelihood?\n\n\n\nThe Student-t distribution has heavier tails than Gaussian:\n\n✅ Robust to outliers and anomalies\n✅ Doesn’t over-fit to spikes\n✅ Better calibrated uncertainty\n✅ Production-ready for operational data\n\nTrain on all data including anomalies - the model learns to handle them!\n\n\n\n\n\n\n\n\n# Train the model\ntrained_model, losses = train_gp_model(\n    model=model,\n    train_x=X_train,\n    train_y=y_train,\n    num_epochs=100,\n    learning_rate=0.01,\n    verbose=True\n)\n\n# Plot training loss\nloss_df = pl.DataFrame({'epoch': range(len(losses)), 'loss': losses})\n\nchart = alt.Chart(loss_df.to_pandas()).mark_line().encode(\n    x='epoch:Q',\n    y=alt.Y('loss:Q', scale=alt.Scale(type='log')),\n).properties(\n    width=600,\n    height=300,\n    title='Training Loss (Log Scale)'\n)\n\nchart\n\nExpected behavior: - Loss decreases rapidly in first 20 epochs - Convergence around epoch 50-80 - Final loss stabilizes\n\n\n\n\n\n\n\n# Prepare test data\nX_test = torch.arange(len(train_df), len(df), dtype=torch.float32).reshape(-1, 1)\ny_test = torch.tensor(test_df['disk_iops'].to_numpy(), dtype=torch.float32)\n\n# Get predictions\ntrained_model.eval()\nwith torch.no_grad(), gpytorch.settings.fast_pred_var():\n    pred_dist = trained_model(X_test)\n\n    # Extract mean and variance\n    pred_mean = pred_dist.mean.numpy()\n    pred_var = pred_dist.variance.numpy()\n    pred_std = np.sqrt(pred_var)\n\n# Add to test dataframe\ntest_df = test_df.with_columns([\n    pl.Series('pred_mean', pred_mean),\n    pl.Series('pred_std', pred_std)\n])\n\n\n\n\n\n# Compute prediction intervals\nlower_95 = pred_mean - 1.96 * pred_std\nupper_95 = pred_mean + 1.96 * pred_std\n\n# Create visualization\ntest_plot = test_df.to_pandas().iloc[::60]  # Subsample\n\n# Build multi-layer chart\nbase = alt.Chart(test_plot).encode(x='timestamp:T')\n\n# Confidence band\nband = base.mark_area(opacity=0.2, color='steelblue').encode(\n    y='lower_95:Q',\n    y2='upper_95:Q'\n)\n\n# Predicted mean\npred_line = base.mark_line(color='steelblue').encode(\n    y='pred_mean:Q'\n)\n\n# Actual values\nactual_points = base.mark_circle(size=20).encode(\n    y='disk_iops:Q',\n    color=alt.condition(\n        alt.datum.is_anomaly == True,\n        alt.value('red'),\n        alt.value('darkblue')\n    )\n)\n\nchart = (band + pred_line + actual_points).properties(\n    width=800,\n    height=400,\n    title='GP Predictions with 95% Confidence Intervals'\n).interactive()\n\nchart\n\n\n\n\n\n\n\nNoteUncertainty Quantification\n\n\n\nNotice how:\n\nConfidence bands widen during anomalies\nNormal periods have tight, confident predictions\nModel uncertainty increases for out-of-distribution events\nMost actual values fall within the 95% interval\n\nThis is calibrated uncertainty - essential for production systems!\n\n\n\n\n\n\n\n\n\n\n# Compute comprehensive metrics\nmetrics = compute_metrics(\n    y_true=y_test.numpy(),\n    y_pred=pred_mean,\n    y_std=pred_std\n)\n\nprint(\"Accuracy Metrics:\")\nprint(f\"  RMSE: {metrics['rmse']:.3f}\")\nprint(f\"  MAE: {metrics['mae']:.3f}\")\nprint(f\"  R²: {metrics['r2']:.3f}\")\nprint(f\"  MAPE: {metrics['mape']:.2f}%\")\n\nprint(\"\\nUncertainty Calibration:\")\nprint(f\"  Coverage@95: {metrics['coverage_95']:.1f}%\")  # Should be ~95%\nprint(f\"  Calibration Error: {metrics['calibration_error']:.3f}\")  # Should be ~0\nprint(f\"  Sharpness: {metrics['sharpness']:.3f}\")  # Lower = tighter intervals\n\n\n\n\n\n\n\nTipInterpreting Metrics\n\n\n\nAccuracy (Lower is better): - RMSE: Root mean squared error - penalizes large errors - MAE: Mean absolute error - robust to outliers - R²: Variance explained (1.0 = perfect)\nCalibration (Target values): - Coverage@95: Should be ≈95% (well-calibrated) - Calibration Error: Should be ≈0 (perfectly calibrated) - Sharpness: Interval width (lower = more confident)\nGood model: High accuracy + good calibration + low sharpness\n\n\n\n\n\n\n\nGPs can detect anomalies using prediction errors:\n\n# Compute standardized residuals\nresiduals = y_test.numpy() - pred_mean\nstandardized = residuals / pred_std\n\n# Flag anomalies (|z| &gt; 3.0)\ndetected_anomalies = np.abs(standardized) &gt; 3.0\n\n# Compare to ground truth\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\ntrue_labels = test_df['is_anomaly'].to_numpy()\n\nprint(\"Anomaly Detection Performance:\")\nprint(f\"  Precision: {precision_score(true_labels, detected_anomalies):.3f}\")\nprint(f\"  Recall: {recall_score(true_labels, detected_anomalies):.3f}\")\nprint(f\"  F1-Score: {f1_score(true_labels, detected_anomalies):.3f}\")\n\n# Confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(true_labels, detected_anomalies)\nprint(f\"\\nConfusion Matrix:\")\nprint(f\"  True Negatives: {cm[0,0]}\")\nprint(f\"  False Positives: {cm[0,1]}\")\nprint(f\"  False Negatives: {cm[1,0]}\")\nprint(f\"  True Positives: {cm[1,1]}\")\n\n\n\n\n\n\n\n\nfrom cloud_sim.ml_models.gaussian_process import save_model, load_model\n\n# Save trained model\nsave_model(trained_model, \"gp_iops_model.pt\")\n\n# Load it back\nloaded_model = load_model(\"gp_iops_model.pt\", device=device)\nprint(\"✓ Model saved and loaded successfully\")\n\n\n\n\nFor production, use batched prediction for efficiency:\n\n# Process data in batches\nbatch_size = 1000\npredictions = []\n\nfor i in range(0, len(X_test), batch_size):\n    X_batch = X_test[i:i+batch_size]\n\n    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n        pred = trained_model(X_batch)\n        predictions.append(pred.mean.numpy())\n\nall_predictions = np.concatenate(predictions)\nprint(f\"✓ Processed {len(X_test):,} predictions in batches of {batch_size}\")\n\n\n\n\nFor streaming data, retrain periodically:\n\n# Pseudo-code for online learning\ndef online_update(model, new_data, window_size=10000):\n    \"\"\"Retrain model on sliding window of recent data\"\"\"\n    recent_data = new_data[-window_size:]\n    updated_model = train_gp_model(\n        model=model,\n        train_x=recent_data['x'],\n        train_y=recent_data['y'],\n        num_epochs=20,  # Fewer epochs for updates\n        learning_rate=0.005  # Lower LR for fine-tuning\n    )\n    return updated_model\n\n# Update every hour with latest data\n# model = online_update(model, latest_data)\n\n\n\n\n\n\nIn this tutorial, you learned:\n✅ Sparse variational GPs for large-scale time series ✅ Composite periodic kernels for multi-scale patterns ✅ Student-t likelihood for robustness to anomalies ✅ Uncertainty quantification with calibrated confidence intervals ✅ Comprehensive evaluation (accuracy + calibration) ✅ Anomaly detection using prediction errors ✅ Production deployment patterns\n\n\n\n\nConcepts: GP Design: Architectural deep dive\nHow-To: Train GP Models: Production recipes\nResearch: Time Series Datasets: More datasets to try\n\n\n\n\n\n\n\n\nTipFurther Reading\n\n\n\nGaussian Processes: - Rasmussen & Williams (2006) - The GP Bible - GPyTorch Documentation - Library docs - Duvenaud (2014) - Kernel cookbook\nSparse GPs: - Titsias (2009) - Variational inducing points - Hensman et al. (2013) - Scalable variational GPs\nCloud Forecasting: - Our Research Foundation\n\n\n\n\n\n\ngraph TB\n    A[Time Series Data] --&gt; B[Sparse Variational GP]\n    B --&gt; C[Composite Periodic Kernel]\n    B --&gt; D[Student-t Likelihood]\n    C --&gt; E[Daily Period Component]\n    C --&gt; F[Weekly Period Component]\n    E --&gt; G[Inducing Points]\n    F --&gt; G\n    G --&gt; H[Predictions + Uncertainty]\n    D --&gt; H\n    H --&gt; I[Anomaly Detection]\n    H --&gt; J[Forecasting]\n\n\n\n\n\n\n\n\n\n\nKey insight: Combine flexible kernels, sparse approximations, and robust likelihoods to build production-ready forecasting systems!",
    "crumbs": [
      "Home",
      "Tutorials",
      "Gaussian Process Modeling"
    ]
  },
  {
    "objectID": "tutorials/gaussian-processes.html#learning-objectives",
    "href": "tutorials/gaussian-processes.html#learning-objectives",
    "title": "Gaussian Process Modeling",
    "section": "",
    "text": "Understand sparse variational GPs for large datasets\nUse composite periodic kernels for multi-scale patterns\nTrain robust models with Student-t likelihood\nEvaluate both accuracy and calibration\nMake predictions with uncertainty quantification\n\n\n\n\n\n\n\nTipEstimated Time\n\n\n\n30 minutes\n\n\n\n\n\n\n\n\nNotePrerequisites\n\n\n\n\nBasic understanding of Gaussian Processes (see Rasmussen & Williams)\nFamiliarity with PyTorch\nCompleted Data Exploration Tutorial\n\nGPU recommended but not required - CPU works well for this tutorial.",
    "crumbs": [
      "Home",
      "Tutorials",
      "Gaussian Process Modeling"
    ]
  },
  {
    "objectID": "tutorials/gaussian-processes.html#setup",
    "href": "tutorials/gaussian-processes.html#setup",
    "title": "Gaussian Process Modeling",
    "section": "",
    "text": "import numpy as np\nimport polars as pl\nimport torch\nimport gpytorch\n\nfrom cloud_sim.ml_models.gaussian_process import (\n    CompositePeriodicKernel,\n    SparseGPModel,\n    train_gp_model,\n    compute_metrics,\n    compute_prediction_intervals\n)\n\n# Visualization\nimport altair as alt\nfrom datetime import datetime, timedelta\n\n# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Device selection\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")",
    "crumbs": [
      "Home",
      "Tutorials",
      "Gaussian Process Modeling"
    ]
  },
  {
    "objectID": "tutorials/gaussian-processes.html#part-1-the-problem",
    "href": "tutorials/gaussian-processes.html#part-1-the-problem",
    "title": "Gaussian Process Modeling",
    "section": "",
    "text": "We need to forecast cloud resource metrics (CPU, IOPS, memory) that exhibit:\n\nMulti-scale periodicity: Daily cycles + weekly patterns\nAnomalies: Operational issues, spikes, outages\nUncertainty: Need confidence intervals, not just point predictions\nScale: Thousands to millions of data points\n\nGaussian Processes are ideal because they provide: - ✅ Flexible modeling via kernel design - ✅ Principled uncertainty quantification - ✅ Non-parametric learning from data",
    "crumbs": [
      "Home",
      "Tutorials",
      "Gaussian Process Modeling"
    ]
  },
  {
    "objectID": "tutorials/gaussian-processes.html#part-2-generate-sample-data",
    "href": "tutorials/gaussian-processes.html#part-2-generate-sample-data",
    "title": "Gaussian Process Modeling",
    "section": "",
    "text": "Let’s create a realistic IOPS (I/O operations per second) workload:\n\nfrom cloud_sim.data_generation import WorkloadPatternGenerator, WorkloadType\n\n# Generate 30 days of data\ngenerator = WorkloadPatternGenerator(seed=42)\ndf = generator.generate_time_series(\n    workload_type=WorkloadType.DATABASE_OLTP,\n    start_time=datetime.now() - timedelta(days=30),\n    end_time=datetime.now(),\n    interval_minutes=1,  # Minute-level granularity\n    include_anomalies=True,\n    anomaly_rate=0.005  # 0.5% anomaly rate\n)\n\n# Use disk IOPS as our target metric\nprint(f\"Dataset size: {len(df):,} samples\")\nprint(f\"Anomalies: {df['is_anomaly'].sum()} ({df['is_anomaly'].mean()*100:.2f}%)\")\n\n\n\n\n# Split: 70% train, 30% test\ntrain_size = int(0.7 * len(df))\n\ntrain_df = df[:train_size]\ntest_df = df[train_size:]\n\nprint(f\"Train: {len(train_df):,} samples\")\nprint(f\"Test: {len(test_df):,} samples\")",
    "crumbs": [
      "Home",
      "Tutorials",
      "Gaussian Process Modeling"
    ]
  },
  {
    "objectID": "tutorials/gaussian-processes.html#part-3-understanding-the-data",
    "href": "tutorials/gaussian-processes.html#part-3-understanding-the-data",
    "title": "Gaussian Process Modeling",
    "section": "",
    "text": "# Plot the data\ntrain_plot = train_df.to_pandas()\n\nchart = alt.Chart(train_plot.iloc[::60]).mark_line(size=1).encode(  # Subsample for visualization\n    x=alt.X('timestamp:T', title='Time'),\n    y=alt.Y('disk_iops:Q', title='IOPS'),\n    color=alt.condition(\n        alt.datum.is_anomaly == True,\n        alt.value('red'),\n        alt.value('steelblue')\n    )\n).properties(\n    width=800,\n    height=300,\n    title='Database OLTP: IOPS Over Time (Training Data)'\n).interactive()\n\nchart\n\n\n\n\n\n\n\nNotePattern Observations\n\n\n\n\nDaily periodicity: Clear 24-hour cycles\nWeekly seasonality: Weekday vs. weekend differences\nAnomalies: Occasional spikes and dips (red points)\nNon-stationary: Mean shifts over time",
    "crumbs": [
      "Home",
      "Tutorials",
      "Gaussian Process Modeling"
    ]
  },
  {
    "objectID": "tutorials/gaussian-processes.html#part-4-model-architecture",
    "href": "tutorials/gaussian-processes.html#part-4-model-architecture",
    "title": "Gaussian Process Modeling",
    "section": "",
    "text": "We use a composite kernel to capture multi-scale patterns:\n\\[\nk(x, x') = k_{daily}(x, x') \\times k_{weekly}(x, x') + k_{noise}(x, x')\n\\]\nWhere: - Daily component: period_length=1440 (minutes per day) - Weekly component: period_length=10080 (minutes per week) - Noise component: Handles residual variance\n\n# Initialize kernel\nkernel = CompositePeriodicKernel(\n    periods=[1440.0, 10080.0],  # Daily + weekly\n    lengthscales=[360.0, 2520.0],  # Quarter-day, half-week\n    add_noise=True\n)\n\n\n\n\nFor large datasets, we use inducing points to approximate the full GP:\n\n# Prepare tensors\nX_train = torch.arange(len(train_df), dtype=torch.float32).reshape(-1, 1)\ny_train = torch.tensor(train_df['disk_iops'].to_numpy(), dtype=torch.float32)\n\n# Initialize model with 100 inducing points\nmodel = SparseGPModel(\n    num_inducing=100,\n    likelihood_type='student_t',  # Robust to outliers\n    kernel=kernel\n)\n\nprint(f\"Model: {model.__class__.__name__}\")\nprint(f\"Inducing points: 100\")\nprint(f\"Likelihood: Student-t (robust to anomalies)\")\n\n\n\n\n\n\n\nTipWhy Student-t Likelihood?\n\n\n\nThe Student-t distribution has heavier tails than Gaussian:\n\n✅ Robust to outliers and anomalies\n✅ Doesn’t over-fit to spikes\n✅ Better calibrated uncertainty\n✅ Production-ready for operational data\n\nTrain on all data including anomalies - the model learns to handle them!",
    "crumbs": [
      "Home",
      "Tutorials",
      "Gaussian Process Modeling"
    ]
  },
  {
    "objectID": "tutorials/gaussian-processes.html#part-5-training",
    "href": "tutorials/gaussian-processes.html#part-5-training",
    "title": "Gaussian Process Modeling",
    "section": "",
    "text": "# Train the model\ntrained_model, losses = train_gp_model(\n    model=model,\n    train_x=X_train,\n    train_y=y_train,\n    num_epochs=100,\n    learning_rate=0.01,\n    verbose=True\n)\n\n# Plot training loss\nloss_df = pl.DataFrame({'epoch': range(len(losses)), 'loss': losses})\n\nchart = alt.Chart(loss_df.to_pandas()).mark_line().encode(\n    x='epoch:Q',\n    y=alt.Y('loss:Q', scale=alt.Scale(type='log')),\n).properties(\n    width=600,\n    height=300,\n    title='Training Loss (Log Scale)'\n)\n\nchart\n\nExpected behavior: - Loss decreases rapidly in first 20 epochs - Convergence around epoch 50-80 - Final loss stabilizes",
    "crumbs": [
      "Home",
      "Tutorials",
      "Gaussian Process Modeling"
    ]
  },
  {
    "objectID": "tutorials/gaussian-processes.html#part-6-predictions-with-uncertainty",
    "href": "tutorials/gaussian-processes.html#part-6-predictions-with-uncertainty",
    "title": "Gaussian Process Modeling",
    "section": "",
    "text": "# Prepare test data\nX_test = torch.arange(len(train_df), len(df), dtype=torch.float32).reshape(-1, 1)\ny_test = torch.tensor(test_df['disk_iops'].to_numpy(), dtype=torch.float32)\n\n# Get predictions\ntrained_model.eval()\nwith torch.no_grad(), gpytorch.settings.fast_pred_var():\n    pred_dist = trained_model(X_test)\n\n    # Extract mean and variance\n    pred_mean = pred_dist.mean.numpy()\n    pred_var = pred_dist.variance.numpy()\n    pred_std = np.sqrt(pred_var)\n\n# Add to test dataframe\ntest_df = test_df.with_columns([\n    pl.Series('pred_mean', pred_mean),\n    pl.Series('pred_std', pred_std)\n])\n\n\n\n\n\n# Compute prediction intervals\nlower_95 = pred_mean - 1.96 * pred_std\nupper_95 = pred_mean + 1.96 * pred_std\n\n# Create visualization\ntest_plot = test_df.to_pandas().iloc[::60]  # Subsample\n\n# Build multi-layer chart\nbase = alt.Chart(test_plot).encode(x='timestamp:T')\n\n# Confidence band\nband = base.mark_area(opacity=0.2, color='steelblue').encode(\n    y='lower_95:Q',\n    y2='upper_95:Q'\n)\n\n# Predicted mean\npred_line = base.mark_line(color='steelblue').encode(\n    y='pred_mean:Q'\n)\n\n# Actual values\nactual_points = base.mark_circle(size=20).encode(\n    y='disk_iops:Q',\n    color=alt.condition(\n        alt.datum.is_anomaly == True,\n        alt.value('red'),\n        alt.value('darkblue')\n    )\n)\n\nchart = (band + pred_line + actual_points).properties(\n    width=800,\n    height=400,\n    title='GP Predictions with 95% Confidence Intervals'\n).interactive()\n\nchart\n\n\n\n\n\n\n\nNoteUncertainty Quantification\n\n\n\nNotice how:\n\nConfidence bands widen during anomalies\nNormal periods have tight, confident predictions\nModel uncertainty increases for out-of-distribution events\nMost actual values fall within the 95% interval\n\nThis is calibrated uncertainty - essential for production systems!",
    "crumbs": [
      "Home",
      "Tutorials",
      "Gaussian Process Modeling"
    ]
  },
  {
    "objectID": "tutorials/gaussian-processes.html#part-7-evaluation-metrics",
    "href": "tutorials/gaussian-processes.html#part-7-evaluation-metrics",
    "title": "Gaussian Process Modeling",
    "section": "",
    "text": "# Compute comprehensive metrics\nmetrics = compute_metrics(\n    y_true=y_test.numpy(),\n    y_pred=pred_mean,\n    y_std=pred_std\n)\n\nprint(\"Accuracy Metrics:\")\nprint(f\"  RMSE: {metrics['rmse']:.3f}\")\nprint(f\"  MAE: {metrics['mae']:.3f}\")\nprint(f\"  R²: {metrics['r2']:.3f}\")\nprint(f\"  MAPE: {metrics['mape']:.2f}%\")\n\nprint(\"\\nUncertainty Calibration:\")\nprint(f\"  Coverage@95: {metrics['coverage_95']:.1f}%\")  # Should be ~95%\nprint(f\"  Calibration Error: {metrics['calibration_error']:.3f}\")  # Should be ~0\nprint(f\"  Sharpness: {metrics['sharpness']:.3f}\")  # Lower = tighter intervals\n\n\n\n\n\n\n\nTipInterpreting Metrics\n\n\n\nAccuracy (Lower is better): - RMSE: Root mean squared error - penalizes large errors - MAE: Mean absolute error - robust to outliers - R²: Variance explained (1.0 = perfect)\nCalibration (Target values): - Coverage@95: Should be ≈95% (well-calibrated) - Calibration Error: Should be ≈0 (perfectly calibrated) - Sharpness: Interval width (lower = more confident)\nGood model: High accuracy + good calibration + low sharpness",
    "crumbs": [
      "Home",
      "Tutorials",
      "Gaussian Process Modeling"
    ]
  },
  {
    "objectID": "tutorials/gaussian-processes.html#part-8-anomaly-detection",
    "href": "tutorials/gaussian-processes.html#part-8-anomaly-detection",
    "title": "Gaussian Process Modeling",
    "section": "",
    "text": "GPs can detect anomalies using prediction errors:\n\n# Compute standardized residuals\nresiduals = y_test.numpy() - pred_mean\nstandardized = residuals / pred_std\n\n# Flag anomalies (|z| &gt; 3.0)\ndetected_anomalies = np.abs(standardized) &gt; 3.0\n\n# Compare to ground truth\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\ntrue_labels = test_df['is_anomaly'].to_numpy()\n\nprint(\"Anomaly Detection Performance:\")\nprint(f\"  Precision: {precision_score(true_labels, detected_anomalies):.3f}\")\nprint(f\"  Recall: {recall_score(true_labels, detected_anomalies):.3f}\")\nprint(f\"  F1-Score: {f1_score(true_labels, detected_anomalies):.3f}\")\n\n# Confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(true_labels, detected_anomalies)\nprint(f\"\\nConfusion Matrix:\")\nprint(f\"  True Negatives: {cm[0,0]}\")\nprint(f\"  False Positives: {cm[0,1]}\")\nprint(f\"  False Negatives: {cm[1,0]}\")\nprint(f\"  True Positives: {cm[1,1]}\")",
    "crumbs": [
      "Home",
      "Tutorials",
      "Gaussian Process Modeling"
    ]
  },
  {
    "objectID": "tutorials/gaussian-processes.html#part-9-production-considerations",
    "href": "tutorials/gaussian-processes.html#part-9-production-considerations",
    "title": "Gaussian Process Modeling",
    "section": "",
    "text": "from cloud_sim.ml_models.gaussian_process import save_model, load_model\n\n# Save trained model\nsave_model(trained_model, \"gp_iops_model.pt\")\n\n# Load it back\nloaded_model = load_model(\"gp_iops_model.pt\", device=device)\nprint(\"✓ Model saved and loaded successfully\")\n\n\n\n\nFor production, use batched prediction for efficiency:\n\n# Process data in batches\nbatch_size = 1000\npredictions = []\n\nfor i in range(0, len(X_test), batch_size):\n    X_batch = X_test[i:i+batch_size]\n\n    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n        pred = trained_model(X_batch)\n        predictions.append(pred.mean.numpy())\n\nall_predictions = np.concatenate(predictions)\nprint(f\"✓ Processed {len(X_test):,} predictions in batches of {batch_size}\")\n\n\n\n\nFor streaming data, retrain periodically:\n\n# Pseudo-code for online learning\ndef online_update(model, new_data, window_size=10000):\n    \"\"\"Retrain model on sliding window of recent data\"\"\"\n    recent_data = new_data[-window_size:]\n    updated_model = train_gp_model(\n        model=model,\n        train_x=recent_data['x'],\n        train_y=recent_data['y'],\n        num_epochs=20,  # Fewer epochs for updates\n        learning_rate=0.005  # Lower LR for fine-tuning\n    )\n    return updated_model\n\n# Update every hour with latest data\n# model = online_update(model, latest_data)",
    "crumbs": [
      "Home",
      "Tutorials",
      "Gaussian Process Modeling"
    ]
  },
  {
    "objectID": "tutorials/gaussian-processes.html#summary",
    "href": "tutorials/gaussian-processes.html#summary",
    "title": "Gaussian Process Modeling",
    "section": "",
    "text": "In this tutorial, you learned:\n✅ Sparse variational GPs for large-scale time series ✅ Composite periodic kernels for multi-scale patterns ✅ Student-t likelihood for robustness to anomalies ✅ Uncertainty quantification with calibrated confidence intervals ✅ Comprehensive evaluation (accuracy + calibration) ✅ Anomaly detection using prediction errors ✅ Production deployment patterns",
    "crumbs": [
      "Home",
      "Tutorials",
      "Gaussian Process Modeling"
    ]
  },
  {
    "objectID": "tutorials/gaussian-processes.html#next-steps",
    "href": "tutorials/gaussian-processes.html#next-steps",
    "title": "Gaussian Process Modeling",
    "section": "",
    "text": "Concepts: GP Design: Architectural deep dive\nHow-To: Train GP Models: Production recipes\nResearch: Time Series Datasets: More datasets to try\n\n\n\n\n\n\n\n\nTipFurther Reading\n\n\n\nGaussian Processes: - Rasmussen & Williams (2006) - The GP Bible - GPyTorch Documentation - Library docs - Duvenaud (2014) - Kernel cookbook\nSparse GPs: - Titsias (2009) - Variational inducing points - Hensman et al. (2013) - Scalable variational GPs\nCloud Forecasting: - Our Research Foundation",
    "crumbs": [
      "Home",
      "Tutorials",
      "Gaussian Process Modeling"
    ]
  },
  {
    "objectID": "tutorials/gaussian-processes.html#architecture-diagram",
    "href": "tutorials/gaussian-processes.html#architecture-diagram",
    "title": "Gaussian Process Modeling",
    "section": "",
    "text": "graph TB\n    A[Time Series Data] --&gt; B[Sparse Variational GP]\n    B --&gt; C[Composite Periodic Kernel]\n    B --&gt; D[Student-t Likelihood]\n    C --&gt; E[Daily Period Component]\n    C --&gt; F[Weekly Period Component]\n    E --&gt; G[Inducing Points]\n    F --&gt; G\n    G --&gt; H[Predictions + Uncertainty]\n    D --&gt; H\n    H --&gt; I[Anomaly Detection]\n    H --&gt; J[Forecasting]\n\n\n\n\n\n\n\n\n\n\nKey insight: Combine flexible kernels, sparse approximations, and robust likelihoods to build production-ready forecasting systems!",
    "crumbs": [
      "Home",
      "Tutorials",
      "Gaussian Process Modeling"
    ]
  },
  {
    "objectID": "tutorials/index.html",
    "href": "tutorials/index.html",
    "title": "Tutorials",
    "section": "",
    "text": "Learn Cloud Resource Simulator through hands-on, learning-oriented tutorials. These guides are designed to help you understand the core concepts and workflows through practical examples.\n\n\n\n\nLearn how to explore and analyze cloud resource utilization data using the simulator’s data generation capabilities.\nWhat you’ll learn: - Loading and exploring synthetic cloud data - Understanding workload patterns - Visualizing resource utilization - Identifying inefficiencies\nDuration: ~15 minutes\n\n\n\n\nUnderstand the different workload archetypes and their characteristic resource usage patterns.\nWhat you’ll learn: - 12+ workload types and their signatures - CPU, memory, and network patterns - Temporal autocorrelation - Application-specific behaviors\nDuration: ~20 minutes\n\n\n\n\nBuild robust time series forecasting models using Gaussian Processes with GPyTorch.\nWhat you’ll learn: - Sparse variational GP models - Composite periodic kernels - Training on operational data - Prediction with uncertainty quantification\nDuration: ~30 minutes\n\n\n\n\n\nWe recommend following the tutorials in order:\n\nData Exploration → Understand the data\nWorkload Signatures → Learn workload patterns\nGaussian Process Modeling → Build forecasting models\n\n\n\n\nAfter completing the tutorials: - Explore How-To Guides for specific tasks - Read Concepts for deeper understanding - Reference the API Documentation for detailed usage",
    "crumbs": [
      "Home",
      "Tutorials",
      "Tutorials"
    ]
  },
  {
    "objectID": "tutorials/index.html#available-tutorials",
    "href": "tutorials/index.html#available-tutorials",
    "title": "Tutorials",
    "section": "",
    "text": "Learn how to explore and analyze cloud resource utilization data using the simulator’s data generation capabilities.\nWhat you’ll learn: - Loading and exploring synthetic cloud data - Understanding workload patterns - Visualizing resource utilization - Identifying inefficiencies\nDuration: ~15 minutes\n\n\n\n\nUnderstand the different workload archetypes and their characteristic resource usage patterns.\nWhat you’ll learn: - 12+ workload types and their signatures - CPU, memory, and network patterns - Temporal autocorrelation - Application-specific behaviors\nDuration: ~20 minutes\n\n\n\n\nBuild robust time series forecasting models using Gaussian Processes with GPyTorch.\nWhat you’ll learn: - Sparse variational GP models - Composite periodic kernels - Training on operational data - Prediction with uncertainty quantification\nDuration: ~30 minutes",
    "crumbs": [
      "Home",
      "Tutorials",
      "Tutorials"
    ]
  },
  {
    "objectID": "tutorials/index.html#learning-path",
    "href": "tutorials/index.html#learning-path",
    "title": "Tutorials",
    "section": "",
    "text": "We recommend following the tutorials in order:\n\nData Exploration → Understand the data\nWorkload Signatures → Learn workload patterns\nGaussian Process Modeling → Build forecasting models",
    "crumbs": [
      "Home",
      "Tutorials",
      "Tutorials"
    ]
  },
  {
    "objectID": "tutorials/index.html#next-steps",
    "href": "tutorials/index.html#next-steps",
    "title": "Tutorials",
    "section": "",
    "text": "After completing the tutorials: - Explore How-To Guides for specific tasks - Read Concepts for deeper understanding - Reference the API Documentation for detailed usage",
    "crumbs": [
      "Home",
      "Tutorials",
      "Tutorials"
    ]
  },
  {
    "objectID": "reference/ml_models.gaussian_process.CompositePeriodicKernel.html",
    "href": "reference/ml_models.gaussian_process.CompositePeriodicKernel.html",
    "title": "ml_models.gaussian_process.CompositePeriodicKernel",
    "section": "",
    "text": "ml_models.gaussian_process.CompositePeriodicKernel(\n    slow_period=1.0,\n    fast_period=0.2,\n    rbf_lengthscale=0.1,\n    **kwargs,\n)\nComposite kernel for two-scale periodic patterns in cloud metrics.\nArchitecture: k_periodic(slow) + k_periodic(fast) + k_rbf\nADDITIVE structure (not multiplicative) for numerical stability. Fixed lengthscales prevent optimization instability.\nThis kernel is designed to capture: 1. Slow periodic component: Long-period cycles (e.g., daily patterns) 2. Fast periodic component: Short-period cycles (e.g., hourly bursts) 3. Smooth baseline: Non-periodic deviations via RBF kernel\nExample: python     # For normalized timestamps in [0, 1] range     kernel = CompositePeriodicKernel(         slow_period=1250/X_range,  # Daily cycle         fast_period=250/X_range,   # Hourly cycle         rbf_lengthscale=0.1     )\nArgs: slow_period: Period length for slow component (in normalized units) fast_period: Period length for fast component (in normalized units) rbf_lengthscale: Lengthscale for RBF kernel (controls smoothness) **kwargs: Additional arguments passed to parent Kernel class\nAttributes: slow_periodic: Scaled periodic kernel for slow component fast_periodic: Scaled periodic kernel for fast component rbf: Scaled RBF kernel for baseline deviations\n\n\n\n\n\nName\nDescription\n\n\n\n\nfast_period\nPeriod length for fast periodic component.\n\n\nis_stationary\nKernel is stationary (invariant to translations).\n\n\nrbf_lengthscale\nLengthscale for RBF component.\n\n\nslow_period\nPeriod length for slow periodic component.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nforward\nCompute covariance matrix using additive kernel combination.\n\n\n\n\n\nml_models.gaussian_process.CompositePeriodicKernel.forward(\n    x1,\n    x2,\n    diag=False,\n    last_dim_is_batch=False,\n    **params,\n)\nCompute covariance matrix using additive kernel combination.\nK(x1, x2) = K_slow(x1, x2) + K_fast(x1, x2) + K_rbf(x1, x2)\nArgs: x1: First set of inputs (n1 x d) x2: Second set of inputs (n2 x d) diag: If True, return only diagonal elements last_dim_is_batch: If True, last dimension is treated as batch **params: Additional kernel parameters\nReturns: Covariance matrix (n1 x n2) or diagonal vector (n1,) if diag=True"
  },
  {
    "objectID": "reference/ml_models.gaussian_process.CompositePeriodicKernel.html#attributes",
    "href": "reference/ml_models.gaussian_process.CompositePeriodicKernel.html#attributes",
    "title": "ml_models.gaussian_process.CompositePeriodicKernel",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nfast_period\nPeriod length for fast periodic component.\n\n\nis_stationary\nKernel is stationary (invariant to translations).\n\n\nrbf_lengthscale\nLengthscale for RBF component.\n\n\nslow_period\nPeriod length for slow periodic component."
  },
  {
    "objectID": "reference/ml_models.gaussian_process.CompositePeriodicKernel.html#methods",
    "href": "reference/ml_models.gaussian_process.CompositePeriodicKernel.html#methods",
    "title": "ml_models.gaussian_process.CompositePeriodicKernel",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nforward\nCompute covariance matrix using additive kernel combination.\n\n\n\n\n\nml_models.gaussian_process.CompositePeriodicKernel.forward(\n    x1,\n    x2,\n    diag=False,\n    last_dim_is_batch=False,\n    **params,\n)\nCompute covariance matrix using additive kernel combination.\nK(x1, x2) = K_slow(x1, x2) + K_fast(x1, x2) + K_rbf(x1, x2)\nArgs: x1: First set of inputs (n1 x d) x2: Second set of inputs (n2 x d) diag: If True, return only diagonal elements last_dim_is_batch: If True, last dimension is treated as batch **params: Additional kernel parameters\nReturns: Covariance matrix (n1 x n2) or diagonal vector (n1,) if diag=True"
  },
  {
    "objectID": "reference/ml_models.gaussian_process.SparseGPModel.html",
    "href": "reference/ml_models.gaussian_process.SparseGPModel.html",
    "title": "ml_models.gaussian_process.SparseGPModel",
    "section": "",
    "text": "ml_models.gaussian_process.SparseGPModel(\n    inducing_points,\n    learn_inducing_locations=True,\n    slow_period=1.0,\n    fast_period=0.2,\n    rbf_lengthscale=0.1,\n)\nSparse Variational Gaussian Process (SVGP) with composite periodic kernel.\nUses inducing points to achieve O(nm²) complexity instead of O(n³), enabling training on datasets with 100k+ samples.\nThe model learns: 1. Inducing point locations (optimized during training) 2. Variational distribution over inducing function values 3. Kernel hyperparameters (outputscale for each component) 4. Mean function parameters (constant baseline)\nExample: ```python # Initialize inducing points (evenly spaced) M = 200 inducing_indices = torch.linspace(0, len(X_train)-1, M, dtype=torch.long) inducing_points = X_train[inducing_indices].clone()\n# Create model\nmodel = SparseGPModel(inducing_points=inducing_points)\nlikelihood = gpytorch.likelihoods.StudentTLikelihood()\n\n# Train (see training.py for full example)\nmll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=len(X_train))\n```\nArgs: inducing_points: Tensor of shape (M, D) where M is number of inducing points, D is input dimensionality (typically 1 for time series) learn_inducing_locations: If True, optimize inducing point locations (default: True) slow_period: Period for slow periodic component (default: 1.0) fast_period: Period for fast periodic component (default: 0.2) rbf_lengthscale: Lengthscale for RBF component (default: 0.1)\nAttributes: mean_module: Constant mean function covar_module: CompositePeriodicKernel for multi-scale patterns variational_strategy: Handles inducing point approximation\n\n\n\n\n\nName\nDescription\n\n\n\n\ninducing_points\nCurrent inducing point locations.\n\n\nnum_inducing_points\nNumber of inducing points used in variational approximation.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nforward\nForward pass through the GP model.\n\n\n\n\n\nml_models.gaussian_process.SparseGPModel.forward(x)\nForward pass through the GP model.\nArgs: x: Input tensor of shape (n, 1) for univariate time series\nReturns: MultivariateNormal distribution with mean and covariance"
  },
  {
    "objectID": "reference/ml_models.gaussian_process.SparseGPModel.html#attributes",
    "href": "reference/ml_models.gaussian_process.SparseGPModel.html#attributes",
    "title": "ml_models.gaussian_process.SparseGPModel",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ninducing_points\nCurrent inducing point locations.\n\n\nnum_inducing_points\nNumber of inducing points used in variational approximation."
  },
  {
    "objectID": "reference/ml_models.gaussian_process.SparseGPModel.html#methods",
    "href": "reference/ml_models.gaussian_process.SparseGPModel.html#methods",
    "title": "ml_models.gaussian_process.SparseGPModel",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nforward\nForward pass through the GP model.\n\n\n\n\n\nml_models.gaussian_process.SparseGPModel.forward(x)\nForward pass through the GP model.\nArgs: x: Input tensor of shape (n, 1) for univariate time series\nReturns: MultivariateNormal distribution with mean and covariance"
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "API Reference",
    "section": "",
    "text": "Synthetic cloud resource data generation with realistic utilization patterns\n\n\n\ndata_generation.WorkloadPatternGenerator\nGenerate realistic cloud workload patterns based on research data\n\n\ndata_generation.WorkloadType\nDifferent application workload patterns based on research\n\n\ndata_generation.CloudMetricsSimulator\nSimulates cloud infrastructure metrics for multiple providers\n\n\ndata_generation.CloudMetricsDatasetBuilder\nBuild and manage HuggingFace datasets for cloud metrics\n\n\n\n\n\n\nGPyTorch-based sparse variational GP for time series forecasting\n\n\n\nml_models.gaussian_process.SparseGPModel\nSparse Variational Gaussian Process (SVGP) with composite periodic kernel.\n\n\nml_models.gaussian_process.CompositePeriodicKernel\nComposite kernel for two-scale periodic patterns in cloud metrics.\n\n\nml_models.gaussian_process.initialize_inducing_points\nInitialize inducing point locations for sparse GP.\n\n\nml_models.gaussian_process.train_gp_model\nTrain sparse GP model using mini-batch variational inference.\n\n\nml_models.gaussian_process.compute_metrics\nCompute comprehensive model evaluation metrics.\n\n\n\n\n\n\nPyMC-based hierarchical models for cloud resource patterns (coming soon)",
    "crumbs": [
      "Home",
      "API Reference",
      "API Reference"
    ]
  },
  {
    "objectID": "reference/index.html#data-generation",
    "href": "reference/index.html#data-generation",
    "title": "API Reference",
    "section": "",
    "text": "Synthetic cloud resource data generation with realistic utilization patterns\n\n\n\ndata_generation.WorkloadPatternGenerator\nGenerate realistic cloud workload patterns based on research data\n\n\ndata_generation.WorkloadType\nDifferent application workload patterns based on research\n\n\ndata_generation.CloudMetricsSimulator\nSimulates cloud infrastructure metrics for multiple providers\n\n\ndata_generation.CloudMetricsDatasetBuilder\nBuild and manage HuggingFace datasets for cloud metrics",
    "crumbs": [
      "Home",
      "API Reference",
      "API Reference"
    ]
  },
  {
    "objectID": "reference/index.html#gaussian-process-models",
    "href": "reference/index.html#gaussian-process-models",
    "title": "API Reference",
    "section": "",
    "text": "GPyTorch-based sparse variational GP for time series forecasting\n\n\n\nml_models.gaussian_process.SparseGPModel\nSparse Variational Gaussian Process (SVGP) with composite periodic kernel.\n\n\nml_models.gaussian_process.CompositePeriodicKernel\nComposite kernel for two-scale periodic patterns in cloud metrics.\n\n\nml_models.gaussian_process.initialize_inducing_points\nInitialize inducing point locations for sparse GP.\n\n\nml_models.gaussian_process.train_gp_model\nTrain sparse GP model using mini-batch variational inference.\n\n\nml_models.gaussian_process.compute_metrics\nCompute comprehensive model evaluation metrics.",
    "crumbs": [
      "Home",
      "API Reference",
      "API Reference"
    ]
  },
  {
    "objectID": "reference/index.html#bayesian-hierarchical-models",
    "href": "reference/index.html#bayesian-hierarchical-models",
    "title": "API Reference",
    "section": "",
    "text": "PyMC-based hierarchical models for cloud resource patterns (coming soon)",
    "crumbs": [
      "Home",
      "API Reference",
      "API Reference"
    ]
  },
  {
    "objectID": "reference/data_generation.WorkloadPatternGenerator.html",
    "href": "reference/data_generation.WorkloadPatternGenerator.html",
    "title": "data_generation.WorkloadPatternGenerator",
    "section": "",
    "text": "data_generation.WorkloadPatternGenerator(seed=None)\nGenerate realistic cloud workload patterns based on research data\n\n\n\n\n\nName\nDescription\n\n\n\n\ngenerate\nGenerate synthetic data with specified number of samples.\n\n\ngenerate_anomalies\nInject realistic anomalies into the data\n\n\ngenerate_time_series\nGenerate time series data for a specific workload type\n\n\n\n\n\ndata_generation.WorkloadPatternGenerator.generate(\n    num_samples=1000,\n    workload_type=None,\n    **kwargs,\n)\nGenerate synthetic data with specified number of samples.\nThis is a wrapper for generate_time_series for backward compatibility.\n\n\n\ndata_generation.WorkloadPatternGenerator.generate_anomalies(\n    df,\n    anomaly_types=None,\n)\nInject realistic anomalies into the data\n\n\n\ndata_generation.WorkloadPatternGenerator.generate_time_series(\n    workload_type,\n    start_time,\n    end_time,\n    interval_minutes=5,\n)\nGenerate time series data for a specific workload type"
  },
  {
    "objectID": "reference/data_generation.WorkloadPatternGenerator.html#methods",
    "href": "reference/data_generation.WorkloadPatternGenerator.html#methods",
    "title": "data_generation.WorkloadPatternGenerator",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ngenerate\nGenerate synthetic data with specified number of samples.\n\n\ngenerate_anomalies\nInject realistic anomalies into the data\n\n\ngenerate_time_series\nGenerate time series data for a specific workload type\n\n\n\n\n\ndata_generation.WorkloadPatternGenerator.generate(\n    num_samples=1000,\n    workload_type=None,\n    **kwargs,\n)\nGenerate synthetic data with specified number of samples.\nThis is a wrapper for generate_time_series for backward compatibility.\n\n\n\ndata_generation.WorkloadPatternGenerator.generate_anomalies(\n    df,\n    anomaly_types=None,\n)\nInject realistic anomalies into the data\n\n\n\ndata_generation.WorkloadPatternGenerator.generate_time_series(\n    workload_type,\n    start_time,\n    end_time,\n    interval_minutes=5,\n)\nGenerate time series data for a specific workload type"
  },
  {
    "objectID": "reference/data_generation.WorkloadType.html",
    "href": "reference/data_generation.WorkloadType.html",
    "title": "data_generation.WorkloadType",
    "section": "",
    "text": "data_generation.WorkloadType\ndata_generation.WorkloadType()\nDifferent application workload patterns based on research"
  }
]