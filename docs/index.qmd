---
title: "Hello Cloud"
subtitle: "Time series analysis for cloud resource optimization"
---

# Hello Cloud

**Systematic exploration of forecasting and anomaly detection applied to cloud infrastructure cost and utilization.**

This project demonstrates technical ramp-up into a new domain through hands-on implementation of state-of-the-art time series techniques: Foundation Models (TimesFM, Chronos), Gaussian Processes (GPyTorch), statistical methods (ARIMA, Prophet), and Spark 4.0 for distributed analysis. All synthetic data generation and modeling decisions are grounded in **35+ peer-reviewed studies** analyzing real cloud infrastructure patterns.

::: {.callout-note}
## Technical Approach

**Forecasting Methods**
- Foundation Models: Zero-shot prediction (TimesFM, Chronos) without training
- Gaussian Processes: Probabilistic forecasts with calibrated uncertainty (GPyTorch SVGPs)
- Statistical Methods: ARIMA, Prophet, and classical time series analysis
- Distributed Analysis: Spark 4.0 for workloads requiring scale

**Anomaly Detection Strategies**
- Forecast-based: Prediction interval violations
- Statistical: Isolation Forest, LOF, One-Class SVM
- Reconstruction-based: LSTM autoencoders, Transformers
- Hybrid: Multiple method ensembles

**Empirical Foundation**
Research shows cloud infrastructure exhibits consistent patterns: 13% average CPU utilization, 30% resource waste, strong temporal autocorrelation (r=0.7-0.8). All synthetic data generation uses these empirically-validated parameters.

[Research Foundation →](concepts/research/cloud-resource-patterns-research.qmd)
:::

## Methodology

### Data Sources
Analysis uses real operational time series (IOPS web server KPIs, Alibaba cluster traces) and synthetic data parameterized from peer-reviewed research. All workload patterns reflect empirically-observed cloud infrastructure behavior.

### Comparative Analysis
Apply multiple techniques (Foundation Models, Gaussian Processes, statistical methods) to identical datasets. Document what works, what fails, and why. Build systematic understanding through controlled experimentation.

### Appropriate Tooling
Local analysis with Ibis+DuckDB for fast iteration. Spark 4.0 for workloads requiring distributed computation (Alibaba's 1.3k machine × 6-month trace dataset). Right tool for the scale.

### Reproducible Workflow
MyST-format notebooks provide version-controlled source. Hot reload via Jupytext for rapid iteration. Dependency management via `uv`. All analysis fully reproducible.

### Empirical Grounding
Workload patterns, resource correlations, and waste profiles derived from 35+ peer-reviewed studies analyzing production cloud infrastructure at Google, Microsoft, Alibaba, and AWS.

---

## Getting Started

### Setup

```bash
# Clone repository
git clone https://github.com/nehalecky/hello-cloud.git
cd hello-cloud

# Install dependencies with uv
just install

# Launch Jupyter Lab
just lab
```

### Example Analysis

View executed notebooks with outputs or run locally:

```bash
# Open notebook in Jupyter Lab
just nb 03_EDA_iops_web_server

# View published tutorial
just tut iops-eda
```

### Generate Synthetic Data

```{python}
#| eval: false
from cloud_sim.data_generation import WorkloadPatternGenerator, WorkloadType
from datetime import datetime, timedelta

# Initialize generator
generator = WorkloadPatternGenerator(seed=42)

# Generate 30 days of web application data
df = generator.generate_time_series(
    workload_type=WorkloadType.WEB_APP,
    start_time=datetime.now() - timedelta(days=30),
    end_time=datetime.now(),
    interval_minutes=60,
    include_anomalies=True
)

print(df.head())
```

**Generated metrics include:**
- CPU & memory utilization
- Network I/O (ingress/egress)
- Disk IOPS
- Efficiency scores & waste estimates
- Anomaly flags

### Train a Gaussian Process Model

```{python}
#| eval: false
from cloud_sim.ml_models.gaussian_process import (
    SparseGPModel,
    train_gp_model,
    compute_metrics
)

# Initialize model with composite periodic kernel
model = SparseGPModel(
    num_inducing=100,
    likelihood="student_t"  # Robust to outliers
)

# Train on your data
trained_model = train_gp_model(
    model=model,
    train_x=train_data,
    train_y=train_targets,
    num_epochs=100
)

# Make predictions with uncertainty
predictions, uncertainty = trained_model.predict(test_x)

# Evaluate (accuracy + calibration)
metrics = compute_metrics(predictions, actual_values)
print(f"RMSE: {metrics['rmse']:.3f}")
print(f"Calibration Error: {metrics['calibration_error']:.3f}")
```

---

## Architecture Overview

```mermaid
graph TB
    A[Application Layer] --> B[ML/Forecasting Layer]
    B --> C[Data Generation Layer]

    A --> A1[Streamlit Dashboard]
    A --> A2[Jupyter Notebooks]

    B --> B1[Gaussian Processes]
    B --> B2[Bayesian Hierarchical]
    B --> B3[Foundation Models]

    C --> C1[Workload Patterns]
    C --> C2[Cloud Metrics Simulator]
```

**Three-tier design:**

1. **Data Generation Layer**: Realistic synthetic data with configurable workload patterns
2. **ML/Forecasting Layer**: Multiple modeling approaches for different use cases
3. **Application Layer**: Interactive dashboards and analysis notebooks

---

## Key Features

### Empirically-Grounded Patterns

Every workload archetype is configured with **real-world statistics**:

| Workload Type | Avg CPU | Avg Memory | Temporal Pattern |
|--------------|---------|------------|------------------|
| Web App | 15-25% | 30-40% | Daily + Weekly |
| ML Training | 70-90% | 60-80% | Burst + Steady |
| Database OLTP | 40-60% | 50-70% | Business Hours |
| Dev Environment | 5-10% | 10-20% | Sporadic |

[See All Archetypes →](tutorials/workload-signatures.qmd)

### Correlation Modeling

Multivariate metrics with **empirical correlations**:

- CPU ↔ Memory: 0.2-0.95 (workload-dependent)
- Network ↔ CPU: 0.7-0.8 (web applications)
- Temporal autocorrelation: 0.7-0.8 (first 10 lags)

[Read Correlation Research →](concepts/research/cloud-resource-correlations-report.qmd)

### Flexible Export Formats

- **Polars DataFrames** (primary format)
- **HuggingFace Datasets** (for ML training)
- **Parquet/CSV** (for external tools)

---

## Documentation

::::: {.columns}

:::: {.column width="50%"}
### Learning-Oriented

**[Tutorials](tutorials/index.qmd)**

Step-by-step guides to learn core concepts:

- Data Exploration
- Workload Signatures
- Gaussian Process Modeling
::::

:::: {.column width="50%"}
### Task-Oriented

**[How-To Guides](how-to/index.qmd)**

Practical recipes for specific tasks:

- Generate Synthetic Data
- Train GP Models
- Export to HuggingFace
::::

:::::

::::: {.columns}

:::: {.column width="50%"}
### Understanding-Oriented

**[Concepts](concepts/index.qmd)**

Deep dives into theory and design:

- Research Foundation (35+ citations)
- Correlation Analysis
- GP Architecture
::::

:::: {.column width="50%"}
### Information-Oriented

**[API Reference](reference/index.qmd)**

Complete API documentation:

- Data Generation
- ML Models
- ETL & Loaders
::::

:::::

---

## Research Foundation

This project is built on **extensive empirical analysis** of cloud resource patterns:

::: {.callout-tip}
## Key Research Findings

- **CPU Utilization**: 13% average (industry-wide) vs. 38% (datacenter-wide)
- **Resource Waste**: 30-32% of cloud spending
- **Development Environments**: 70% waste (often idle/forgotten)
- **Temporal Autocorrelation**: 0.7-0.8 for operational workloads

**35+ academic citations** from Google, Microsoft Research, Alibaba, and more.

[Explore Research →](concepts/index.qmd)
:::

---

## Technical Demonstrations

### Foundation Model Evaluation
Applied TimesFM and Chronos to real cloud KPI data. Evaluated zero-shot forecasting performance without training or hyperparameter tuning. Documented failure modes and applicability boundaries.

### Production-Ready Gaussian Processes
Implemented sparse variational GPs (GPyTorch) with composite periodic kernels. Kernel design informed by ACF/FFT analysis. Achieved calibrated uncertainty quantification with 92% test coverage on model library.

### Appropriate Scale Awareness
Local analysis with Ibis+DuckDB for fast iteration (most workloads). Spark 4.0 reserved for datasets requiring distributed computation (Alibaba 1.3k machine × 6-month traces). Documented when to scale vs. when to stay local.

### Realistic Anomaly Detection
Applied detection methods to time series with 13% average utilization, seasonal patterns, and correlated metrics. Evaluated forecast-based, statistical, and reconstruction-based approaches on expert-labeled ground truth.

### Research-Grounded Synthesis
Synthesized 35+ peer-reviewed studies into parameterized workload generators. All synthetic data reflects empirically-observed patterns: 13% avg CPU, 30% waste, 0.7-0.8 temporal autocorrelation.

---

## Open Source

All notebooks, analysis, and code available at [github.com/nehalecky/hello-cloud](https://github.com/nehalecky/hello-cloud).

**License**: MIT

---

## Tech Stack

**Data Processing**
- [Ibis](https://ibis-project.org): Portable DataFrame API (DuckDB backend locally, PySpark for scale)
- [DuckDB](https://duckdb.org): In-memory OLAP for fast analytical queries
- [Spark 4.0](https://spark.apache.org): Distributed computation for large-scale workloads

**ML & Modeling**
- [GPyTorch](https://gpytorch.ai): Sparse variational Gaussian Processes
- [PyMC](https://www.pymc.io): Bayesian hierarchical models
- HuggingFace: Foundation model integration (TimesFM, Chronos)

**Development Workflow**
- [Jupytext](https://jupytext.readthedocs.io): MyST notebooks for version control
- [Quarto](https://quarto.org): Documentation publishing
- [uv](https://docs.astral.sh/uv/): Fast dependency management
- [just](https://just.systems): Command runner for workflows
